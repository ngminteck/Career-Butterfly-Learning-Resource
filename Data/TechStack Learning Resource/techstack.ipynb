{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b9b981e-d994-41b8-81d8-8ff213a69c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "import pandas as pd\n",
    "import pypandoc\n",
    "from pypandoc.pandoc_download import download_pandoc\n",
    "#download_pandoc()\n",
    "import csv\n",
    "import docx\n",
    "import os\n",
    "from pathlib import Path\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import shutil\n",
    "import html2text\n",
    "\n",
    "class Skill:\n",
    "    def __init__(self,name, keyword , groups=None,prerequisites= None):\n",
    "        filename = name\n",
    "        filename = filename.replace('/','-')\n",
    "        filename = filename.replace(\"\\\\\",'-')\n",
    "        filename = filename + \".html\"\n",
    "        path = os.path.join(\"skill\", filename)\n",
    "        path = path.replace(\"\\\\\",'/')\n",
    "        self.resource_path = path # for the resource path\n",
    "        self.keyword_search = keyword  # keyword for searching LLM\n",
    "        self.group_set = set()\n",
    "        if groups is not None:    \n",
    "            self.UpdateGroupSet(groups)\n",
    "        \n",
    "    def UpdateGroupSet(self,groups):\n",
    "        self.group_set.update(groups)\n",
    "        #print(\"skill group set updated.\")\n",
    "\n",
    "    def ChangeKeyword(self,keyword):\n",
    "        self.keyword_search = keyword\n",
    "  \n",
    "class Group:\n",
    "    def __init__(self,name,skills):\n",
    "        filename = name\n",
    "        filename = filename.replace('/','-')\n",
    "        filename = filename.replace(\"\\\\\",'-')\n",
    "        filename = filename + \".html\"\n",
    "        path = os.path.join(\"group\", filename)\n",
    "        path = path.replace(\"\\\\\",'/')\n",
    "        self.resource_path = path # for the resource path\n",
    "        self.keyword_search = name + \" in tech\" # keyword for searching LLM\n",
    "        self.skill_set = skills\n",
    "\n",
    "    def UpdateSkillSet(self,skill):\n",
    "        self.skill_set.update(skill)\n",
    "        #print(\"group skill set updated.\")\n",
    "\n",
    "    def ChangeKeyword(self,keyword):\n",
    "        self.keyword_search = keyword\n",
    "\n",
    "class TechStack:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load('en_core_web_md')\n",
    "        self.skill_dict_list = {}\n",
    "        self.group_dict_list = {}\n",
    "        self.exact_match_replace_dict_list = {}\n",
    "        self.partial_match_replace_dict_list = {}\n",
    "        self.vector_group_dict_list = {}\n",
    "        self.ignore_set = set()\n",
    "        self.not_found_dict_list = {}\n",
    "        self.document_pepare_set = set()\n",
    "        self.three_word_skill_classification_set =set()\n",
    "        self.two_word_skill_classification_set =set()\n",
    "        self.one_word_skill_classification_set =set()\n",
    "        self.backup_keyword_dict_list={}\n",
    "        self.ImportIgnoreSet()\n",
    "        self.AllThisWillBeRemoveOnceFinalize()\n",
    "        self.ImportClassificationSet()\n",
    "        self.ImportSkillDictList()\n",
    "        self.GroupTextVectorization()\n",
    "\n",
    "               \n",
    "    def AddSkillDictList(self,name,keyword,groups=None):\n",
    "        if name not in self.skill_dict_list:\n",
    "            self.skill_dict_list[name] = Skill(name,keyword,groups)\n",
    "            #print(name,\"added in skill_dict_list.\")\n",
    "            if groups is not None:\n",
    "                for g in groups:\n",
    "                    if g in self.group_dict_list:\n",
    "                        self.group_dict_list.get(g).UpdateSkillSet({name})\n",
    "                        #print(name,\"added in\",g,\".\")\n",
    "                    else:\n",
    "                        self.group_dict_list[g] = Group(g,{name})\n",
    "                        #print(\"new group:\",g,\"have been created and added\",name,\".\")\n",
    "        else:\n",
    "            self.UpdateSkillDictList(name,groups)\n",
    "\n",
    "    def ReClassificationSkillDictList(self,name,keyword,groups):\n",
    "        search_keyword = keyword\n",
    "        if name in self.backup_keyword_dict_list:\n",
    "            search_keyword = self.backup_keyword_dict_list[name]\n",
    "        self.AddSkillDictList(name,search_keyword,groups)\n",
    "       \n",
    "                    \n",
    "    def UpdateSkillDictList(self,name,groups):\n",
    "        if name in self.skill_dict_list:\n",
    "            self.skill_dict_list[name].UpdateGroupSet(groups)\n",
    "\n",
    "    def AddGroupDictList(self,name,skills):\n",
    "        if skills is not None:\n",
    "            if name in self.group_dict_list:\n",
    "                self.UpdateGroupDictList(name,skills)\n",
    "            else:\n",
    "                found_set = set()\n",
    "                for s in skills:\n",
    "                    if s in self.skill_dict_list:\n",
    "                        self.skill_dict_list[s].UpdateGroupSet({name})\n",
    "                        found_set.add(s)  \n",
    "                        #print(s,\"added in\",name,\"group set.\")\n",
    "                self.group_dict_list[name] = Group(name,found_set)\n",
    "\n",
    "    def UpdateGroupDictList(self,name,skills):\n",
    "        if name in self.group_dict_list:\n",
    "            found_set = set()\n",
    "            for s in skills:\n",
    "                if s in self.skill_dict_list:\n",
    "                      found_set.add(s)  \n",
    "            self.group_dict_list[name].UpdateSkillSet(found_set)\n",
    "        else:\n",
    "            self.AddGroupDictList(name,skills)\n",
    "\n",
    "    def AddNotFoundDictList(self,name,keyword):\n",
    "        if name not in self.not_found_dict_list:\n",
    "            path = \"unclassified\"\n",
    "            self.not_found_dict_list[name] =  Skill(name,path,keyword,None)\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "    def ImportIgnoreSet(self):\n",
    "        f = open(\"ignore.txt\", \"r\")\n",
    "        for c in f:\n",
    "            c = c.replace(\"\\n\", \"\")\n",
    "            self.ignore_set.add(c)\n",
    "        f.close()\n",
    "\n",
    "    def ImportClassificationSet(self):\n",
    "        f = open(\"three word skill classification.txt\", \"r\")\n",
    "        for l in f:\n",
    "            l = l.replace(\"\\n\", \"\")\n",
    "            self.three_word_skill_classification_set.add(l)\n",
    "        f.close()\n",
    "        f = open(\"two word skill classification.txt\", \"r\")\n",
    "        for l in f:\n",
    "            l = l.replace(\"\\n\", \"\")\n",
    "            self.two_word_skill_classification_set.add(l)\n",
    "        f.close()\n",
    "        f = open(\"one word skill classification.txt\", \"r\")\n",
    "        for l in f:\n",
    "            l = l.replace(\"\\n\", \"\")\n",
    "            self.one_word_skill_classification_set.add(l)\n",
    "        f.close()\n",
    "        \n",
    "    def ExportSkillDictList(self):\n",
    "        file_path = \"skills.csv\"\n",
    "        with open(file_path, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Name\", \"Search Keyword\",\"Resource Path\",\"Groups\"])\n",
    "            for key, value in self.skill_dict_list.items():\n",
    "                name=key\n",
    "                search = value.keyword_search\n",
    "                path = value.resource_path\n",
    "                groups =\"\"\n",
    "            \n",
    "                for g in value.group_set:\n",
    "                    groups += \"[\"\n",
    "                    groups += g\n",
    "                    groups +=\"]\"\n",
    "             \n",
    "                writer.writerow([name,search,path,groups])\n",
    "            file.close()\n",
    "        with open('skills.txt', 'w') as f:\n",
    "            for i in self.skill_dict_list:\n",
    "                f.write(i)\n",
    "                f.write(\"\\n\")\n",
    "            f.close()\n",
    "            \n",
    "    def ImportSkillDictList(self):\n",
    "        df = pd.read_csv(\"skills.csv\")\n",
    "        for index, row in df.iterrows():\n",
    "            name = str(row['Name'])\n",
    "            keyword = str(row['Search Keyword'])\n",
    "            groups = str(row['Groups'])\n",
    "            groups_set = None\n",
    "            groups = groups.replace('[', '')\n",
    "            groups_list = groups.split(']')\n",
    "            if len(groups_list) > 0 :\n",
    "                groups_list = groups_list[:-1]\n",
    "                groups_set = set()\n",
    "                for g in groups_list:\n",
    "                    groups_set.add(g)\n",
    "            # auto create group also\n",
    "            self.AddSkillDictList(name,keyword,groups_set)\n",
    "                \n",
    "\n",
    "        \n",
    "    \n",
    "    def ExportGroupDictList(self):\n",
    "        file_path = \"groups.csv\"\n",
    "        with open(file_path, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Name\", \"Search Keyword\",\"Resource Path\",\"skills\"])\n",
    "            for key, value in self.group_dict_list.items():\n",
    "                name=key\n",
    "                search = value.keyword_search\n",
    "                path = value.resource_path\n",
    "                skills =\"\"\n",
    "                for s in value.skill_set:\n",
    "                    skills += \"[\"\n",
    "                    skills += s\n",
    "                    skills +=\"]\"\n",
    "                writer.writerow([name,search,path,skills])\n",
    "            file.close()\n",
    "\n",
    "\n",
    "\n",
    "    def ExportMatchReplaceDictList(self):\n",
    "        file_path = \"exact match.csv\"\n",
    "        with open(file_path, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Word\", \"Replace\"])\n",
    "            for key, value in self.exact_match_replace_dict_list.items():\n",
    "                writer.writerow([key,value])\n",
    "            file.close()\n",
    "        file_path = \"partial match.csv\"\n",
    "        with open(file_path, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Word\", \"Replace\"])\n",
    "            for key, value in self.partial_match_replace_dict_list.items():\n",
    "                writer.writerow([key,value])\n",
    "            file.close()\n",
    "\n",
    "\n",
    "\n",
    "    def ExportNotFoundSet(self):\n",
    "        file_path = \"not found.csv\"\n",
    "        with open(file_path, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Name\", \"Search Keyword\",\"Resource Path\",\"Groups\"])\n",
    "            for key, value in self.not_found_dict_list.items():\n",
    "                name=key\n",
    "                search = value.keyword_search\n",
    "                path = value.resource_path\n",
    "             \n",
    "                writer.writerow([name,search,path,\"\"])\n",
    "            file.close()\n",
    "     \n",
    "\n",
    "    def Filter(self, text):\n",
    "        text = text.lower()\n",
    "        text = text.replace(\"/\",\" \")\n",
    "        if text.find('(') != -1:\n",
    "            text = text.split(\"(\")[0]\n",
    "            text = text.rsplit()[0]\n",
    "            \n",
    "        words = text.split()\n",
    "    \n",
    "        if text in self.ignore_set:\n",
    "            return str(\"\")\n",
    "            \n",
    "        if text in self.exact_match_replace_dict_list:\n",
    "            text = self.exact_match_replace_dict_list.get(text)\n",
    "            \n",
    "        words = text.split()\n",
    "        new_text = \"\"\n",
    "        for word in words:\n",
    "            if word in self.partial_match_replace_dict_list:\n",
    "                new_text += self.partial_match_replace_dict_list.get(word) \n",
    "                new_text +=\" \"\n",
    "            else:\n",
    "                new_text += word \n",
    "                new_text +=\" \"\n",
    "        return new_text[:-1]\n",
    "\n",
    "    def Search(self,text):\n",
    "        if text in self.skill_dict_list:\n",
    "            self.document_pepare_set.add(text)\n",
    "            return True\n",
    "        if text in self.group_dict_list:\n",
    "            self.document_pepare_set.add(text)\n",
    "            return True\n",
    "        # check for . - space and .js js\n",
    "        for sdl in  self.skill_dict_list:\n",
    "            check1 = sdl\n",
    "            check1 = check1.replace(\".\",\"\")\n",
    "            check2 = text\n",
    "            check2 = check2.replace(\".\",\"\")\n",
    "            if check1 == check2:\n",
    "                self.document_pepare_set.add(sdl)\n",
    "                return True\n",
    "            check1 = sdl\n",
    "            check1 = check1.replace(\"-\",\" \")\n",
    "            check2 = text\n",
    "            check2 = check2.replace(\"-\",\" \")\n",
    "            if check1 == check2:\n",
    "                self.document_pepare_set.add(sdl)\n",
    "                return True\n",
    "            check1 = sdl\n",
    "            check1 = check1.replace(\" \",\"\")\n",
    "            check2 = text\n",
    "            check2 = check2.replace(\" \",\"\")\n",
    "            if check1 == check2:\n",
    "                self.document_pepare_set.add(sdl)\n",
    "                return True\n",
    "            check1 = sdl\n",
    "            check1 = check1.replace(\".js\",\"\")\n",
    "            check1 = check1.replace(\"js\",\"\")\n",
    "            check2 = text\n",
    "            check2 = check2.replace(\".js\",\"\")\n",
    "            check2 = check2.replace(\"js\",\"\")\n",
    "            if check1 == check2:\n",
    "                self.document_pepare_set.add(sdl)\n",
    "                return True\n",
    "   \n",
    "        found = False      \n",
    "        words = text.split()\n",
    "        # check word by word\n",
    "        for word in words:   \n",
    "            if word in self.skill_dict_list:\n",
    "                self.document_pepare_set.add(word)\n",
    "                found = True\n",
    "            elif word in self.group_dict_list:\n",
    "                self.document_pepare_set.add(word)\n",
    "                found = True\n",
    "        \n",
    "        return found \n",
    "        \n",
    "\n",
    "    def GenerateLearningResource(self,your_skills, job_skills):\n",
    "        skills = set()\n",
    "        if your_skills is not None:\n",
    "            skills =  job_skills -  your_skills \n",
    "        else:\n",
    "            skills = job_skills\n",
    "\n",
    "        if len(skills) == 0:\n",
    "            print(\"you are good.\")\n",
    "            return False\n",
    "        \n",
    "        self.document_pepare_set.clear()\n",
    "\n",
    "        for s in skills:\n",
    "            s = self.Filter(s)\n",
    "            if s != \"\":\n",
    "                found = self.Search(s)\n",
    "                if found == False:\n",
    "                    self.AddNotFoundDictList(s,s + \" in tech\")\n",
    "                    \n",
    "        if len(self.document_pepare_set) == 0 :\n",
    "            print(\"No any learning resource generated.\")\n",
    "            return False\n",
    "        print(self.document_pepare_set)\n",
    "        html_content = \"\"\n",
    "        for d in self.document_pepare_set:\n",
    "            path = \"\"\n",
    "            if d in self.skill_dict_list:\n",
    "                v = self.skill_dict_list.get(d)\n",
    "                path = v.resource_path\n",
    "            elif d in self.skill_dict_list:\n",
    "                v = self.group_dict_list.get(d)\n",
    "                path = v.resource_path\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            if os.path.isfile(path) == False:\n",
    "                print(d,\"not found in\",path)\n",
    "            else:\n",
    "                with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "                    title = d.title()\n",
    "                    html_content +=\"<h1><u><b>\"\n",
    "                    html_content += title\n",
    "                    html_content +=\"</b></u></h1>\"\n",
    "                    html_content += file.read()\n",
    "                file.close()\n",
    "        with open(\"learning resource.html\", 'w', encoding='utf-8') as file:\n",
    "            file.write(html_content)  \n",
    "            file.close()\n",
    "        output = pypandoc.convert_text(html_content, 'docx', format='html', outputfile='learning resource.docx')\n",
    "        if output == \"\":\n",
    "            print(\"Document output sucessfully.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Document output failed\")\n",
    "            return False\n",
    "\n",
    "    def GroupTextVectorization(self):\n",
    "        for word in self.group_dict_list:\n",
    "            if self.nlp.vocab[word].has_vector == True:\n",
    "                vector_word = self.nlp(word)\n",
    "                if vector_word not in self.vector_group_dict_list:\n",
    "                    self.vector_group_dict_list[vector_word] = set()\n",
    "                self.vector_group_dict_list[vector_word].add(word)\n",
    "\n",
    "    def VectorSearch(self, word):\n",
    "        if self.nlp.vocab[word].has_vector == True:\n",
    "            vector_word = self.nlp(word)\n",
    "            for vw in self.vector_group_dict_list:\n",
    "                similarity_score = vector_word.similarity(vw)\n",
    "                if similarity_score >= 0.9:\n",
    "                    for w in vector_group_dict_list[vw]:\n",
    "                        print(w)\n",
    "\n",
    "    def CopyReplaceFolder(self, source_dir ,dest_dir , filename): \n",
    "        keyword = \"\"\n",
    "        if dest_dir == \"unknown\":\n",
    "            keyword = filename + \" in tech\"\n",
    "        else:\n",
    "            keyword = filename\n",
    "        self.ReClassificationSkillDictList(filename, keyword , {dest_dir})\n",
    "        if not os.path.exists(dest_dir):\n",
    "            os.makedirs(dest_dir)\n",
    "        source_path_doc = source_dir + \"/\" + filename + \".docx\"\n",
    "        source_path_html = source_dir + \"/\" + filename + \".html\"\n",
    "        destination_path_doc = dest_dir + \"/\" + filename + \".docx\"\n",
    "        destination_path_html = dest_dir + \"/\" + filename + \".html\"\n",
    "        if source_path_doc != destination_path_doc:\n",
    "            shutil.copyfile(source_path_doc, destination_path_doc)\n",
    "        if source_path_html != destination_path_html:\n",
    "            shutil.copyfile(source_path_html, destination_path_html)\n",
    "\n",
    "\n",
    "    def MakeDocsFromHtml(self):\n",
    "        dir = 'unclassified'\n",
    "        filenames = [f for f in listdir(dir) if isfile(join(dir, f))]\n",
    "        for f in filenames:\n",
    "            print(f)\n",
    "            words = f.rsplit(\".\")\n",
    "            extension = words[len(words)-1]\n",
    "            if extension ==\"html\":\n",
    "                filename = f.replace(\".html\",\"\")\n",
    "                output = pypandoc.convert_file(dir + \"/\" + f, 'docx', outputfile= dir + \"/\" + filename +\".docx\")\n",
    " \n",
    "    def DeleteAllSkillFile(self):\n",
    "        for dir in self.three_word_skill_classification_set:\n",
    "            if os.path.isdir(dir):\n",
    "                for filename in os.listdir(dir):\n",
    "                    file_path = os.path.join(dir, filename)\n",
    "                    try:\n",
    "                        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                            os.unlink(file_path)\n",
    "                        elif os.path.isdir(file_path):\n",
    "                            shutil.rmtree(file_path)\n",
    "                    except Exception as e:\n",
    "                        print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "        for dir in self.two_word_skill_classification_set:\n",
    "            if os.path.isdir(dir):\n",
    "                for filename in os.listdir(dir):\n",
    "                    file_path = os.path.join(dir, filename)\n",
    "                    try:\n",
    "                        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                            os.unlink(file_path)\n",
    "                        elif os.path.isdir(file_path):\n",
    "                            shutil.rmtree(file_path)\n",
    "                    except Exception as e:\n",
    "                        print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "        for dir in self.one_word_skill_classification_set:\n",
    "            if os.path.isdir(dir):\n",
    "                for filename in os.listdir(dir):\n",
    "                    file_path = os.path.join(dir, filename)\n",
    "                    try:\n",
    "                        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                            os.unlink(file_path)\n",
    "                        elif os.path.isdir(file_path):\n",
    "                            shutil.rmtree(file_path)\n",
    "                    except Exception as e:\n",
    "                        print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "        source_dir = 'skill'\n",
    "        destination_dir = 'unknown'\n",
    "        os.makedirs(destination_dir, exist_ok=True)\n",
    "\n",
    "        for file_name in os.listdir(source_dir):\n",
    "            source_file = os.path.join(source_dir, file_name)\n",
    "            destination_file = os.path.join(destination_dir, file_name)\n",
    "            shutil.copy(source_file, destination_file)\n",
    "\n",
    "        \n",
    "    def SkillReClassification(self): \n",
    "        self.backup_keyword_dict_list.clear()\n",
    "        for s in self.skill_dict_list:\n",
    "            self.backup_keyword_dict_list[s] = self.skill_dict_list[s].keyword_search\n",
    "        \n",
    "        self.skill_dict_list.clear()\n",
    "        self.group_dict_list.clear()\n",
    "        self.vector_group_dict_list.clear()\n",
    "        self.DeleteAllSkillFile()\n",
    "        h = html2text.HTML2Text()\n",
    "        h.ignore_links = False\n",
    "        h.inline_links = False\n",
    "        h.reference_links = True\n",
    "        dir = 'unknown'\n",
    "        filenames = [f for f in listdir(dir) if isfile(join(dir, f))]\n",
    "         \n",
    "        for f in filenames:\n",
    "            words = f.rsplit(\".\")\n",
    "            extension = words[len(words)-1]\n",
    "            if extension ==\"html\":\n",
    "                filename = f.replace(\".html\",\"\")\n",
    "                html_content = str(\"\")\n",
    "                with open(dir+\"/\"+ f, 'r', encoding=\"utf-8\") as file:\n",
    "                    html_content = file.read()\n",
    "                    file.close()\n",
    "                text_content = h.handle(html_content)\n",
    "                text_content = text_content.lower()\n",
    "                text_content = text_content.replace(\"[1]\",\"\")\n",
    "                text_content = text_content.replace(\"[2]\",\"\")\n",
    "                text_content = text_content.replace(\"[3]\",\"\")\n",
    "                text_content = text_content.replace(\"[4]\",\"\")\n",
    "                text_content = text_content.replace(\"[5]\",\"\")\n",
    "                text_content = text_content.replace(\"[6]\",\"\")\n",
    "                text_content = text_content.replace(\"[7]\",\"\")\n",
    "                text_content = text_content.replace(\"[8]\",\"\")\n",
    "                text_content = text_content.replace(\"[9]\",\"\")\n",
    "                text_content = text_content.replace(\"[0]\",\"\")\n",
    "                text_content = text_content.replace(\"[\",\"\")\n",
    "                text_content = text_content.replace(\"]\",\"\")\n",
    "                text_content = text_content.replace(\"(\",\"\")\n",
    "                text_content = text_content.replace(\")\",\"\")\n",
    "                text_content = text_content.replace(\"*\",\"\")\n",
    "                text_content = text_content.replace(\"\\\"\",\"\")\n",
    "                text_content = text_content.replace(\"’s\",\"\")\n",
    "                text_content = text_content.replace(\"!\",\"\")\n",
    "                text_content = text_content.replace(\":\",\"\")\n",
    "                text_content = text_content.replace(\",\",\"\")\n",
    "                text_content = text_content.replace(\"\\n\",\" \")\n",
    "                text_content = text_content.replace(\"/\",\" \")\n",
    "                text_content = text_content.replace(\"-\",\" \")\n",
    "              \n",
    "                words = text_content.split()\n",
    "                have_classific = False\n",
    "                for i in range(len(words)):\n",
    "                    first_word = words[i]\n",
    "                    if first_word.endswith('.'):\n",
    "                        first_word = first_word[:-1]\n",
    "                   \n",
    "                    one_word = first_word\n",
    "                    one_word = one_word.replace(\"microservices\",\"microservice\")\n",
    "                    one_word = one_word.replace(\"protocols\",\"protocol\")\n",
    "                    one_word = one_word.replace(\"networks\",\"network\")\n",
    "                    one_word = one_word.replace(\"website\",\"web\")\n",
    "                    one_word = one_word.replace(\"test\",\"testing\")\n",
    "                    one_word = one_word.replace(\"visualizations\",\"visualization\")\n",
    "                    one_word = one_word.replace(\"aws\",\"amazon\")\n",
    "        \n",
    "                    if one_word in self.one_word_skill_classification_set:\n",
    "                        self.CopyReplaceFolder(dir,one_word,filename)\n",
    "                        have_classific = True\n",
    "        \n",
    "                    if one_word ==\"ai\":\n",
    "                        one_word = \"artificial intelligence\"\n",
    "                        if one_word in self.two_word_skill_classification_set:\n",
    "                            self.CopyReplaceFolder(dir,one_word,filename)\n",
    "                            have_classific = True\n",
    "                    if one_word ==\"api\":\n",
    "                        one_word = \"application programming interface\"\n",
    "                        if one_word in self.three_word_skill_classification_set:\n",
    "                            self.CopyReplaceFolder(dir,one_word,filename)\n",
    "                            have_classific = True\n",
    "                    if one_word ==\"nlp\":\n",
    "                        one_word = \"natural language processing\"\n",
    "                        if one_word in self.three_word_skill_classification_set:\n",
    "                            self.CopyReplaceFolder(dir,one_word,filename)\n",
    "                            have_classific = True\n",
    "\n",
    "                    if i + 1 >= len(words):\n",
    "                        break\n",
    "                    second_word = words[i+1]\n",
    "                    if second_word.endswith('.'):\n",
    "                        second_word = second_word[:-1] \n",
    "                \n",
    "\n",
    "                    two_word = first_word + \" \" + second_word\n",
    "                    two_word = two_word.replace(\" servers\",\" server\")\n",
    "                    two_word = two_word.replace(\" services\",\" service\")\n",
    "                    two_word = two_word.replace(\" applications\",\" application\")\n",
    "                    two_word = two_word.replace(\" apps\",\" application\")\n",
    "                    two_word = two_word.replace(\" app\",\" application\")\n",
    "                    two_word = two_word.replace(\" databases\",\" database\")\n",
    "                    two_word = two_word.replace(\" machines\",\" machine\")\n",
    "                    two_word = two_word.replace(\"website\",\"web\")\n",
    "                      \n",
    "                    if two_word in self.two_word_skill_classification_set:\n",
    "                        self.CopyReplaceFolder(dir,two_word,filename)\n",
    "                        have_classific = True\n",
    "\n",
    "                    if i + 2 >= len(words):\n",
    "                        break\n",
    "                    third_word = words[i+2]\n",
    "                    if third_word.endswith('.'):\n",
    "                        third_word = third_word[:-1] \n",
    "                    three_word = first_word + \" \" + second_word + \" \" +  third_word\n",
    "                    if three_word in self.three_word_skill_classification_set:\n",
    "                        self.CopyReplaceFolder(dir,three_word,filename)\n",
    "                        have_classific = True\n",
    "        \n",
    "                   \n",
    "        \n",
    "                    \n",
    "                \n",
    "                if have_classific == True:\n",
    "                    file_path = dir +\"/\" + filename + \".html\"\n",
    "                    if os.path.isfile(file_path):\n",
    "                        os.remove(file_path)\n",
    "                    else:\n",
    "                        print(f\"The file {file_path} does not exist.\")\n",
    "                    file_path = dir +\"/\" + filename + \".docx\"\n",
    "                    if os.path.isfile(file_path):\n",
    "                        os.remove(file_path)\n",
    "                    else:\n",
    "                        print(f\"The file {file_path} does not exist.\")\n",
    "                else:\n",
    "                    self.ReClassificationSkillDictList(filename, filename + \" in tech\" , {\"unknown\"})\n",
    "                        \n",
    "        self.GroupTextVectorization()\n",
    "        self.ExportSkillDictList()\n",
    "        self.ExportGroupDictList()\n",
    "\n",
    "    def FindClassificationKeyword(self):\n",
    "        h = html2text.HTML2Text()\n",
    "        h.ignore_links = False\n",
    "        h.inline_links = False\n",
    "        h.reference_links = True\n",
    "        dir = 'unknown'\n",
    "        filenames = [f for f in listdir(dir) if isfile(join(dir, f))]\n",
    "        one_word_dict_list = {}\n",
    "        two_word_dict_list = {}\n",
    "        three_word_dict_list = {}\n",
    "        ignore_word_list = [\"a\",\"an\",\"the\",\"of\",\"on\",\"as\",\"by\",\"to\",\"with\",\"for\",\"is\",\"are\",\"was\",\"were\", \"in\"]\n",
    "        for f in filenames:\n",
    "            words = f.rsplit(\".\")\n",
    "            extension = words[len(words)-1]\n",
    "            if extension ==\"html\":\n",
    "                filename = f.replace(\".html\",\"\")\n",
    "                html_content = str(\"\")\n",
    "                with open(dir+\"/\"+ f, 'r', encoding=\"utf-8\") as file:\n",
    "                    html_content = file.read()\n",
    "                    file.close()\n",
    "                text_content = h.handle(html_content)\n",
    "                text_content = text_content.lower()\n",
    "                text_content = text_content.replace(\"[1]\",\"\")\n",
    "                text_content = text_content.replace(\"[2]\",\"\")\n",
    "                text_content = text_content.replace(\"[3]\",\"\")\n",
    "                text_content = text_content.replace(\"[4]\",\"\")\n",
    "                text_content = text_content.replace(\"[5]\",\"\")\n",
    "                text_content = text_content.replace(\"[6]\",\"\")\n",
    "                text_content = text_content.replace(\"[7]\",\"\")\n",
    "                text_content = text_content.replace(\"[8]\",\"\")\n",
    "                text_content = text_content.replace(\"[9]\",\"\")\n",
    "                text_content = text_content.replace(\"[0]\",\"\")\n",
    "                text_content = text_content.replace(\"[\",\"\")\n",
    "                text_content = text_content.replace(\"]\",\"\")\n",
    "                text_content = text_content.replace(\"(\",\"\")\n",
    "                text_content = text_content.replace(\")\",\"\")\n",
    "                text_content = text_content.replace(\"*\",\"\")\n",
    "                text_content = text_content.replace(\"\\\"\",\"\")\n",
    "                text_content = text_content.replace(\"’s\",\"\")\n",
    "                text_content = text_content.replace(\"!\",\"\")\n",
    "                text_content = text_content.replace(\":\",\"\")\n",
    "                text_content = text_content.replace(\",\",\"\")\n",
    "                text_content = text_content.replace(\"\\n\",\" \")\n",
    "                text_content = text_content.replace(\"/\",\" \")\n",
    "                text_content = text_content.replace(\"-\",\" \")\n",
    "                words = text_content.split()\n",
    "                for i in range(len(words)):\n",
    "                    first_word = words[i]\n",
    "                    if '1.' in first_word:\n",
    "                        break\n",
    "                    if first_word.endswith('.'):\n",
    "                        first_word = first_word[:-1]\n",
    "                    if first_word in ignore_word_list:\n",
    "                        continue\n",
    "                    one_word = first_word\n",
    "                    if one_word not in one_word_dict_list:\n",
    "                        one_word_dict_list[one_word] = 0\n",
    "                    one_word_dict_list[one_word]+=1\n",
    "                    \n",
    "                    second_word = words[i+1]\n",
    "                    if second_word.endswith('.'):\n",
    "                        second_word = second_word[:-1] \n",
    "                    if second_word in ignore_word_list:\n",
    "                        continue\n",
    "                    two_word = first_word + \" \" + second_word\n",
    "                    if two_word not in two_word_dict_list:\n",
    "                        two_word_dict_list[two_word] = 0\n",
    "                    two_word_dict_list[two_word]+=1\n",
    "\n",
    "                    third_word = words[i+2]\n",
    "                    if third_word.endswith('.'):\n",
    "                        third_word = third_word[:-1] \n",
    "                    if third_word in ignore_word_list:\n",
    "                        continue\n",
    "                    three_word = first_word + \" \" + second_word + \" \" +third_word\n",
    "                    if three_word not in three_word_dict_list:\n",
    "                        three_word_dict_list[three_word] = 0\n",
    "                    three_word_dict_list[three_word]+=1\n",
    "        with open('count one word.txt', 'w', encoding=\"utf-8\" ) as f:\n",
    "            for s in sorted(one_word_dict_list, key=one_word_dict_list.get, reverse=True):\n",
    "                f.write(str(s) + \" - \" + str(one_word_dict_list[s]))\n",
    "                f.write('\\n')\n",
    "            file.close()\n",
    "        with open('count two word.txt', 'w', encoding=\"utf-8\") as f:\n",
    "            for s in sorted(two_word_dict_list, key=two_word_dict_list.get, reverse=True):\n",
    "                f.write(str(s) + \" - \" + str(two_word_dict_list[s]))\n",
    "                f.write('\\n')\n",
    "            file.close()\n",
    "        with open('count three word.txt', 'w', encoding=\"utf-8\") as f:\n",
    "            for s in sorted(three_word_dict_list, key=three_word_dict_list.get, reverse=True):\n",
    "                f.write(str(s) + \" - \" + str(three_word_dict_list[s]))\n",
    "                f.write('\\n')\n",
    "            file.close()\n",
    "          \n",
    "    \n",
    "                 \n",
    "            \n",
    "    def AllThisWillBeRemoveOnceFinalize(self):\n",
    "        \n",
    "        self.exact_match_replace_dict_list[\"aws\"]=\"amazon web services\"\n",
    "        self.exact_match_replace_dict_list[\"tdd\"]=\"testing\"\n",
    "        self.exact_match_replace_dict_list[\"webdriver\"]=\"web crawler\"\n",
    "        self.exact_match_replace_dict_list[\"vbnet\"]=\"visual basic .net\"\n",
    "        self.exact_match_replace_dict_list[\"vb.net\"]=\"visual basic .net\"\n",
    "        self.exact_match_replace_dict_list[\"vb\"]=\"visual basic\"\n",
    "        self.exact_match_replace_dict_list[\"html5\"]=\"html\"\n",
    "        self.exact_match_replace_dict_list[\"svn\"]=\"subversion\"\n",
    "        self.exact_match_replace_dict_list[\"rdbms\"]=\"relational\"\n",
    "        self.exact_match_replace_dict_list[\"unity3d\"]=\"unity\"\n",
    "        self.exact_match_replace_dict_list[\"mssql\"]=\"microsoft sql\"\n",
    "        self.exact_match_replace_dict_list[\"shaders\"]=\"shader\"\n",
    "        self.exact_match_replace_dict_list[\"uat\"]=\"testing\"\n",
    "        self.exact_match_replace_dict_list[\"mui\"]=\"material ui\"\n",
    "        self.exact_match_replace_dict_list[\"gui\"]=\"graphical user interface\"\n",
    "        self.exact_match_replace_dict_list[\"ui\"]=\"user interface\"\n",
    "        \n",
    "        self.partial_match_replace_dict_list[\"ms\"]=\"microsoft\"\n",
    "        self.partial_match_replace_dict_list[\"system\"]=\"systems\"\n",
    "        self.partial_match_replace_dict_list[\"window\"]=\"windows\"\n",
    "        self.partial_match_replace_dict_list[\"databases\"]=\"database\"\n",
    "        self.partial_match_replace_dict_list[\"website\"]=\"web\"\n",
    "        self.partial_match_replace_dict_list[\"test\"]=\"testing\"\n",
    "        self.partial_match_replace_dict_list[\"networking\"]=\"network\"\n",
    "        self.partial_match_replace_dict_list[\"solarwinds\"]=\"solarwind\"\n",
    "        \n",
    "        self.ExportMatchReplaceDictList()\n",
    "     \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ee87e14-341a-414f-af4d-ce82b3d82d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = TechStack()\n",
    "test.SkillReClassification()\n",
    "test.FindClassificationKeyword()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e3fd68-61e5-4463-8050-07d85b44cc30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3ea5afc-91a8-450e-b689-2f8610dbd841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'etl', 'qt', 'datax', 'stack', 'sprint', 'cloudstack', 'flink', 'web3.js', 'qilkview', 'storybook', 'liquibase', 'geneos', 'vert.x', 'meteor', 'signalr', 'syslog', 'hadoop', 'powershell', 'sqoop', 'glassfish', 'boost', 'charts.js', 'cloudflare', 'jena', 'nagios', 'plc', 'sr sam 34 35', 'posix', 'dbaas', 'gatling', 'rnn', 'gstreamer', 'udb', 'checkmarx', 'caffe', 'jee', 'kustomize', 'jupyter', 'mstest', 'scalding', 'hyperv', 'geojson', 'cpanel', 'dataframe', 'glusterfs', 'mobx', 'bigtable', 'cognito', 'loopback', 'microsoft', 'rwd', 'mapreduce', 'xslt', 'mesos', 'graphql', 'mapbox', 'knime', 'matlab', 'red hat fuse', 'kinesis', 'koin', 'uml', 'pyqt', 'reactivex', 'maximo', 'caches', 'css', 'aiops', 'objective c', 'filebeat', 'factory', 'xhtml', 'tensorflow', 'efs', 'yaml', 'vb script', 'browserstack', 'core data', 'ast', 'elasticsearch', 'linux', 'xcode', 'viz', 'stm32wl', 'jaeger', 'websphere', 'appcheck', 'glide', 'google', 'ansible', 'winform', 'dubbo', 'kafka', 'sybase', 'oop', 'sqlite', 'rollup.js', 'riak', 'cucumber', 'jetty', 'sisense', 'sparkml', 'grunt', 'fortify', 'xmlrpc', 'ejb', 'postgis', 'mercurial', 'sysdig', 'xen', 'open vpn', 'relational', 'pycharm', 'codeigniter', 'tosca', 'gitlab', 'dart', 'maxwell', 'hdp', 'unix', 'openshift', 'magento', 'scrapy', 'osgi', 'rds', 'saml', 'memcached', 'garden', 'titan', 'tfx', 'h2o', 'bigquery', 'aws cdk', 'es6', 'openid', 'voldemort', 'immutable.js', 'devexpress', 'high charts', 'mailgun', 'aws appsync', 'alfresco', 'tcpflow', 'ceph', 'vmware', 'lambda', 'scss', 'kibana', 'htmlunit', 'php', 'spring cloud', 'dom', 'nosql', 'dl4j', 'cosmodb', 'weka', 'spss', 'jest', 'sqlalchemy', 'snowflake', 'mcv', 'podman', 'corda', 'vpc', 'c', 'c4.js', 'soa', 'artifactory', 'spark', 'ui automator', 'druid', 'hashicorp', 'attunity', 'ibatis', 'glacier', 'sailsjs', 'orm', 'greensock', 'electron', 'ado.net', 'stylus', 'laravel', 'airflow', 'xamarin', 'codepush', 'lit', 'gemalto', 'keras', 'sass', 'amplify', 'gherkin', 'icinga', 'jasper', 'mlib', 'xml', 'django', 'eks', 'guava', 'siebel', 'bazel', 'dotnetnuke', 'oauth', 'wsk', 'chef', 'etcd', 'bitrise', 'salt', 'netbeans', 'jmeter', 'vagrant', 'codebuild', 'gatsby', 'erlang', 'rackspace', 'jenkins', 'grails', 'core animation', 'seaborn', 'symfony', 'butterknife', 'grpc', 'dataproc', 'elastic', 'nextjs', 'tcp', 'eslint', 'codepipeline', 'xmpp', 'grafana', 'mechanicalsoup', 'featherjs', 'activiti', 'raspberry pi', 'knative', 'junit', 'aks', 'amazon', 'odata', 'ssis', 'wsdl', 'labview', 'awk', 'azure', 'rxkotlin', 'cakephp', 'assertj', 'titanium', 'jobserver', 'dhtml', 'kedro', 'zeplin', 'appdynamics', 'mvt', 'pig', 'inversify', 'hbase', 'sinonjs', 'percona xtradb', 'testrail', 'vsts', 'pytorch', 'sonatype nexus', 'pojo', 'hpux', 'coredns', 'karaf', 'anaconda', 'puppet', 'knockoutjs', 'ecs', 'activemq', 'vault', 'oracle', 'twistlock', 'whitesource', 'bower', 'oauth2', 'dax', 'nuxtjs', 'telegraf', 'less', 'istio', 'flutter', 'ssrs', 'nixos', 'postman', 'perl', 'openvz', 'analytics', 'jmx', 'rxswift', 'json', 'phpunit', 'flume', 'control m', 'packer', 'pwa', 'ranorex', 'odoo', 'sql', 'spotfire', 'babel', 'react', 'http', 'mule', 'mvrx', 'lake formation', 'nethereum', 'canvasjs', 'ignite', 'informatica', 'webgl', 'dremio', 'swift', 'axway', 'socketio', 'zend', 'tibco', 'qlik', 'android studio', 'talend', 'sketch', 'opencv', 'casperjs', 'clojure', 'asp.net', 'ecmascript', 'security', 'lucene', 'intellij', 'sentry', 'prototype', 'gephi', 'redshift', 'beautiful soup', 'gradle', 'opentsdb', 'mockito', 'couchdb', 'webpack', 'vue.js', 'webhooks', 'node.js', 'experian', 'msmq', 'spock', 'visual basic .net', 'squid proxy', 'clickhouse', 'documentdb', 'ant', 'spring boot', 'structs', 'opencart', 'okhttp', 'gogs', 'wordpress', 'server', 'azure data lake', 'kms', 'jndi', 'cnn', 'elementor', 'antd', 'sonarlint', 'data mart', 'graylog', 'consul', 'maven', 'databricks', 'appium', 'python', 'gocd', 'npm', 'windows', 'asyncio', 'phpspec', 'cft', 'pl sql', 'lightgbm', 'presto', 'validata qs', 'terraform', 'uikit', 'fiddler', 'glue', 'elasticip', 'wildfly', 'extjs', 'varnish', 'teamcity', 'websockets', 'chakra ui', 'dynamodb', 'metastore', 'luigi', 'lerna', 'ble', 'jade template', 'hd insights', 'beelin', 'jpa', 'ide', 'buildkite', 'jquery', 'r', 'conan', 'flask', 'drupal', 'dbeaver', 'cntk', 'ubuntu', 'mermaid', 'google cloud', 'nativescript', 'zabbix', 'typescript', 'avro', 'envoy', 'jboss', 'virtuozzo', 'samza', 'camunda', 'datarobot', 'pug', 'sns', 'superset', 'hal', 'nunit', 'x-pack', 'swagger', 'robot', 'android sdk', 'solace', 'strategy', 'datalake', 'alamofire', 'openapi', 'tableau', 'influxdb', 'maxscale', 'alerta', 'swarm', 'afnetworking', 'qubole', 'theano', 'durandaljs', 'protractor', 'jdbc', 'oidc', 'git', 'parquet', 'perforce', 'aws sam', 'microstrategy', 'nhibernate', 'webrtc', 'google compute engine', 'fluentd', 'mongodb', 'realm', 'cloudfront', 'gwt', 'svelte', 'ses', 'splunk', 'scylladb', 'firebase', 'go', 'aurora', 'power bi', 'pytest', 'ovirt', 'ajax', 'kaggle', 'mariadb', 'kapacitor', 'coroutines', 'api gateway', 'ada', 'pillow', 'haproxy', 'javaee', 'ggplot', 'sparksql', 'oracle sql', 'birt report', 'ehcache', 'gulp', 'esri-leaflet', 'jslint', 'kotlin', 'rapidminer', 'hive', 'phaser', 'postcss', 'mvicore', 'athena', 'cordova', 'kubernetes', 'jruby', 'c++', 'mvc', 'android', 'cloudfoundry', 'scikit', 'sparkr', 'debian', 'udp', 'query', 'j2se', 'jasmine', 'ws02 apim', 'elasticcache', 'wpbakery', 'rpc', 'locust', 'aws device farm', 'rundeck', 'graph', 'play', 'prometheus', 'amazon web services', 'hysterix', 'pinpoint', 'microservice', 'bash', 'pyspark', 'javascript', 'hdfs', 'elb', 'periscope', 'playcanvas', 'impala', 'silverlight', 'testing', 'cloudera', 'redux', 'chai', 'fargate', 'phabricator', 'scikit-image', 'zipkin', 'nginx', 'cmake', 'docker', 'fastapi', 'fabric', 'paw', 'konga', 'liferay', 'mahout', 's3', 'respondjs', 'jsp', 'cognos tm1', 'amqp', 'jmp', 'hortonworks', 'ibm', 'ios', 'gemnasium', 'react native', 'ipython', 'pixijs', 'java', 'logstash', 'neptune', 'backbone', 'queue', 'essbase', 'retrofit 2', 'messagepack', 'vba', 'route53', 'mxnet', 'rabbitmq', 'rust', 'jshint', 'pyspider', 'httpunit', 'shell script', 'alluxio', 'workbench', 'iis', 'kvm', 'jfrog', 'unity', 'redis', 'dask', 'ec2', 'clearquest', 'geronimo', 'buildforge', 'restassured', 'vercel', 'netezza', 'bugzilla', 'xarray', 'amcharts', 'mybatis', 'fedora', 'jms', 'boomi', 'es5', 'stata', 'db2', 'axios', 'composer', 'lando', 'new relic', 'octave', 'elk', 'struts', 'cassandra', 'circleci', 'ldap', 'esp32', 'visual studio', 'jinja', 'matplotlib', 'knn', 'arkit', 'd3', 'es7', 'appkit', 'jwe', 'vertica', 'haskell', 'directx', 'cesiumjs', 'tfs', 'phonegap', 'ambari', 'elixir', 'aspx', 'solaris', 'dagger', 'celery', 'undertow', 'mobile', '.net core', 'hazelcast', 'linkerd', 'newsql', 'tiered', 'polly.js', 'delta lake', 'datadog', 'asp.mvc', 'apache', 'ftp', 'nifi', 'jax-rs', 'alteryx', 'clearcase', 'camel', 'clair', 'groovy', 'message queue', 'mqtt', 'jsf', 'pax', 'eventbus', 'akka', 'dynatrace', 'specflow', 'plotly', 'kdb', 'zookeeper', 'ruby', 'visual basic', 'weblogic', 'ionic', 'argocd', 'quorum', 'jamstack', 'vreazlise', 'aquasec', 'behat', 'emr', 'shiro', 'datastage', 'mootools', 'azkaban', 'data extraction', 'nuget', 'zenoss', 'ios sdk', 'pcf', 'cxf', 'gurobi', 'teradata', 'heroku', 'blazor', 'strata', 'windows server', 'digitalocean', 'three.js', 'scala', 'material ui', 'oozie', 'sap', 'parcel', 'iam', 'papertrail', 'opsgenie', 'markdown', 'carthage', 'helm', 'mysql', 'ocaml', 'kudu', 'synapse', 'calabash', 'singleton', 'filezilla', 'rhel', 'jakarta ee', 'crystal', 'vsphere', 'crashlytics', 'drools', 'tanzu', 'wicket', 'c#', 'neo4j', 'mvvm', 'dapper', 'axiom', 'pagerduty', 'fastlane', 'qliksense', 'testng', 'flaskapi', 'api', '.net', 'openssl', 'ie10+', 'vkey', 'koajs', 'storm', 'gke', 'selenium', 'codedeploy', 'sas', 'solr', 'esxi', 'cplex', 'datastax', 'rancher', 'foundationdb', 'sagemaker', 'vanillajs', 'rxjava', 'intelij idea', 'dataset', 'cypress', 'spacy', 'tomcat', 'sensu', 'axis', 'tensorrt', 'coreos', 'streamsets', 'figma', 'hilt', 'redux-saga', 'octopus deploy', 'apigee', 'gemfire', 'buddy', 'scipy', 'dataflow', 'elastalert', 'soap', 'greenplum', 'yugabytedb', 'netegrity', 'hana', 'ebs', 'xsd', 'ethereum', 'reactivecocoa', 'wcf', 'beam', 'jwt', 'zigbee', 'rspec', 'blackduck', 'postgresql', 'web', 'bsd', 'wpf', 'sqs', 'fcm', 'soapui', 'nestjs', 'ci', 'pandas', 'mapr', 'geopandas', 'ribbon', 'cloudformation', 'j2ee', 'aerospike', 'mixpanel', 'nomad', 'instana', 'html', 'babylon.js', 'sonar qube', 'kubeflow', 'bootstrap', 'skimage', 'hibernate', 'qlikview', 'shell', 'rvest', 'numpy', 'f#', 'opengl', 'timescaledb', 'orc', 'sonarcloud', 'dataiku', 'cloudwatch', 'centos', 'flow', 'geoserver', 'firestore', 'servlets', 'cocoa touch', 'mojolicious', 'spinnaker', 'dds', 'angular', 'xampp', 'zuul', 'web crawler', 'metricbeat', 'nats', 'portainer', 'yarn', 'ruby on rails', 'spring', 'pullreview', 'expressjs', 'ood', 'statsd', 'openstack', 'code commit', 'subversion', 'sling', 'xgboost', 'toad', 'kylin', 'bitbucket', 'pentaho', 'leakcanary', 'sockets', 'autoprefixer', 'es2015', 'ember', 'jsdoc', 'toplink', 'cloud'}\n",
      "Document output sucessfully.\n"
     ]
    }
   ],
   "source": [
    "#test = TechStack()\n",
    "f = open(\"nodeflair skill.txt\", \"r\")\n",
    "skills = set()\n",
    "for c in f:\n",
    "    c = c.replace(\"\\n\", \"\")\n",
    "    if c == 'x':\n",
    "        continue\n",
    "    skills.add(c)\n",
    "f.close\n",
    "result = test.GenerateLearningResource(None, skills)\n",
    "test.ExportNotFoundSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc6e9d6-5b6b-48ba-a152-71f5b2d11f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#openauth not found in unknown/openauth.html\n",
    "#elastic bean stalk not found in unknown/elastic bean stalk.html\n",
    "#https://stackoverflow.com/questions/75475470/how-to-extract-the-all-hyperlink-and-their-text-from-a-word-document-using-pytho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "242a69d0-06d5-4df6-a57f-866c3b58c8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "![C++][1]\n",
      "\n",
      "![C++][2]\n",
      "\n",
      "Explore\n",
      "\n",
      "Certainly! **C++** is a **cross-platform programming language** that extends\n",
      "the capabilities of the C language, providing high control over system\n",
      "resources and memory. [It’s widely used for creating high-performance\n",
      "applications, operating systems, and embedded systems][3][1][3][2][4][3][5].\n",
      "\n",
      "Here are **five free resources** where you can learn C++:\n",
      "\n",
      "  1. [****][3]**[W3Schools C++ Introduction][3]** : This tutorial covers the basics of C++, including syntax, variables, and development[1][3].\n",
      "  2. [****][3]**[LearnCpp.com][6]** : A comprehensive website with step-by-step tutorials, examples, and quizzes to help you master C++ programming[4][6].\n",
      "  3. [****][3]**[Programiz C++ Tutorial][7]** : Offers interactive lessons, examples, and references for learning C++[5][7].\n",
      "  4. [****][3]**[Codecademy C++ Course][8]** : A beginner-friendly course that covers C++ essentials for software development[6][8].\n",
      "  5. **Official C++ Documentation** : The official documentation provides in-depth information about C++ features, syntax, and libraries. You can find it on the C++ Standard website.\n",
      "\n",
      "Happy learning! 🚀👩‍💻\n",
      "\n",
      "   [1]:\n",
      "https://www.bing.com/th?id=OSK.830992e6b8f0c7bc66cd3d6fa3db36b4&pid=cdx&w=320&h=189&c=7&rs=1\n",
      "\n",
      "   [2]:\n",
      "https://www.bing.com/th?id=OSK.830992e6b8f0c7bc66cd3d6fa3db36b4&pid=cdx&w=168&h=189&c=7\n",
      "\n",
      "   [3]: https://www.w3schools.com/cpp/cpp_intro.asp\n",
      "\n",
      "   [4]: https://en.wikipedia.org/wiki/C%2B%2B\n",
      "\n",
      "   [5]: https://www.geeksforgeeks.org/introduction-to-c-programming-language/\n",
      "\n",
      "   [6]: https://www.learncpp.com/\n",
      "\n",
      "   [7]: https://www.programiz.com/cpp-programming\n",
      "\n",
      "   [8]: https://www.codecademy.com/learn/learn-c-plus-plus\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import html2text\n",
    "\n",
    "html_content = str(\"\")\n",
    "with open('unknown/c++.html', 'r', encoding=\"utf-8\") as file:\n",
    "    html_content = file.read()\n",
    "file.close()\n",
    "\n",
    "h = html2text.HTML2Text()\n",
    "\n",
    "h.ignore_links = False\n",
    "h.inline_links = False\n",
    "h.reference_links = True\n",
    "\n",
    "# Convert HTML to text with separated links\n",
    "text_content = h.handle(html_content)\n",
    "\n",
    "print(text_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62e3f713-9098-41d2-ac39-22929b5945c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'flask'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflask\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Flask, request, jsonify, send_file, after_this_request\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mzipfile\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'flask'"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify, send_file, after_this_request\n",
    "import zipfile\n",
    "import os\n",
    "from io import BytesIO\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/get_zip', methods=['GET'])\n",
    "def get_zip():\n",
    "    # Retrieve parameters from the GET request\n",
    "    param1 = request.args.get('param1')\n",
    "    param2 = request.args.get('param2')\n",
    "\n",
    "    # Create a zip file in memory\n",
    "    memory_file = BytesIO()\n",
    "    with zipfile.ZipFile(memory_file, 'w') as zf:\n",
    "        # Add files to the zip file using the parameters\n",
    "        zf.writestr(f'{param1}.txt', f'Content for {param1}')\n",
    "        zf.writestr(f'{param2}.txt', f'Content for {param2}')\n",
    "    memory_file.seek(0)\n",
    "\n",
    "    # Define a function to remove the zip file after sending it\n",
    "    @after_this_request\n",
    "    def remove_file(response):\n",
    "        try:\n",
    "            os.remove(zip_path)\n",
    "        except Exception as error:\n",
    "            app.logger.error(\"Error removing or closing downloaded file handle\", error)\n",
    "        return response\n",
    "\n",
    "    # Send the zip file\n",
    "    response = send_file(memory_file, attachment_filename='files.zip', as_attachment=True)\n",
    "\n",
    "    # Return the JSON response with the download link\n",
    "    return jsonify({'success': True, 'message': 'Files are ready for download', 'download_link': '/get_zip'})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cba39eb0-2936-4fa2-a1ed-a2e88a6baf98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4c4910-cb60-4ca0-b94f-8bf01f758b75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
