{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a25ea2e0-da69-43df-ae53-d256c9ca2e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "import pandas as pd\n",
    "import pypandoc\n",
    "from pypandoc.pandoc_download import download_pandoc\n",
    "#download_pandoc()\n",
    "import csv\n",
    "import os\n",
    "from pathlib import Path\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import shutil\n",
    "import docx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "488066d5-b842-4fbe-9350-5056a0692885",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skill:\n",
    "    def __init__(self,name,dir, keyword , groups=None,prerequisites= None):\n",
    "        filename = name\n",
    "        filename = filename.replace('/','-')\n",
    "        filename = filename.replace(\"\\\\\",'-')\n",
    "        filename = filename + \".html\"\n",
    "        path = os.path.join(dir, filename)\n",
    "        path = path.replace(\"\\\\\",'/')\n",
    "        self.resource_path = path # for the resource path\n",
    "        self.keyword_search = keyword  # keyword for searching LLM\n",
    "        self.group_set = set()\n",
    "        if groups is not None:    \n",
    "            self.UpdateGroupSet(groups)\n",
    "        \n",
    "    def UpdateGroupSet(self,groups):\n",
    "        self.group_set.update(groups)\n",
    "        #print(\"skill group set updated.\")\n",
    "\n",
    "    def ChangeKeyword(self,keyword):\n",
    "        self.keyword_search = keyword\n",
    "\n",
    "    def ChangePath(self,name,dir):\n",
    "        filename = name\n",
    "        filename = filename.replace('/','-')\n",
    "        filename = filename.replace(\"\\\\\",'-')\n",
    "        filename = filename + \".html\"\n",
    "        path = os.path.join(dir, filename)\n",
    "        path = path.replace(\"\\\\\",'/')\n",
    "        self.resource_path = path\n",
    "      \n",
    "        \n",
    "class Group:\n",
    "    def __init__(self,name,skills):\n",
    "        filename = name\n",
    "        filename = filename.replace('/','-')\n",
    "        filename = filename.replace(\"\\\\\",'-')\n",
    "        filename = filename + \".html\"\n",
    "        path = os.path.join(\"group\", filename)\n",
    "        path = path.replace(\"\\\\\",'/')\n",
    "        self.resource_path = path # for the resource path\n",
    "        self.keyword_search = name + \" in tech\" # keyword for searching LLM\n",
    "        self.skill_set = skills\n",
    "\n",
    "    def UpdateSkillSet(self,skill):\n",
    "        self.skill_set.update(skill)\n",
    "        #print(\"group skill set updated.\")\n",
    "\n",
    "    def ChangeKeyword(self,keyword):\n",
    "        self.keyword_search = keyword\n",
    "\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b9b981e-d994-41b8-81d8-8ff213a69c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TechStack:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load('en_core_web_md')\n",
    "        self.skill_dict_list = {}\n",
    "        self.group_dict_list = {}\n",
    "        self.exact_match_replace_dict_list = {}\n",
    "        self.partial_match_replace_dict_list = {}\n",
    "        self.vector_group_dict_list = {}\n",
    "        self.ignore_set = set()\n",
    "        self.not_found_dict_list = {}\n",
    "        self.document_pepare_set = set()\n",
    "        self.three_word_skill_classification_set =set()\n",
    "        self.two_word_skill_classification_set =set()\n",
    "        self.one_word_skill_classification_set =set()\n",
    "        self.backup_keyword_dict_list={}\n",
    "        self.ImportIgnoreSet()\n",
    "        self.AllThisWillBeRemoveOnceFinalize()\n",
    "        self.ImportClassificationSet()\n",
    "        self.ImportSkillDictList()\n",
    "        self.GroupTextVectorization()\n",
    "\n",
    "               \n",
    "    def AddSkillDictList(self,name,path,keyword,groups=None):\n",
    "        if name not in self.skill_dict_list:\n",
    "            self.skill_dict_list[name] = Skill(name,path,keyword,groups)\n",
    "            #print(name,\"added in skill_dict_list.\")\n",
    "            if groups is not None:\n",
    "                for g in groups:\n",
    "                    if g in self.group_dict_list:\n",
    "                        self.group_dict_list.get(g).UpdateSkillSet({name})\n",
    "                        #print(name,\"added in\",g,\".\")\n",
    "                    else:\n",
    "                        self.group_dict_list[g] = Group(g,{name})\n",
    "                        #print(\"new group:\",g,\"have been created and added\",name,\".\")\n",
    "        else:\n",
    "            self.UpdateSkillDictList(name,groups)\n",
    "\n",
    "    def ReClassificationSkillDictList(self,name,path,keyword,groups):\n",
    "        search_keyword = keyword\n",
    "        if name in self.backup_keyword_dict_list:\n",
    "            search_keyword = self.backup_keyword_dict_list[name]\n",
    "        self.AddSkillDictList(name,path,search_keyword,groups)\n",
    "       \n",
    "                    \n",
    "    def UpdateSkillDictList(self,name,groups):\n",
    "        if name in self.skill_dict_list:\n",
    "            self.skill_dict_list[name].UpdateGroupSet(groups)\n",
    "\n",
    "    def AddGroupDictList(self,name,skills):\n",
    "        if skills is not None:\n",
    "            if name in self.group_dict_list:\n",
    "                self.UpdateGroupDictList(name,skills)\n",
    "            else:\n",
    "                found_set = set()\n",
    "                for s in skills:\n",
    "                    if s in self.skill_dict_list:\n",
    "                        self.skill_dict_list[s].UpdateGroupSet({name})\n",
    "                        found_set.add(s)  \n",
    "                        #print(s,\"added in\",name,\"group set.\")\n",
    "                self.group_dict_list[name] = Group(name,found_set)\n",
    "\n",
    "    def UpdateGroupDictList(self,name,skills):\n",
    "        if name in self.group_dict_list:\n",
    "            found_set = set()\n",
    "            for s in skills:\n",
    "                if s in self.skill_dict_list:\n",
    "                      found_set.add(s)  \n",
    "            self.group_dict_list[name].UpdateSkillSet(found_set)\n",
    "        else:\n",
    "            self.AddGroupDictList(name,skills)\n",
    "\n",
    "    def AddNotFoundDictList(self,name,keyword):\n",
    "        if name not in self.not_found_dict_list:\n",
    "            path = \"unclassified\"\n",
    "            self.not_found_dict_list[name] =  Skill(name,path,keyword,None)\n",
    "\n",
    "    def UpdateUnknownGroupInSkillDictList(self,name,new_group):\n",
    "        if name in self.skill_dict_list:\n",
    "            self.skill_dict_list[name].group_set.discard(\"unknown\")\n",
    "            self.skill_dict_list[name].group_set.add(new_group)\n",
    "        if \"unknown\" in self.group_dict_list:\n",
    "            self.group_dict_list[\"unknown\"].skill_set.discard(name)\n",
    "        if new_group in self.group_dict_list:\n",
    "            self.group_dict_list[new_group].skill_set.add(name)\n",
    "            \n",
    "    def UpdateSkillDictPath(self, name, new_dir):\n",
    "        print(name,new_dir)\n",
    "        if name in self.skill_dict_list:\n",
    "            self.skill_dict_list[name].ChangePath(name,new_dir)\n",
    "\n",
    "    def ImportIgnoreSet(self):\n",
    "        f = open(\"ignore.txt\", \"r\")\n",
    "        for c in f:\n",
    "            c = c.replace(\"\\n\", \"\")\n",
    "            self.ignore_set.add(c)\n",
    "        f.close()\n",
    "\n",
    "    def ImportClassificationSet(self):\n",
    "        f = open(\"three word skill classification.txt\", \"r\")\n",
    "        for l in f:\n",
    "            l = l.replace(\"\\n\", \"\")\n",
    "            self.three_word_skill_classification_set.add(l)\n",
    "        f.close()\n",
    "        f = open(\"two word skill classification.txt\", \"r\")\n",
    "        for l in f:\n",
    "            l = l.replace(\"\\n\", \"\")\n",
    "            self.two_word_skill_classification_set.add(l)\n",
    "        f.close()\n",
    "        f = open(\"one word skill classification.txt\", \"r\")\n",
    "        for l in f:\n",
    "            l = l.replace(\"\\n\", \"\")\n",
    "            self.one_word_skill_classification_set.add(l)\n",
    "        f.close()\n",
    "        \n",
    "    def ExportSkillDictList(self):\n",
    "        file_path = \"skills.csv\"\n",
    "        with open(file_path, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Name\", \"Search Keyword\",\"Resource Path\",\"Groups\"])\n",
    "            for key, value in self.skill_dict_list.items():\n",
    "                name=key\n",
    "                search = value.keyword_search\n",
    "                path = value.resource_path\n",
    "                groups =\"\"\n",
    "            \n",
    "                for g in value.group_set:\n",
    "                    groups += \"[\"\n",
    "                    groups += g\n",
    "                    groups +=\"]\"\n",
    "             \n",
    "                writer.writerow([name,search,path,groups])\n",
    "            file.close()\n",
    "        with open('skills.txt', 'w') as f:\n",
    "            for i in self.skill_dict_list:\n",
    "                f.write(i)\n",
    "                f.write(\"\\n\")\n",
    "            f.close()\n",
    "            \n",
    "    def ImportSkillDictList(self):\n",
    "        df = pd.read_csv(\"skills.csv\")\n",
    "        for index, row in df.iterrows():\n",
    "            name = str(row['Name'])\n",
    "            keyword = str(row['Search Keyword'])\n",
    "            path = str(row['Resource Path'])\n",
    "            dir_path = os.path.dirname(path)\n",
    "            groups = str(row['Groups'])\n",
    "            groups_set = None\n",
    "            groups = groups.replace('[', '')\n",
    "            groups_list = groups.split(']')\n",
    "            if len(groups_list) > 0 :\n",
    "                groups_list = groups_list[:-1]\n",
    "                groups_set = set()\n",
    "                for g in groups_list:\n",
    "                    groups_set.add(g)\n",
    "            # auto create group also\n",
    "            self.AddSkillDictList(name,dir_path,keyword,groups_set)\n",
    "                \n",
    "\n",
    "        \n",
    "    \n",
    "    def ExportGroupDictList(self):\n",
    "        file_path = \"groups.csv\"\n",
    "        with open(file_path, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Name\", \"Search Keyword\",\"Resource Path\",\"skills\"])\n",
    "            for key, value in self.group_dict_list.items():\n",
    "                name=key\n",
    "                search = value.keyword_search\n",
    "                path = value.resource_path\n",
    "                skills =\"\"\n",
    "                for s in value.skill_set:\n",
    "                    skills += \"[\"\n",
    "                    skills += s\n",
    "                    skills +=\"]\"\n",
    "                writer.writerow([name,search,path,skills])\n",
    "            file.close()\n",
    "\n",
    "\n",
    "\n",
    "    def ExportMatchReplaceDictList(self):\n",
    "        file_path = \"exact match.csv\"\n",
    "        with open(file_path, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Word\", \"Replace\"])\n",
    "            for key, value in self.exact_match_replace_dict_list.items():\n",
    "                writer.writerow([key,value])\n",
    "            file.close()\n",
    "        file_path = \"partial match.csv\"\n",
    "        with open(file_path, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Word\", \"Replace\"])\n",
    "            for key, value in self.partial_match_replace_dict_list.items():\n",
    "                writer.writerow([key,value])\n",
    "            file.close()\n",
    "\n",
    "\n",
    "\n",
    "    def ExportNotFoundSet(self):\n",
    "        file_path = \"not found.csv\"\n",
    "        with open(file_path, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Name\", \"Search Keyword\",\"Resource Path\",\"Groups\"])\n",
    "            for key, value in self.not_found_dict_list.items():\n",
    "                name=key\n",
    "                search = value.keyword_search\n",
    "                path = value.resource_path\n",
    "             \n",
    "                writer.writerow([name,search,path,\"\"])\n",
    "            file.close()\n",
    "     \n",
    "\n",
    "    def Filter(self, text):\n",
    "        text = text.lower()\n",
    "        text = text.replace(\"/\",\" \")\n",
    "        if text.find('(') != -1:\n",
    "            text = text.split(\"(\")[0]\n",
    "            text = text.rsplit()[0]\n",
    "            \n",
    "        words = text.split()\n",
    "    \n",
    "        if text in self.ignore_set:\n",
    "            return str(\"\")\n",
    "            \n",
    "        if text in self.exact_match_replace_dict_list:\n",
    "            text = self.exact_match_replace_dict_list.get(text)\n",
    "            \n",
    "        words = text.split()\n",
    "        new_text = \"\"\n",
    "        for word in words:\n",
    "            if word in self.partial_match_replace_dict_list:\n",
    "                new_text += self.partial_match_replace_dict_list.get(word) \n",
    "                new_text +=\" \"\n",
    "            else:\n",
    "                new_text += word \n",
    "                new_text +=\" \"\n",
    "        return new_text[:-1]\n",
    "\n",
    "    def Search(self,text):\n",
    "        if text in self.skill_dict_list:\n",
    "            self.document_pepare_set.add(text)\n",
    "            return True\n",
    "        if text in self.group_dict_list:\n",
    "            self.document_pepare_set.add(text)\n",
    "            return True\n",
    "        # check for . - space and .js js\n",
    "        for sdl in  self.skill_dict_list:\n",
    "            check1 = sdl\n",
    "            check1 = check1.replace(\".\",\"\")\n",
    "            check2 = text\n",
    "            check2 = check2.replace(\".\",\"\")\n",
    "            if check1 == check2:\n",
    "                self.document_pepare_set.add(sdl)\n",
    "                return True\n",
    "            check1 = sdl\n",
    "            check1 = check1.replace(\"-\",\" \")\n",
    "            check2 = text\n",
    "            check2 = check2.replace(\"-\",\" \")\n",
    "            if check1 == check2:\n",
    "                self.document_pepare_set.add(sdl)\n",
    "                return True\n",
    "            check1 = sdl\n",
    "            check1 = check1.replace(\" \",\"\")\n",
    "            check2 = text\n",
    "            check2 = check2.replace(\" \",\"\")\n",
    "            if check1 == check2:\n",
    "                self.document_pepare_set.add(sdl)\n",
    "                return True\n",
    "            check1 = sdl\n",
    "            check1 = check1.replace(\".js\",\"\")\n",
    "            check1 = check1.replace(\"js\",\"\")\n",
    "            check2 = text\n",
    "            check2 = check2.replace(\".js\",\"\")\n",
    "            check2 = check2.replace(\"js\",\"\")\n",
    "            if check1 == check2:\n",
    "                self.document_pepare_set.add(sdl)\n",
    "                return True\n",
    "   \n",
    "        found = False      \n",
    "        words = text.split()\n",
    "        # check word by word\n",
    "        for word in words:   \n",
    "            if word in self.skill_dict_list:\n",
    "                self.document_pepare_set.add(word)\n",
    "                found = True\n",
    "            elif word in self.group_dict_list:\n",
    "                self.document_pepare_set.add(word)\n",
    "                found = True\n",
    "        \n",
    "        return found \n",
    "        \n",
    "\n",
    "    def GenerateLearningResource(self,your_skills, job_skills):\n",
    "        skills = set()\n",
    "        if your_skills is not None:\n",
    "            skills =  job_skills -  your_skills \n",
    "        else:\n",
    "            skills = job_skills\n",
    "\n",
    "        if len(skills) == 0:\n",
    "            print(\"you are good.\")\n",
    "            return False\n",
    "        \n",
    "        self.document_pepare_set.clear()\n",
    "\n",
    "        for s in skills:\n",
    "            s = self.Filter(s)\n",
    "            if s != \"\":\n",
    "                found = self.Search(s)\n",
    "                if found == False:\n",
    "                    self.AddNotFoundDictList(s,s + \" in tech\")\n",
    "                    \n",
    "        if len(self.document_pepare_set) == 0 :\n",
    "            print(\"No any learning resource generated.\")\n",
    "            return False\n",
    "        print(self.document_pepare_set)\n",
    "        html_content = \"\"\n",
    "        for d in self.document_pepare_set:\n",
    "            path = \"\"\n",
    "            if d in self.skill_dict_list:\n",
    "                v = self.skill_dict_list.get(d)\n",
    "                path = v.resource_path\n",
    "            elif d in self.skill_dict_list:\n",
    "                v = self.group_dict_list.get(d)\n",
    "                path = v.resource_path\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            if os.path.isfile(path) == False:\n",
    "                print(d,\"not found in\",path)\n",
    "            else:\n",
    "                with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "                    title = d.title()\n",
    "                    html_content +=\"<h1><u><b>\"\n",
    "                    html_content += title\n",
    "                    html_content +=\"</b></u></h1>\"\n",
    "                    html_content += file.read()\n",
    "                file.close()\n",
    "\n",
    "        output = pypandoc.convert_text(html_content, 'docx', format='html', outputfile='learning resource.docx')\n",
    "        if output == \"\":\n",
    "            print(\"Document output sucessfully.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Document output failed\")\n",
    "            return False\n",
    "\n",
    "    def GroupTextVectorization(self):\n",
    "        for word in self.group_dict_list:\n",
    "            if self.nlp.vocab[word].has_vector == True:\n",
    "                vector_word = self.nlp(word)\n",
    "                if vector_word not in self.vector_group_dict_list:\n",
    "                    self.vector_group_dict_list[vector_word] = set()\n",
    "                self.vector_group_dict_list[vector_word].add(word)\n",
    "\n",
    "    def VectorSearch(self, word):\n",
    "        if self.nlp.vocab[word].has_vector == True:\n",
    "            vector_word = self.nlp(word)\n",
    "            for vw in self.vector_group_dict_list:\n",
    "                similarity_score = vector_word.similarity(vw)\n",
    "                if similarity_score >= 0.9:\n",
    "                    for w in vector_group_dict_list[vw]:\n",
    "                        print(w)\n",
    "\n",
    "    def CopyReplaceFolder(self, source_dir ,dest_dir , filename): \n",
    "        keyword = \"\"\n",
    "        if dest_dir == \"unknown\":\n",
    "            keyword = filename + \" in tech\"\n",
    "        else:\n",
    "            keyword = filename\n",
    "        self.ReClassificationSkillDictList(filename,dest_dir, keyword , {dest_dir})\n",
    "        if not os.path.exists(dest_dir):\n",
    "            os.makedirs(dest_dir)\n",
    "        source_path_doc = source_dir + \"/\" + filename + \".docx\"\n",
    "        source_path_html = source_dir + \"/\" + filename + \".html\"\n",
    "        destination_path_doc = dest_dir + \"/\" + filename + \".docx\"\n",
    "        destination_path_html = dest_dir + \"/\" + filename + \".html\"\n",
    "        if source_path_doc != destination_path_doc:\n",
    "            shutil.copyfile(source_path_doc, destination_path_doc)\n",
    "        if source_path_html != destination_path_html:\n",
    "            shutil.copyfile(source_path_html, destination_path_html)\n",
    "\n",
    "    def CleanupUnknownFolder(self):\n",
    "        my_path = 'unknown'\n",
    "        unknown_files = [f for f in listdir(my_path) if isfile(join(my_path, f))]\n",
    "        dir_list = next(os.walk('.'))[1]\n",
    "        for dir in dir_list:\n",
    "            if dir == \"unknown\" or dir == \"Leetcode\" or dir == \"group\":\n",
    "                continue\n",
    "            filenames = next(os.walk(dir), (None, None, []))[2]\n",
    "            for f in filenames:\n",
    "                if f in unknown_files:\n",
    "                    if os.path.exists(\"unknown/\"+f):\n",
    "                        name = f\n",
    "                        name = name.replace(\".html\",\"\")\n",
    "                        name = name.replace(\".docx\",\"\")\n",
    "                        self.UpdateSkillDictPath(name,dir)\n",
    "                        self.UpdateUnknownGroupInSkillDictList(name,dir)\n",
    "                        os.remove(\"unknown/\"+ f)\n",
    "\n",
    "    def MakeDocsFromHtml(self):\n",
    "        dir = 'unclassified'\n",
    "        filenames = [f for f in listdir(dir) if isfile(join(dir, f))]\n",
    "        for f in filenames:\n",
    "            print(f)\n",
    "            words = f.rsplit(\".\")\n",
    "            extension = words[len(words)-1]\n",
    "            if extension ==\"html\":\n",
    "                filename = f.replace(\".html\",\"\")\n",
    "                output = pypandoc.convert_file(dir + \"/\" + f, 'docx', outputfile= dir + \"/\" + filename +\".docx\")\n",
    " \n",
    "\n",
    "    def SkillReClassification(self): \n",
    "        self.backup_keyword_dict_list.clear()\n",
    "        for s in self.skill_dict_list:\n",
    "            self.backup_keyword_dict_list[s] = self.skill_dict_list[s].keyword_search\n",
    "        \n",
    "        self.skill_dict_list.clear()\n",
    "        self.group_dict_list.clear()\n",
    "        self.vector_group_dict_list.clear()\n",
    "        dir_list = next(os.walk('.'))[1]\n",
    "        for dir in dir_list:\n",
    "            if dir == \"leetcode\" or dir == \"unclassified\" or dir == \"group\":\n",
    "                continue\n",
    "            filenames = next(os.walk(dir), (None, None, []))[2]\n",
    "            for f in filenames:\n",
    "                words = f.rsplit(\".\")\n",
    "                extension = words[len(words)-1]\n",
    "                if extension ==\"docx\":\n",
    "                    filename = f.replace(\".docx\",\"\")\n",
    "                    doc = docx.Document(dir + \"/\" + f)    \n",
    "                    doc_content = \"\\n\".join([paragraph.text for paragraph in doc.paragraphs])\n",
    "                    doc_content = doc_content.lower()\n",
    "                    doc_content = doc_content.replace(\"\\n\",\" \")\n",
    "                    doc_content = doc_content.replace(\"certainly\",\"\")\n",
    "                    doc_content = doc_content.replace(\"explore\",\"\")\n",
    "                    doc_content = doc_content.replace(\",\",\"\")\n",
    "                    doc_content = doc_content.replace(\"/\",\" \")\n",
    "                    doc_content = doc_content.replace(\"-\",\" \")\n",
    "                    doc_content = doc_content.replace(\"(\",\"\")\n",
    "                    doc_content = doc_content.replace(\")\",\"\")\n",
    "                    doc_content = doc_content.replace(\"!\",\"\")\n",
    "                    doc_content = doc_content.replace(\"\\\"\",\"\")\n",
    "                    doc_content = doc_content.replace(\"1\",\"\")\n",
    "                    doc_content = doc_content.replace(\"2\",\"\")\n",
    "                    doc_content = doc_content.replace(\"3\",\"\")\n",
    "                    doc_content = doc_content.replace(\". \",\" \")\n",
    "                    #print(doc_content)\n",
    "                    words = doc_content.split()\n",
    "                    have_classific = False\n",
    "                    for i in range(len(words)):\n",
    "                        if ':' in words[i]:\n",
    "                            break\n",
    "                        three_word = words[i] + \" \" + words[i+1] + \" \" +  words[i+2]\n",
    "                        if three_word in self.three_word_skill_classification_set:\n",
    "                            self.CopyReplaceFolder(dir,three_word,filename)\n",
    "                            have_classific = True\n",
    "        \n",
    "                        two_word = words[i] + \" \" + words[i+1]\n",
    "                        two_word = two_word.replace(\" servers\",\" server\")\n",
    "                        two_word = two_word.replace(\" services\",\" service\")\n",
    "                        two_word = two_word.replace(\" applications\",\" application\")\n",
    "                        two_word = two_word.replace(\" apps\",\" application\")\n",
    "                        two_word = two_word.replace(\" app\",\" application\")\n",
    "                        two_word = two_word.replace(\" databases\",\" database\")\n",
    "                        two_word = two_word.replace(\" machines\",\" machine\")\n",
    "                        two_word = two_word.replace(\"website\",\"web\")\n",
    "                      \n",
    "                        if two_word in self.two_word_skill_classification_set:\n",
    "                            self.CopyReplaceFolder(dir,two_word,filename)\n",
    "                            have_classific = True\n",
    "        \n",
    "                        one_word = words[i]\n",
    "                        one_word = one_word.replace(\"microservices\",\"microservice\")\n",
    "                        one_word = one_word.replace(\"protocols\",\"protocol\")\n",
    "                        one_word = one_word.replace(\"networks\",\"network\")\n",
    "                        one_word = one_word.replace(\"website\",\"web\")\n",
    "                        one_word = one_word.replace(\"test\",\"testing\")\n",
    "                        one_word = one_word.replace(\"visualizations\",\"visualization\")\n",
    "                        one_word = one_word.replace(\"aws\",\"amazon\")\n",
    "        \n",
    "                        if one_word in self.one_word_skill_classification_set:\n",
    "                            self.CopyReplaceFolder(dir,one_word,filename)\n",
    "                            have_classific = True\n",
    "        \n",
    "                        if one_word ==\"ai\":\n",
    "                            two_word = \"artificial intelligence\"\n",
    "                            if two_word in self.two_word_skill_classification_set:\n",
    "                                self.CopyReplaceFolder(dir,two_word,filename)\n",
    "                            have_classific = True\n",
    "                        if one_word ==\"api\":\n",
    "                            three_word = \"application programming interface\"\n",
    "                            if three_word in self.three_word_skill_classification_set:\n",
    "                                self.CopyReplaceFolder(dir,three_word,filename)\n",
    "                                have_classific = True\n",
    "                        if one_word ==\"nlp\":\n",
    "                            three_word = \"natural language processing\"\n",
    "                            if three_word in self.three_word_skill_classification_set:\n",
    "                                self.CopyReplaceFolder(dir,three_word,filename)\n",
    "                                have_classific = True\n",
    "        \n",
    "                    if have_classific == False:\n",
    "                        self.CopyReplaceFolder(dir,\"unknown\",filename)\n",
    "                        \n",
    "        self.CleanupUnknownFolder()\n",
    "        self.GroupTextVectorization()\n",
    "        self.ExportSkillDictList()\n",
    "        self.ExportGroupDictList()\n",
    "                        \n",
    "            \n",
    "    \n",
    "    def AllThisWillBeRemoveOnceFinalize(self):\n",
    "        \n",
    "        self.exact_match_replace_dict_list[\"aws\"]=\"amazon web services\"\n",
    "        self.exact_match_replace_dict_list[\"tdd\"]=\"testing\"\n",
    "        self.exact_match_replace_dict_list[\"webdriver\"]=\"web crawler\"\n",
    "        self.exact_match_replace_dict_list[\"vbnet\"]=\"visual basic .net\"\n",
    "        self.exact_match_replace_dict_list[\"vb.net\"]=\"visual basic .net\"\n",
    "        self.exact_match_replace_dict_list[\"vb\"]=\"visual basic\"\n",
    "        self.exact_match_replace_dict_list[\"html5\"]=\"html\"\n",
    "        self.exact_match_replace_dict_list[\"svn\"]=\"subversion\"\n",
    "        self.exact_match_replace_dict_list[\"rdbms\"]=\"relational\"\n",
    "        self.exact_match_replace_dict_list[\"unity3d\"]=\"unity\"\n",
    "        self.exact_match_replace_dict_list[\"mssql\"]=\"microsoft sql\"\n",
    "        self.exact_match_replace_dict_list[\"shaders\"]=\"shader\"\n",
    "        self.exact_match_replace_dict_list[\"uat\"]=\"testing\"\n",
    "        self.exact_match_replace_dict_list[\"mui\"]=\"material ui\"\n",
    "        self.exact_match_replace_dict_list[\"gui\"]=\"graphical user interface\"\n",
    "        self.exact_match_replace_dict_list[\"ui\"]=\"user interface\"\n",
    "        \n",
    "        self.partial_match_replace_dict_list[\"ms\"]=\"microsoft\"\n",
    "        self.partial_match_replace_dict_list[\"system\"]=\"systems\"\n",
    "        self.partial_match_replace_dict_list[\"window\"]=\"windows\"\n",
    "        self.partial_match_replace_dict_list[\"databases\"]=\"database\"\n",
    "        self.partial_match_replace_dict_list[\"website\"]=\"web\"\n",
    "        self.partial_match_replace_dict_list[\"test\"]=\"testing\"\n",
    "        self.partial_match_replace_dict_list[\"networking\"]=\"network\"\n",
    "        self.partial_match_replace_dict_list[\"solarwinds\"]=\"solarwind\"\n",
    "        \n",
    "        self.ExportMatchReplaceDictList()\n",
    "     \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee87e14-341a-414f-af4d-ce82b3d82d98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3ea5afc-91a8-450e-b689-2f8610dbd841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'oracle sql', 'windows server', 'activiti', 'jinja', 'celery', 'flaskapi', 'firebase', 'posix', 'api gateway', 'rancher', 'messagepack', 'foundationdb', 'gemalto', 'elastalert', 'pyspider', 'lightgbm', 'cosmodb', 'airflow', 'storybook', 'matlab', 'kylin', 'tfs', 'hpux', 'jest', 'ast', 'jade template', 'scrapy', 'vreazlise', 'es2015', 'clearquest', 'hazelcast', 'workbench', 'maxwell', 'respondjs', 'kustomize', 'istio', 'objective c', 'shiro', 'jsdoc', 'typescript', 'avro', 'canvasjs', 'newsql', 'aws cdk', 'pytest', 'd3', 'cognito', 'grpc', 'quorum', 'superset', 'testng', 'dremio', 'phaser', 'vanillajs', 'tensorrt', 'stm32wl', 'shell script', 'theano', 'wordpress', 'j2se', 'sailsjs', 'varnish', 'xslt', 'antd', 'oauth', 'haskell', 'windows', 'ui automator', 'hana', 'openshift', 'glue', 'mixpanel', 'vert.x', 'wicket', 'elementor', 'api', 'qlikview', 'hysterix', 'winform', 'zabbix', 'validata qs', 'intellij', 'afnetworking', 'tibco', 'xml', 'sqlalchemy', 'django', 'mvicore', 'podman', 'teradata', 'postcss', 'socketio', 'nifi', 'mojolicious', 'go', 'elasticcache', 'figma', 'crystal', 'druid', 'sqs', 'caches', 'ibatis', 'gephi', 'jdbc', 'flutter', 'plc', 'hortonworks', 'es7', 'wildfly', 'zeplin', 'swagger', 'jupyter', 'butterknife', 'htmlunit', 'hashicorp', 'glide', 'casperjs', 'dataflow', 'less', 'openstack', 'linux', 'swarm', 'bsd', 'titanium', 'fastapi', 'glusterfs', 'pullreview', 'cordova', 'rxkotlin', 'material ui', 'nestjs', 'css', 'testrail', 'web crawler', 'perforce', 'jena', 'pyqt', 'kinesis', 'parquet', 'oop', 'qilkview', 'ecs', '.net core', 'riak', 'greenplum', 'openssl', 'camunda', 'aurora', 'helm', 'servlets', 'grafana', 'leakcanary', 'struts', 'camel', 'specflow', 'heroku', 'graph', 'spark', 'xen', 'sprint', 'gogs', 'powershell', 'awk', 'mockito', 'gradle', 'factory', 'hibernate', 'vpc', 'f#', 'rapidminer', 'jenkins', 'robot', 'buildkite', 'timescaledb', 'rvest', 'jwt', 'influxdb', 'scikit', 'ftp', 'storm', 'c', 'guava', 'route53', 'gulp', 'kvm', 'webgl', 'composer', 's3', 'nginx', 'extjs', 'teamcity', 'soa', 'javaee', 'geoserver', 'seaborn', 'featherjs', 'splunk', 'filebeat', 'data extraction', 'gitlab', 'ibm', 'zenoss', 'osgi', 'emr', 'cakephp', 'es5', 'mxnet', 'glassfish', 'uml', 'red hat fuse', 'maxscale', 'artifactory', 'codeigniter', 'netegrity', 'elb', 'ignite', 'bigtable', 'coroutines', 'laravel', 'aws device farm', 'mvvm', 'virtuozzo', 'wcf', 'fastlane', 'neptune', 'hd insights', 'netbeans', 'ember', 'mechanicalsoup', 'conan', 'angular', 'html', 'xarray', 'cxf', 'pojo', 'udp', 'redux-saga', 'dataframe', 'sparkml', 'hive', 'asyncio', 'microservice', 'amcharts', 'scikit-image', 'spotfire', 'haproxy', 'web', 'hadoop', 'boost', 'codepipeline', 'birt report', 'websockets', 'phpspec', 'caffe', 'nuget', 'spring cloud', 'prototype', 'hdfs', 'cloudfront', 'erlang', 'nextjs', 'java', 'viz', 'apigee', 'icinga', 'consul', 'jmeter', 'skimage', 'datax', 'jshint', 'oauth2', 'ranorex', 'queue', 'weka', 'x-pack', 'structs', 'google compute engine', 'geneos', 'drools', 'swift', 'c++', 'react', 'mcv', 'dhtml', 'tcpflow', 'ws02 apim', 'jaeger', 'jax-rs', 'tanzu', 'knative', 'chakra ui', 'mybatis', 'mesos', 'sketch', 'rollup.js', 'dataiku', 'iam', 'open vpn', 'akka', 'pixijs', 'blazor', 'phabricator', 'bitbucket', 'loopback', 'signalr', 'ant', 'jmp', 'databricks', 'numpy', 'lando', 'opentsdb', 'dagger', 'ejb', 'esri-leaflet', 'core data', 'c#', 'microsoft', 'eks', 'kaggle', 'ruby', 'percona xtradb', 'uikit', 'meteor', 'cloudera', 'spinnaker', 'xmlrpc', 'mapreduce', 'ssis', 'magento', 'aws appsync', 'c4.js', 'hbase', 'qlik', 'lambda', 'dbaas', 'orc', 'instana', 'opsgenie', 'kms', 'asp.net', 'ggplot', 'new relic', 'grails', 'restassured', 'ec2', 'bower', 'crashlytics', 'zigbee', 'babel', 'fabric', 'rspec', 'lit', 'kapacitor', 'sns', 'xampp', 'xgboost', 'boomi', 'drupal', 'durandaljs', 'core animation', 'couchdb', 'postman', 'autoprefixer', 'jboss', 'ldap', 'msmq', 'devexpress', 'ood', 'sqoop', 'qubole', 'vsts', 'server', 'beautiful soup', 'webpack', 'shell', 'ribbon', 'rpc', 'mvrx', 'sagemaker', 'code commit', 'ceph', 'axway', 'openid', 'jwe', 'scipy', 'protractor', 'solr', 'eventbus', 'opengl', 'hal', 'directx', 'phonegap', 'aerospike', 'mariadb', 'testing', 'google', 'ios', 'asp.mvc', 'redux', 'cnn', 'webrtc', 'raspberry pi', 'codepush', 'toad', 'reactivecocoa', 'jakarta ee', 'mqtt', 'sap', 'ie10+', 'cplex', 'undertow', 'sisense', 'pycharm', 'redis', 'pug', 'ethereum', 'neo4j', 'ado.net', 'jobserver', 'junit', 'docker', 'geronimo', 'jfrog', 'httpunit', 'opencart', 'informatica', 'blackduck', 'db2', 'gatling', 'dataset', 'centos', 'pagerduty', 'realm', 'beam', 'azure', 'postgresql', 'sling', 'vba', 'soap', 'dbeaver', 'gemnasium', 'dom', 'vsphere', 'pl sql', 'hilt', 'php', 'three.js', 'vue.js', 'ajax', 'solaris', 'ada', 'singleton', 'yaml', 'mule', 'jasper', 'subversion', 'netezza', 'j2ee', 'gatsby', 'spock', 'metastore', 'terraform', 'octave', 'etl', 'etcd', 'sybase', 'vault', 'papertrail', 'scylladb', 'mobx', 'weblogic', 'spring boot', 'flume', 'bugzilla', 'xsd', 'scala', 'cocoa touch', 'spss', 'appcheck', 'elasticip', 'geopandas', 'dynatrace', 'polly.js', 'immutable.js', 'cntk', 'webhooks', 'assertj', 'nethereum', 'vkey', 'zipkin', 'fluentd', 'jetty', 'kotlin', 'checkmarx', 'rwd', 'tosca', 'dubbo', 'intelij idea', 'flask', 'clair', 'aws sam', 'chai', 'amazon web services', '.net', 'cloudflare', 'iis', 'ipython', 'jee', 'symfony', 'cmake', 'koajs', 'ehcache', 'gherkin', 'high charts', 'cloudwatch', 'pytorch', 'ocaml', 'gwt', 'web3.js', 'orm', 'zuul', 'silverlight', 'fiddler', 'aquasec', 'ios sdk', 'pwa', 'vb script', 'anaconda', 'groovy', 'stata', 'solace', 'cassandra', 'aiops', 'clickhouse', 'alamofire', 'codebuild', 'qliksense', 'matplotlib', 'garden', 'statsd', 'metricbeat', 'bitrise', 'playcanvas', 'parcel', 'sinonjs', 'bigquery', 'kibana', 'svelte', 'delta lake', 'ubuntu', 'spring', 'oracle', 'pax', 'rabbitmq', 'geojson', 'glacier', 'unity', 'markdown', 'luigi', 'koin', 'chef', 'pandas', 'athena', 'npm', 'pinpoint', 'openapi', 'memcached', 'cucumber', 'periscope', 'greensock', 'vertica', 'mapbox', 'rust', 'sparksql', 'buddy', 'security', 'keras', 'ble', 'buildforge', 'nomad', 'wsk', 'mlib', 'samza', 'elasticsearch', 'stylus', 'cloudfoundry', 'amqp', 'knockoutjs', 'cognos tm1', 'xcode', 'mahout', 'cft', 'oozie', 'liferay', 'tcp', 'dax', 'jsf', 'dds', 'cloud', 'opencv', 'power bi', 'websphere', 'h2o', 'dask', 'postgis', 'linkerd', 'pillow', 'liquibase', 'perl', 'wpf', 'rds', 'rhel', 'odoo', 'arkit', 'ansible', 'calabash', 'paw', 'sr sam 34 35', 'http', 'scss', 'jslint', 'tomcat', 'axis', 'kubeflow', 'whitesource', 'esxi', 'nats', 'clojure', 'dl4j', 'nunit', 'voldemort', 'pig', 'fargate', 'presto', 'eslint', 'debian', 'electron', 'maximo', 'babylon.js', 'documentdb', 'octopus deploy', 'konga', 'xamarin', 'sqlite', 'ambari', 'yugabytedb', 'talend', 'sensu', 'clearcase', 'phpunit', 'amazon', 'mailgun', 'grunt', 'behat', 'filezilla', 'dart', 'jruby', 'node.js', 'rundeck', 'ecmascript', 'browserstack', 'labview', 'ruby on rails', 'nixos', 'axios', 'datastage', 'reactivex', 'locust', 'kdb', 'cypress', 'streamsets', 'json', 'firestore', 'elk', 'expressjs', 'rnn', 'cpanel', 'microstrategy', 'ide', 'charts.js', 'toplink', 'azure data lake', 'mapr', 'xmpp', 'attunity', 'datastax', 'zend', 'mermaid', 'mysql', 'rackspace', 'strata', 'xhtml', 'bash', 'nativescript', 'mvt', 'python', 'dynamodb', 'strategy', 'aspx', 'odata', 'jms', 'synapse', 'ionic', 'alerta', 'gemfire', 'kubernetes', 'android studio', 'git', 'backbone', 'okhttp', 'packer', 'spacy', 'zookeeper', 'beelin', 'r', 'codedeploy', 'bootstrap', 'scalding', 'relational', 'sas', 'gurobi', 'hdp', 'cloudstack', 'impala', 'qt', 'gstreamer', 'efs', 'appkit', 'sonatype nexus', 'pentaho', 'pyspark', 'kedro', 'dapper', 'mercurial', 'vercel', 'tiered', 'dotnetnuke', 'lake formation', 'portainer', 'bazel', 'android sdk', 'alluxio', 'elastic', 'lerna', 'openvz', 'syslog', 'jquery', 'jasmine', 'nuxtjs', 'plotly', 'sonarlint', 'visual studio', 'flow', 'unix', 'query', 'play', 'react native', 'dataproc', 'rxswift', 'gke', 'google cloud', 'ses', 'jpa', 'twistlock', 'nagios', 'jsp', 'siebel', 'kafka', 'ci', 'hyperv', 'soapui', 'mvc', 'sonar qube', 'oidc', 'coredns', 'sql', 'datadog', 'apache', 'puppet', 'snowflake', 'alfresco', 'javascript', 'envoy', 'sparkr', 'knn', 'lucene', 'cloudformation', 'stack', 'jndi', 'esp32', 'tableau', 'mobile', 'ssrs', 'cesiumjs', 'graphql', 'knime', 'experian', 'datarobot', 'data mart', 'argocd', 'message queue', 'jamstack', 'azkaban', 'visual basic .net', 'sass', 'mongodb', 'corda', 'maven', 'es6', 'ebs', 'vagrant', 'jmx', 'saml', 'sonarcloud', 'amplify', 'sysdig', 'logstash', 'flink', 'kudu', 'redshift', 'carthage', 'squid proxy', 'fcm', 'selenium', 'circleci', 'coreos', 'appdynamics', 'mstest', 'elixir', 'axiom', 'appium', 'aks', 'nosql', 'vmware', 'digitalocean', 'graylog', 'ovirt', 'datalake', 'visual basic', 'prometheus', 'analytics', 'tfx', 'retrofit 2', 'telegraf', 'karaf', 'sentry', 'titan', 'fortify', 'android', 'fedora', 'nhibernate', 'essbase', 'wsdl', 'alteryx', 'sockets', 'salt', 'rxjava', 'inversify', 'pcf', 'activemq', 'mootools', 'wpbakery', 'udb', 'tensorflow', 'control m', 'gocd', 'yarn'}\n",
      "Document output sucessfully.\n"
     ]
    }
   ],
   "source": [
    "test = TechStack()\n",
    "f = open(\"nodeflair skill.txt\", \"r\")\n",
    "skills = set()\n",
    "for c in f:\n",
    "    c = c.replace(\"\\n\", \"\")\n",
    "    if c == 'x':\n",
    "        continue\n",
    "    skills.add(c)\n",
    "f.close\n",
    "result = test.GenerateLearningResource(None, skills)\n",
    "test.ExportNotFoundSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc6e9d6-5b6b-48ba-a152-71f5b2d11f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#openauth not found in unknown/openauth.html\n",
    "#elastic bean stalk not found in unknown/elastic bean stalk.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "242a69d0-06d5-4df6-a57f-866c3b58c8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "![C++][1]\n",
      "\n",
      "![C++][2]\n",
      "\n",
      "Explore\n",
      "\n",
      "Certainly! **C++** is a **cross-platform programming language** that extends\n",
      "the capabilities of the C language, providing high control over system\n",
      "resources and memory. [It‚Äôs widely used for creating high-performance\n",
      "applications, operating systems, and embedded systems][3][1][3][2][4][3][5].\n",
      "\n",
      "Here are **five free resources** where you can learn C++:\n",
      "\n",
      "  1. [****][3]**[W3Schools C++ Introduction][3]** : This tutorial covers the basics of C++, including syntax, variables, and development[1][3].\n",
      "  2. [****][3]**[LearnCpp.com][6]** : A comprehensive website with step-by-step tutorials, examples, and quizzes to help you master C++ programming[4][6].\n",
      "  3. [****][3]**[Programiz C++ Tutorial][7]** : Offers interactive lessons, examples, and references for learning C++[5][7].\n",
      "  4. [****][3]**[Codecademy C++ Course][8]** : A beginner-friendly course that covers C++ essentials for software development[6][8].\n",
      "  5. **Official C++ Documentation** : The official documentation provides in-depth information about C++ features, syntax, and libraries. You can find it on the C++ Standard website.\n",
      "\n",
      "Happy learning! üöÄüë©‚Äçüíª\n",
      "\n",
      "   [1]:\n",
      "https://www.bing.com/th?id=OSK.830992e6b8f0c7bc66cd3d6fa3db36b4&pid=cdx&w=320&h=189&c=7&rs=1\n",
      "\n",
      "   [2]:\n",
      "https://www.bing.com/th?id=OSK.830992e6b8f0c7bc66cd3d6fa3db36b4&pid=cdx&w=168&h=189&c=7\n",
      "\n",
      "   [3]: https://www.w3schools.com/cpp/cpp_intro.asp\n",
      "\n",
      "   [4]: https://en.wikipedia.org/wiki/C%2B%2B\n",
      "\n",
      "   [5]: https://www.geeksforgeeks.org/introduction-to-c-programming-language/\n",
      "\n",
      "   [6]: https://www.learncpp.com/\n",
      "\n",
      "   [7]: https://www.programiz.com/cpp-programming\n",
      "\n",
      "   [8]: https://www.codecademy.com/learn/learn-c-plus-plus\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import html2text\n",
    "\n",
    "html_content = str(\"\")\n",
    "with open('c++/c++.html', 'r', encoding=\"utf-8\") as file:\n",
    "    html_content = file.read()\n",
    "file.close()\n",
    "\n",
    "h = html2text.HTML2Text()\n",
    "\n",
    "h.inline_links = False\n",
    "h.reference_links = True\n",
    "\n",
    "# Convert HTML to text with separated links\n",
    "text_content = h.handle(html_content)\n",
    "\n",
    "print(text_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e3f713-9098-41d2-ac39-22929b5945c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba39eb0-2936-4fa2-a1ed-a2e88a6baf98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
