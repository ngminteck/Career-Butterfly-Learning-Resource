{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b9b981e-d994-41b8-81d8-8ff213a69c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "import pandas as pd\n",
    "import pypandoc\n",
    "from pypandoc.pandoc_download import download_pandoc\n",
    "#download_pandoc()\n",
    "import csv\n",
    "import docx\n",
    "import os\n",
    "from pathlib import Path\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import shutil\n",
    "import html2text\n",
    "\n",
    "class Skill:\n",
    "    def __init__(self,name, keyword , groups=None,prerequisites= None):\n",
    "        filename = name\n",
    "        filename = filename.replace('/','-')\n",
    "        filename = filename.replace(\"\\\\\",'-')\n",
    "        filename = filename + \".html\"\n",
    "        path = os.path.join(\"skill\", filename)\n",
    "        path = path.replace(\"\\\\\",'/')\n",
    "        self.resource_path = path # for the resource path\n",
    "        self.keyword_search = keyword  # keyword for searching LLM\n",
    "        self.group_set = set()\n",
    "        if groups is not None:    \n",
    "            self.UpdateGroupSet(groups)\n",
    "        \n",
    "    def UpdateGroupSet(self,groups):\n",
    "        self.group_set.update(groups)\n",
    "        #print(\"skill group set updated.\")\n",
    "\n",
    "    def ChangeKeyword(self,keyword):\n",
    "        self.keyword_search = keyword\n",
    "  \n",
    "class Group:\n",
    "    def __init__(self,name,skills):\n",
    "        filename = name\n",
    "        filename = filename.replace('/','-')\n",
    "        filename = filename.replace(\"\\\\\",'-')\n",
    "        filename = filename + \".html\"\n",
    "        path = os.path.join(\"group\", filename)\n",
    "        path = path.replace(\"\\\\\",'/')\n",
    "        self.resource_path = path # for the resource path\n",
    "        self.keyword_search = name + \" in tech\" # keyword for searching LLM\n",
    "        self.skill_set = skills\n",
    "\n",
    "    def UpdateSkillSet(self,skill):\n",
    "        self.skill_set.update(skill)\n",
    "        #print(\"group skill set updated.\")\n",
    "\n",
    "    def ChangeKeyword(self,keyword):\n",
    "        self.keyword_search = keyword\n",
    "\n",
    "class TechStack:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load('en_core_web_md')\n",
    "        self.skill_dict_list = {}\n",
    "        self.group_dict_list = {}\n",
    "        self.exact_match_replace_dict_list = {}\n",
    "        self.partial_match_replace_dict_list = {}\n",
    "        self.vector_group_dict_list = {}\n",
    "        self.ignore_set = set()\n",
    "        self.not_found_dict_list = {}\n",
    "        self.three_word_skill_classification_set =set()\n",
    "        self.two_word_skill_classification_set =set()\n",
    "        self.one_word_skill_classification_set =set()\n",
    "        self.backup_keyword_dict_list={}\n",
    "        self.leetcode_list = [\"c++\",\"c\",\"c#\",\"python\",\"java\",\"javascript\",\"typescript\",\"php\",\"swift\",\"kotlin\",\"go\",\"ruby\",\"scala\",\"rust\",\"racket\"]\n",
    "        self.leetcode_company_dict_list = {}\n",
    "        self.leetcode_overall_frequency_dict_list = {}\n",
    "        self.ImportIgnoreSet()\n",
    "        self.AllThisWillBeRemoveOnceFinalize()\n",
    "        self.ImportClassificationSet()\n",
    "        self.ImportSkillDictList()\n",
    "        self.GroupTextVectorization()\n",
    "        self.InitLeetCodeCompanyNameDictList()\n",
    "        self.InitLeetcodeOverallFrequencyDictList()\n",
    "\n",
    "    def GenerateLearningResource(self,your_skills, job_skills, company_name):\n",
    "        result_dict = {\"Leetcode Question\":None , \"Skill Learning Resource Content\":None ,\"Skill Learning Resource Remarks\": str(\"\") }\n",
    "        for key in job_skills:\n",
    "            text = key\n",
    "            text = text.lower()\n",
    "            if text in self.leetcode_list:\n",
    "                result_dict[\"Leetcode Question\"] = self.GenerateLeetcodeResource(company_name)\n",
    "                break\n",
    "        difference_skill_dict_list = {}\n",
    "        #difference_skill_dict_list = [dict_ for dict_ in job_skills if not any(dict_ == dict2 for dict2 in your_skills)]\n",
    "        difference_skill_dict_list = job_skills\n",
    "        if len(difference_skill_dict_list) != 0:\n",
    "            skill_result_dict = self.GenerateSkillResource(difference_skill_dict_list)\n",
    "            result_dict[\"Skill Learning Resource Content\"] = skill_result_dict[\"Skill Learning Resource Content\"]\n",
    "            result_dict[\"Skill Learning Resource Remarks\"] = skill_result_dict[\"Skill Learning Resource Remarks\"]\n",
    "        return result_dict\n",
    "\n",
    "    def GenerateLeetcodeResource(self,company):\n",
    "        leetcode_dict_list = {}\n",
    "        check_company = company\n",
    "        check_company = check_company.lower()\n",
    "        company_name_to_search = str(\"\")\n",
    "        for c in self.leetcode_company_dict_list:\n",
    "            check = c.lower()\n",
    "            if check == check_company:\n",
    "                company_name_to_search = c\n",
    "                break\n",
    "        if company_name_to_search == \"\":\n",
    "            shutil.copyfile(\"leetcode/leetcode learning resource.html\", \"learning resource/leetcode learning resource.html\")\n",
    "            shutil.copyfile(\"leetcode/leetcode learning resource.docx\", \"learning resource/leetcode learning resource.docx\")\n",
    "            df = pd.read_csv(\"leetcode/Top 100 Question List.csv\")\n",
    "            questions_content = \"\"\n",
    "            for index, row in df.iterrows():\n",
    "                no = str(row['No'])\n",
    "                title = str(row['Title'])\n",
    "                link = str(row['Link'])\n",
    "                path = \"leetcode/Question/\" + no + \".html\"\n",
    "                if os.path.isfile(path) == True:\n",
    "                    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "                        file_content =file.read()\n",
    "                    \n",
    "                        questions_content +=\"<h1><u><b>\"\n",
    "                        questions_content += no\n",
    "                        questions_content += \". \"\n",
    "                        questions_content += title\n",
    "                        questions_content +=\"</b></u></h1>\\n\"\n",
    "                        questions_content += link\n",
    "                        questions_content +=\"\\n\"\n",
    "                        questions_content += file_content\n",
    "                       \n",
    "                        h = html2text.HTML2Text()\n",
    "                        h.ignore_links = False\n",
    "                        h.inline_links = False\n",
    "                        h.reference_links = False\n",
    "                        string_format =  h.handle(file_content)\n",
    "                        string_format =  string_format.replace(\"**\",\"\")\n",
    "                        leetcode_dict_list[no] = no + \". \" + title +\"\\n\" + link + \"\\n\\n\" + string_format\n",
    "                        file.close()\n",
    "                  \n",
    "            with open(\"learning resource/leetcode question.html\", 'w', encoding='utf-8') as file:\n",
    "                file.write(questions_content)  \n",
    "                file.close()\n",
    "            output = pypandoc.convert_text(questions_content, 'docx', format='html', outputfile='learning resource/leetcode question.docx')\n",
    "            df[company +\" Company Frequency\"] = 0\n",
    "            df[\"Overall Frequency\"] = df[\"Frequency\"]\n",
    "            df = df.drop(columns=['Frequency'])\n",
    "            df.to_csv(\"learning resource/leetcode question list.csv\", encoding='utf-8', index=False)\n",
    "            return leetcode_dict_list\n",
    "        else:\n",
    "            html_content = \"\"\n",
    "            title =\"<h1><u><b>\"+ company + \" Leetcode Tag Type Appear in the Question Count</b></u></h1>\\n\"\n",
    "            html_content += title\n",
    "            html_content +=\"<table>\\n\"\n",
    "            html_content +=\"<tr>\\n\"\n",
    "            html_content +=\"  <th>Tag</th>\\n\"\n",
    "            html_content +=\"  <th>Count</th>\\n\"\n",
    "            html_content +=\"</tr>\\n\"\n",
    "            df = pd.read_csv(\"leetcode/Top Tag/\"+ company_name_to_search + \".csv\" )\n",
    "            for index, row in df.iterrows():\n",
    "                html_content += \"<tr>\\n\" \n",
    "                tag_html =\"  <td>\" + str(row[\"Tag\"]) + \"</td>\\n\" \n",
    "                count_html =\"  <td>\" + str(row[\"Appearance\"]) + \"</td>\\n\" \n",
    "                html_content += tag_html\n",
    "                html_content += count_html\n",
    "                html_content += \"</tr>\\n\"\n",
    "            html_content +=\"</table>\\n\"\n",
    "            with open(\"leetcode/leetcode learning resource.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "                html_content +=file.read()\n",
    "                file.close()\n",
    "            with open(\"learning resource/leetcode learning resource.html\", 'w', encoding='utf-8') as file:\n",
    "                file.write(html_content)  \n",
    "                file.close()\n",
    "            output = pypandoc.convert_text(html_content, 'docx', format='html', outputfile='learning resource/leetcode learning resource.docx')\n",
    "            df1 = pd.read_csv(\"leetcode/Companies Leetcode/\" + company_name_to_search + \".csv\")\n",
    "            df1[company +\" Company Frequency\"] = df1[\"Frequency\"]\n",
    "            df1 = df1.drop(columns=['Frequency'])\n",
    "            df1[\"Overall Frequency\"] = str(\"\")\n",
    "            for index, row in df1.iterrows():\n",
    "                no = str(row['No'])\n",
    "                if no in self.leetcode_overall_frequency_dict_list:\n",
    "                    df1.at[index,\"Overall Frequency\"]= self.leetcode_overall_frequency_dict_list[no]\n",
    "            df = pd.read_csv(\"leetcode/Top 100 Question List.csv\")\n",
    "            df[company +\" Company Frequency\"] = 0\n",
    "            df[\"Overall Frequency\"] = df['Frequency']\n",
    "            df = df.drop(columns=['Frequency'])\n",
    "            appended_df = pd.concat([df1, df], ignore_index=True)\n",
    "            appended_df = appended_df.drop_duplicates(keep='first')\n",
    "            final_df = appended_df.head(100).copy()\n",
    "            final_df.to_csv(\"learning resource/leetcode question list.csv\", encoding='utf-8', index=False)\n",
    "            questions_content = \"\"\n",
    "            for index, row in final_df.iterrows():\n",
    "                no = str(row['No'])\n",
    "                title = str(row['Title'])\n",
    "                link = str(row['Link'])\n",
    "                path = \"leetcode/Question/\" + no + \".html\"\n",
    "                if os.path.isfile(path) == True:\n",
    "                    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "                        file_content =file.read()\n",
    "                    \n",
    "                        questions_content +=\"<h1><u><b>\"\n",
    "                        questions_content += no\n",
    "                        questions_content += \". \"\n",
    "                        questions_content += title\n",
    "                        questions_content +=\"</b></u></h1>\\n\"\n",
    "                        questions_content += link\n",
    "                        questions_content +=\"\\n\"\n",
    "                        questions_content += file_content\n",
    "                        \n",
    "                        h = html2text.HTML2Text()\n",
    "                        h.ignore_links = False\n",
    "                        h.inline_links = False\n",
    "                        h.reference_links = False\n",
    "                        string_format =  h.handle(file_content)\n",
    "                        string_format =  string_format.replace(\"**\",\"\")\n",
    "                        leetcode_dict_list[no] = no + \". \" + title +\"\\n\" + link + \"\\n\\n\" + string_format\n",
    "                        file.close()\n",
    "                  \n",
    "            with open(\"learning resource/leetcode question.html\", 'w', encoding='utf-8') as file:\n",
    "                file.write(questions_content)  \n",
    "                file.close()\n",
    "            output = pypandoc.convert_text(questions_content, 'docx', format='html', outputfile='learning resource/leetcode question.docx')                    \n",
    "            return leetcode_dict_list\n",
    "    \n",
    "    def GenerateSkillResource(self,skills):\n",
    "        result_dict = {\"Skill Learning Resource Content\":None ,\"Skill Learning Resource Remarks\" : str(\"\") }\n",
    "       \n",
    "        result_dict[\"Skill Learning Resource Remarks\"], document_pepare_set= self.GenerateSkillResourcePreProcessing(skills, result_dict[\"Skill Learning Resource Remarks\"])\n",
    "        if len(document_pepare_set) == 0 :\n",
    "            return result_dict\n",
    "\n",
    "        result_dict[\"Skill Learning Resource Remarks\"],result_dict[\"Skill Learning Resource Content\"] = self.GenerateSkillResourceContent(skills,document_pepare_set, result_dict[\"Skill Learning Resource Remarks\"])\n",
    "        return result_dict\n",
    "       \n",
    "\n",
    "    def GenerateSkillResourcePreProcessing(self, skills, remarks):\n",
    "        document_pepare_set = set()\n",
    "        \n",
    "        for key, value in skills.items():\n",
    "            remarks,skills[key] = self.SkillLearningResourceFilter(key,value,remarks)\n",
    "            if skills[key] != \"\":\n",
    "                remarks,document_pepare_set = self.SkillLearningResourceSearch(key,skills[key],document_pepare_set,remarks)\n",
    "        return remarks,document_pepare_set\n",
    "\n",
    "    def GenerateSkillResourceContent(self,skills, document_pepare_set,remarks):\n",
    "        skill_dict = {}\n",
    "        html_content = \"\"\n",
    "        for d in document_pepare_set:\n",
    "            path = \"\"\n",
    "            if d in self.skill_dict_list:\n",
    "                v = self.skill_dict_list.get(d)\n",
    "                path = v.resource_path\n",
    "            elif d in self.skill_dict_list:\n",
    "                v = self.group_dict_list.get(d)\n",
    "                path = v.resource_path\n",
    "            else:\n",
    "                continue\n",
    "            if os.path.isfile(path) == False:\n",
    "                if len(remarks)!= 0:\n",
    "                    remarks += \"\\n\"\n",
    "                remarks += \"can't generate content for \"\n",
    "                remarks += d.title()\n",
    "            else:\n",
    "                with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "                    title = d.title()\n",
    "                    html_content +=\"<h1><u><b>\"\n",
    "                    html_content += title\n",
    "                    html_content +=\"</b></u></h1>\"\n",
    "                    file_content = file.read()\n",
    "                    html_content += file_content\n",
    "                    h = html2text.HTML2Text()\n",
    "                    h.ignore_links = False\n",
    "                    h.inline_links = False\n",
    "                    h.reference_links = True\n",
    "                    skill_dict[title] =  h.handle(file_content)\n",
    "                file.close()\n",
    "        with open(\"learning resource/skill learning resource.html\", 'w', encoding='utf-8') as file:\n",
    "            file.write(html_content)  \n",
    "            file.close()\n",
    "        output = pypandoc.convert_text(html_content, 'docx', format='html', outputfile='learning resource/skill learning resource.docx')\n",
    "        return remarks, skill_dict\n",
    "\n",
    "    def SkillLearningResourceFilter(self, key,text,remarks):\n",
    "        text = text.lower()\n",
    "        text = text.replace(\"/\",\" \")\n",
    "        if text.find('(') != -1:\n",
    "            text = text.split(\"(\")[0]\n",
    "            text = text.rsplit()[0]\n",
    "        words = text.split()\n",
    "        if text in self.ignore_set:\n",
    "            if len(remarks)!= 0:\n",
    "                remarks += \"\\n\"\n",
    "            remarks += key\n",
    "            remarks += \" not found\"\n",
    "            return remarks,str(\"\")\n",
    "        if text in self.exact_match_replace_dict_list:\n",
    "            text = self.exact_match_replace_dict_list.get(text)\n",
    "        words = text.split()\n",
    "        new_text = \"\"\n",
    "        for word in words:\n",
    "            if word in self.partial_match_replace_dict_list:\n",
    "                new_text += self.partial_match_replace_dict_list.get(word) \n",
    "                new_text +=\" \"\n",
    "            else:\n",
    "                new_text += word \n",
    "                new_text +=\" \"\n",
    "        new_text = new_text[:-1]\n",
    "        lower_key = key.lower()\n",
    "        if lower_key != new_text:\n",
    "            if len(remarks)!= 0:\n",
    "                remarks += \"\\n\"\n",
    "            remarks += key\n",
    "            remarks += \" also known as \"\n",
    "            remarks += new_text.title()\n",
    "        return remarks, new_text\n",
    "\n",
    "    def SkillLearningResourceSearch(self,key,text,document_pepare_set,remarks):\n",
    "        if text in self.skill_dict_list:\n",
    "            document_pepare_set.add(text)\n",
    "            return remarks,document_pepare_set\n",
    "            \n",
    "        if text in self.group_dict_list:\n",
    "            document_pepare_set.add(text)\n",
    "            return remarks,document_pepare_set\n",
    "            \n",
    "        # check for . - space and .js js\n",
    "        for sdl in  self.skill_dict_list:\n",
    "           \n",
    "            check1 = sdl\n",
    "            if check1.endswith('s'):\n",
    "                check1 = check1[:-1]\n",
    "            check2 = text\n",
    "            if check2.endswith('s'):\n",
    "                check2 = check2[:-1]\n",
    "            if check1 == check2:\n",
    "                document_pepare_set.add(sdl)\n",
    "                return remarks,document_pepare_set\n",
    "            check1 = sdl\n",
    "            check1 = check1.replace(\".\",\"\")\n",
    "            check2 = text\n",
    "            check2 = check2.replace(\".\",\"\")\n",
    "            if check1 == check2:\n",
    "                document_pepare_set.add(sdl)\n",
    "                return remarks,document_pepare_set\n",
    "            check1 = sdl\n",
    "            check1 = check1.replace(\"-\",\" \")\n",
    "            check2 = text\n",
    "            check2 = check2.replace(\"-\",\" \")\n",
    "            if check1 == check2:\n",
    "                document_pepare_set.add(sdl)\n",
    "                return remarks,document_pepare_set\n",
    "            check1 = sdl\n",
    "            check1 = check1.replace(\" \",\"\")\n",
    "            check2 = text\n",
    "            check2 = check2.replace(\" \",\"\")\n",
    "            if check1 == check2:\n",
    "                document_pepare_set.add(sdl)\n",
    "                return remarks,document_pepare_set\n",
    "            check1 = sdl\n",
    "            check1 = check1.replace(\".js\",\"\")\n",
    "            check1 = check1.replace(\"js\",\"\")\n",
    "            check2 = text\n",
    "            check2 = check2.replace(\".js\",\"\")\n",
    "            check2 = check2.replace(\"js\",\"\")\n",
    "            if check1 == check2:\n",
    "                document_pepare_set.add(sdl)\n",
    "                return remarks,document_pepare_set\n",
    "   \n",
    "        found = False      \n",
    "        words = text.split()\n",
    "        # check word by word\n",
    "        for word in words:   \n",
    "            if word in self.skill_dict_list:\n",
    "                document_pepare_set.add(word)\n",
    "                if len(remarks)!= 0:\n",
    "                    remarks += \"\\n\"\n",
    "                remarks += key\n",
    "                remarks += \" also known as \"\n",
    "                remarks += word.title()\n",
    "                found = True\n",
    "            elif word in self.group_dict_list:\n",
    "                document_pepare_set.add(word)\n",
    "                if len(remarks)!= 0:\n",
    "                    remarks += \"\\n\"\n",
    "                remarks += key\n",
    "                remarks += \" also known as \"\n",
    "                remarks += word.title()\n",
    "                found = True\n",
    "                \n",
    "        if found == False:\n",
    "            if len(remarks)!= 0:\n",
    "                remarks += \"\\n\"\n",
    "            remarks += key\n",
    "            remarks += \" not found\"\n",
    "        return remarks, document_pepare_set\n",
    "          \n",
    "    def AddSkillDictList(self,name,keyword,groups=None):\n",
    "        if name not in self.skill_dict_list:\n",
    "            self.skill_dict_list[name] = Skill(name,keyword,groups)\n",
    "            #print(name,\"added in skill_dict_list.\")\n",
    "            if groups is not None:\n",
    "                for g in groups:\n",
    "                    if g in self.group_dict_list:\n",
    "                        self.group_dict_list.get(g).UpdateSkillSet({name})\n",
    "                        #print(name,\"added in\",g,\".\")\n",
    "                    else:\n",
    "                        self.group_dict_list[g] = Group(g,{name})\n",
    "                        #print(\"new group:\",g,\"have been created and added\",name,\".\")\n",
    "        else:\n",
    "            self.UpdateSkillDictList(name,groups)\n",
    "\n",
    "    def ReClassificationSkillDictList(self,name,keyword,groups):\n",
    "        search_keyword = keyword\n",
    "        if name in self.backup_keyword_dict_list:\n",
    "            search_keyword = self.backup_keyword_dict_list[name]\n",
    "        self.AddSkillDictList(name,search_keyword,groups)\n",
    "                    \n",
    "    def UpdateSkillDictList(self,name,groups):\n",
    "        if name in self.skill_dict_list:\n",
    "            self.skill_dict_list[name].UpdateGroupSet(groups)\n",
    "\n",
    "    def AddGroupDictList(self,name,skills):\n",
    "        if skills is not None:\n",
    "            if name in self.group_dict_list:\n",
    "                self.UpdateGroupDictList(name,skills)\n",
    "            else:\n",
    "                found_set = set()\n",
    "                for s in skills:\n",
    "                    if s in self.skill_dict_list:\n",
    "                        self.skill_dict_list[s].UpdateGroupSet({name})\n",
    "                        found_set.add(s)  \n",
    "                        #print(s,\"added in\",name,\"group set.\")\n",
    "                self.group_dict_list[name] = Group(name,found_set)\n",
    "\n",
    "    def UpdateGroupDictList(self,name,skills):\n",
    "        if name in self.group_dict_list:\n",
    "            found_set = set()\n",
    "            for s in skills:\n",
    "                if s in self.skill_dict_list:\n",
    "                      found_set.add(s)  \n",
    "            self.group_dict_list[name].UpdateSkillSet(found_set)\n",
    "        else:\n",
    "            self.AddGroupDictList(name,skills)\n",
    "\n",
    "    def AddNotFoundDictList(self,name,keyword):\n",
    "        if name not in self.not_found_dict_list:\n",
    "            path = \"unclassified\"\n",
    "            self.not_found_dict_list[name] =  Skill(name,path,keyword,None)   \n",
    "\n",
    "    def ImportIgnoreSet(self):\n",
    "        f = open(\"ignore.txt\", \"r\")\n",
    "        for c in f:\n",
    "            c = c.replace(\"\\n\", \"\")\n",
    "            self.ignore_set.add(c)\n",
    "        f.close()\n",
    "\n",
    "    def ImportClassificationSet(self):\n",
    "        f = open(\"three word skill classification.txt\", \"r\")\n",
    "        for l in f:\n",
    "            l = l.replace(\"\\n\", \"\")\n",
    "            self.three_word_skill_classification_set.add(l)\n",
    "        f.close()\n",
    "        f = open(\"two word skill classification.txt\", \"r\")\n",
    "        for l in f:\n",
    "            l = l.replace(\"\\n\", \"\")\n",
    "            self.two_word_skill_classification_set.add(l)\n",
    "        f.close()\n",
    "        f = open(\"one word skill classification.txt\", \"r\")\n",
    "        for l in f:\n",
    "            l = l.replace(\"\\n\", \"\")\n",
    "            self.one_word_skill_classification_set.add(l)\n",
    "        f.close()\n",
    "        \n",
    "    def ExportSkillDictList(self):\n",
    "        file_path = \"skills.csv\"\n",
    "        with open(file_path, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Name\", \"Search Keyword\",\"Resource Path\",\"Groups\"])\n",
    "            for key, value in self.skill_dict_list.items():\n",
    "                name=key\n",
    "                search = value.keyword_search\n",
    "                path = value.resource_path\n",
    "                groups =\"\"\n",
    "            \n",
    "                for g in value.group_set:\n",
    "                    groups += \"[\"\n",
    "                    groups += g\n",
    "                    groups +=\"]\"\n",
    "             \n",
    "                writer.writerow([name,search,path,groups])\n",
    "            file.close()\n",
    "        with open('skills.txt', 'w') as f:\n",
    "            for i in self.skill_dict_list:\n",
    "                f.write(i)\n",
    "                f.write(\"\\n\")\n",
    "            f.close()\n",
    "            \n",
    "    def ImportSkillDictList(self):\n",
    "        df = pd.read_csv(\"skills.csv\")\n",
    "        for index, row in df.iterrows():\n",
    "            name = str(row['Name'])\n",
    "            keyword = str(row['Search Keyword'])\n",
    "            groups = str(row['Groups'])\n",
    "            groups_set = None\n",
    "            groups = groups.replace('[', '')\n",
    "            groups_list = groups.split(']')\n",
    "            if len(groups_list) > 0 :\n",
    "                groups_list = groups_list[:-1]\n",
    "                groups_set = set()\n",
    "                for g in groups_list:\n",
    "                    groups_set.add(g)\n",
    "            # auto create group also\n",
    "            self.AddSkillDictList(name,keyword,groups_set)\n",
    "                \n",
    "    def ExportGroupDictList(self):\n",
    "        file_path = \"groups.csv\"\n",
    "        with open(file_path, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Name\", \"Search Keyword\",\"Resource Path\",\"skills\"])\n",
    "            for key, value in self.group_dict_list.items():\n",
    "                name=key\n",
    "                search = value.keyword_search\n",
    "                path = value.resource_path\n",
    "                skills =\"\"\n",
    "                for s in value.skill_set:\n",
    "                    skills += \"[\"\n",
    "                    skills += s\n",
    "                    skills +=\"]\"\n",
    "                writer.writerow([name,search,path,skills])\n",
    "            file.close()\n",
    "\n",
    "    def ExportMatchReplaceDictList(self):\n",
    "        file_path = \"exact match.csv\"\n",
    "        with open(file_path, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Word\", \"Replace\"])\n",
    "            for key, value in self.exact_match_replace_dict_list.items():\n",
    "                writer.writerow([key,value])\n",
    "            file.close()\n",
    "        file_path = \"partial match.csv\"\n",
    "        with open(file_path, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Word\", \"Replace\"])\n",
    "            for key, value in self.partial_match_replace_dict_list.items():\n",
    "                writer.writerow([key,value])\n",
    "            file.close()\n",
    "\n",
    "    def ExportNotFoundSet(self):\n",
    "        file_path = \"not found.csv\"\n",
    "        with open(file_path, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Name\", \"Search Keyword\",\"Resource Path\"])\n",
    "            for key, value in self.not_found_dict_list.items():\n",
    "                name=key\n",
    "                search = value.keyword_search\n",
    "                path = \"skill unclassified/not tech/\" + name + \".html\"\n",
    "             \n",
    "                writer.writerow([name,search,path])\n",
    "            file.close()\n",
    "        \n",
    "    def GroupTextVectorization(self):\n",
    "        for word in self.group_dict_list:\n",
    "            if self.nlp.vocab[word].has_vector == True:\n",
    "                vector_word = self.nlp(word)\n",
    "                if vector_word not in self.vector_group_dict_list:\n",
    "                    self.vector_group_dict_list[vector_word] = set()\n",
    "                self.vector_group_dict_list[vector_word].add(word)\n",
    "\n",
    "    def VectorSearch(self, word):\n",
    "        if self.nlp.vocab[word].has_vector == True:\n",
    "            vector_word = self.nlp(word)\n",
    "            for vw in self.vector_group_dict_list:\n",
    "                similarity_score = vector_word.similarity(vw)\n",
    "                if similarity_score >= 0.9:\n",
    "                    for w in vector_group_dict_list[vw]:\n",
    "                        print(w)\n",
    "\n",
    "    def CopyReplaceFolder(self, source_dir ,dest_dir , filename): \n",
    "        keyword = \"\"\n",
    "        if dest_dir == \"unknown\":\n",
    "            keyword = filename + \" in tech\"\n",
    "        else:\n",
    "            keyword = filename\n",
    "        self.ReClassificationSkillDictList(filename, keyword , {dest_dir})\n",
    "        dest_dir = \"skill classified/\" + dest_dir\n",
    "        if not os.path.exists(dest_dir):\n",
    "            os.makedirs(dest_dir)\n",
    "        source_path_doc = source_dir + \"/\" + filename + \".docx\"\n",
    "        source_path_html = source_dir + \"/\" + filename + \".html\"\n",
    "        destination_path_doc =  dest_dir + \"/\" + filename + \".docx\"\n",
    "        destination_path_html = dest_dir + \"/\" + filename + \".html\"\n",
    "        if source_path_doc != destination_path_doc:\n",
    "            shutil.copyfile(source_path_doc, destination_path_doc)\n",
    "        if source_path_html != destination_path_html:\n",
    "            shutil.copyfile(source_path_html, destination_path_html)\n",
    "\n",
    "    def MakeDocsFromHtml(self):\n",
    "        dir = 'skill unclassified/not tech/'\n",
    "        filenames = [f for f in listdir(dir) if isfile(join(dir, f))]\n",
    "        for f in filenames:\n",
    "            print(f)\n",
    "            words = f.rsplit(\".\")\n",
    "            extension = words[len(words)-1]\n",
    "            if extension ==\"html\":\n",
    "                filename = f.replace(\".html\",\"\")\n",
    "                output = pypandoc.convert_file(dir + \"/\" + f, 'docx', outputfile= dir + \"/\" + filename +\".docx\")\n",
    " \n",
    "    def DeleteAllSkillFile(self):\n",
    "        for dir in self.three_word_skill_classification_set:\n",
    "            path = \"skill classified/\" + dir\n",
    "            if os.path.isdir(path):\n",
    "                for filename in os.listdir(path):\n",
    "                    file_path = os.path.join(path, filename)\n",
    "                    try:\n",
    "                        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                            os.unlink(file_path)\n",
    "                        elif os.path.isdir(file_path):\n",
    "                            shutil.rmtree(file_path)\n",
    "                    except Exception as e:\n",
    "                        print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "        for dir in self.two_word_skill_classification_set:\n",
    "            path = \"skill classified/\" + dir\n",
    "            if os.path.isdir(dir):\n",
    "                for filename in os.listdir(path):\n",
    "                    file_path = os.path.join(path, filename)\n",
    "                    try:\n",
    "                        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                            os.unlink(file_path)\n",
    "                        elif os.path.isdir(file_path):\n",
    "                            shutil.rmtree(file_path)\n",
    "                    except Exception as e:\n",
    "                        print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "        for dir in self.one_word_skill_classification_set:\n",
    "            path = \"skill classified/\" + dir\n",
    "            if os.path.isdir(path):\n",
    "                for filename in os.listdir(path):\n",
    "                    file_path = os.path.join(path, filename)\n",
    "                    try:\n",
    "                        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                            os.unlink(file_path)\n",
    "                        elif os.path.isdir(file_path):\n",
    "                            shutil.rmtree(file_path)\n",
    "                    except Exception as e:\n",
    "                        print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "        source_dir = 'skill'\n",
    "        destination_dir = 'skill classified/unknown'\n",
    "        os.makedirs(destination_dir, exist_ok=True)\n",
    "\n",
    "        for file_name in os.listdir(source_dir):\n",
    "            source_file = os.path.join(source_dir, file_name)\n",
    "            destination_file = os.path.join(destination_dir, file_name)\n",
    "            shutil.copy(source_file, destination_file)\n",
    "        \n",
    "    def SkillReClassification(self): \n",
    "        self.backup_keyword_dict_list.clear()\n",
    "        for s in self.skill_dict_list:\n",
    "            self.backup_keyword_dict_list[s] = self.skill_dict_list[s].keyword_search\n",
    "        \n",
    "        self.skill_dict_list.clear()\n",
    "        self.group_dict_list.clear()\n",
    "        self.vector_group_dict_list.clear()\n",
    "        self.DeleteAllSkillFile()\n",
    "        h = html2text.HTML2Text()\n",
    "        h.ignore_links = False\n",
    "        h.inline_links = False\n",
    "        h.reference_links = True\n",
    "        dir = 'skill classified/unknown'\n",
    "        filenames = [f for f in listdir(dir) if isfile(join(dir, f))]\n",
    "         \n",
    "        for f in filenames:\n",
    "            words = f.rsplit(\".\")\n",
    "            extension = words[len(words)-1]\n",
    "            if extension ==\"html\":\n",
    "                filename = f.replace(\".html\",\"\")\n",
    "                html_content = str(\"\")\n",
    "                with open(dir+\"/\"+ f, 'r', encoding=\"utf-8\") as file:\n",
    "                    html_content = file.read()\n",
    "                    file.close()\n",
    "                text_content = h.handle(html_content)\n",
    "                text_content = text_content.lower()\n",
    "                text_content = text_content.replace(\"[1]\",\"\")\n",
    "                text_content = text_content.replace(\"[2]\",\"\")\n",
    "                text_content = text_content.replace(\"[3]\",\"\")\n",
    "                text_content = text_content.replace(\"[4]\",\"\")\n",
    "                text_content = text_content.replace(\"[5]\",\"\")\n",
    "                text_content = text_content.replace(\"[6]\",\"\")\n",
    "                text_content = text_content.replace(\"[7]\",\"\")\n",
    "                text_content = text_content.replace(\"[8]\",\"\")\n",
    "                text_content = text_content.replace(\"[9]\",\"\")\n",
    "                text_content = text_content.replace(\"[0]\",\"\")\n",
    "                text_content = text_content.replace(\"[\",\"\")\n",
    "                text_content = text_content.replace(\"]\",\"\")\n",
    "                text_content = text_content.replace(\"(\",\"\")\n",
    "                text_content = text_content.replace(\")\",\"\")\n",
    "                text_content = text_content.replace(\"*\",\"\")\n",
    "                text_content = text_content.replace(\"\\\"\",\"\")\n",
    "                text_content = text_content.replace(\"’s\",\"\")\n",
    "                text_content = text_content.replace(\"!\",\"\")\n",
    "                text_content = text_content.replace(\":\",\"\")\n",
    "                text_content = text_content.replace(\",\",\"\")\n",
    "                text_content = text_content.replace(\"\\n\",\" \")\n",
    "                text_content = text_content.replace(\"/\",\" \")\n",
    "                text_content = text_content.replace(\"-\",\" \")\n",
    "              \n",
    "                words = text_content.split()\n",
    "                have_classific = False\n",
    "                for i in range(len(words)):\n",
    "                    first_word = words[i]\n",
    "                    if first_word.endswith('.'):\n",
    "                        first_word = first_word[:-1]\n",
    "                   \n",
    "                    one_word = first_word\n",
    "                    one_word = one_word.replace(\"microservices\",\"microservice\")\n",
    "                    one_word = one_word.replace(\"protocols\",\"protocol\")\n",
    "                    one_word = one_word.replace(\"networks\",\"network\")\n",
    "                    one_word = one_word.replace(\"website\",\"web\")\n",
    "                    one_word = one_word.replace(\"test\",\"testing\")\n",
    "                    one_word = one_word.replace(\"visualizations\",\"visualization\")\n",
    "                    one_word = one_word.replace(\"aws\",\"amazon\")\n",
    "        \n",
    "                    if one_word in self.one_word_skill_classification_set:\n",
    "                        self.CopyReplaceFolder(dir,one_word,filename)\n",
    "                        have_classific = True\n",
    "        \n",
    "                    if one_word ==\"ai\":\n",
    "                        one_word = \"artificial intelligence\"\n",
    "                        if one_word in self.two_word_skill_classification_set:\n",
    "                            self.CopyReplaceFolder(dir,one_word,filename)\n",
    "                            have_classific = True\n",
    "                    if one_word ==\"api\":\n",
    "                        one_word = \"application programming interface\"\n",
    "                        if one_word in self.three_word_skill_classification_set:\n",
    "                            self.CopyReplaceFolder(dir,one_word,filename)\n",
    "                            have_classific = True\n",
    "                    if one_word ==\"nlp\":\n",
    "                        one_word = \"natural language processing\"\n",
    "                        if one_word in self.three_word_skill_classification_set:\n",
    "                            self.CopyReplaceFolder(dir,one_word,filename)\n",
    "                            have_classific = True\n",
    "\n",
    "                    if i + 1 >= len(words):\n",
    "                        break\n",
    "                    second_word = words[i+1]\n",
    "                    if second_word.endswith('.'):\n",
    "                        second_word = second_word[:-1] \n",
    "                \n",
    "                    two_word = first_word + \" \" + second_word\n",
    "                    two_word = two_word.replace(\" servers\",\" server\")\n",
    "                    two_word = two_word.replace(\" services\",\" service\")\n",
    "                    two_word = two_word.replace(\" applications\",\" application\")\n",
    "                    two_word = two_word.replace(\" apps\",\" application\")\n",
    "                    two_word = two_word.replace(\" app\",\" application\")\n",
    "                    two_word = two_word.replace(\" databases\",\" database\")\n",
    "                    two_word = two_word.replace(\" machines\",\" machine\")\n",
    "                    two_word = two_word.replace(\"website\",\"web\")\n",
    "                      \n",
    "                    if two_word in self.two_word_skill_classification_set:\n",
    "                        self.CopyReplaceFolder(dir,two_word,filename)\n",
    "                        have_classific = True\n",
    "\n",
    "                    if i + 2 >= len(words):\n",
    "                        break\n",
    "                    third_word = words[i+2]\n",
    "                    if third_word.endswith('.'):\n",
    "                        third_word = third_word[:-1] \n",
    "                    three_word = first_word + \" \" + second_word + \" \" +  third_word\n",
    "                    if three_word in self.three_word_skill_classification_set:\n",
    "                        self.CopyReplaceFolder(dir,three_word,filename)\n",
    "                        have_classific = True\n",
    "                \n",
    "                if have_classific == True:\n",
    "                    file_path = dir +\"/\" + filename + \".html\"\n",
    "                    if os.path.isfile(file_path):\n",
    "                        os.remove(file_path)\n",
    "                    else:\n",
    "                        print(f\"The file {file_path} does not exist.\")\n",
    "                    file_path = dir +\"/\" + filename + \".docx\"\n",
    "                    if os.path.isfile(file_path):\n",
    "                        os.remove(file_path)\n",
    "                    else:\n",
    "                        print(f\"The file {file_path} does not exist.\")\n",
    "                else:\n",
    "                    self.ReClassificationSkillDictList(filename, filename + \" in tech\" , {\"unknown\"})\n",
    "                        \n",
    "        self.GroupTextVectorization()\n",
    "        self.ExportSkillDictList()\n",
    "        self.ExportGroupDictList()\n",
    "\n",
    "    def ClassificationUnClassifedSkill(self):\n",
    "        h = html2text.HTML2Text()\n",
    "        h.ignore_links = False\n",
    "        h.inline_links = False\n",
    "        h.reference_links = True\n",
    "        dir = 'skill unclassified/not tech'\n",
    "        filenames = [f for f in listdir(dir) if isfile(join(dir, f))]\n",
    "        one_word_dict_list = {}\n",
    "        two_word_dict_list = {}\n",
    "        three_word_dict_list = {}\n",
    "        ignore_word_list = [\"a\",\"an\",\"the\",\"of\",\"on\",\"as\",\"by\",\"to\",\"with\",\"for\",\"is\",\"are\",\"was\",\"were\", \"in\"]\n",
    "        tech_word_list = [\"software\" ,\"application\", \"applications\", \"platform\", \"platforms\",\"api\", \"web\", \"website\",\"network\",\"networks\",\"security\",\"architecture\", \"development\" , \"system\", \"systems\", \"language\", \"cloud\", \"data\", \"open\",\"source\", \"windows\"]\n",
    "        for f in filenames:\n",
    "            words = f.rsplit(\".\")\n",
    "            extension = words[len(words)-1]\n",
    "            if extension ==\"html\":\n",
    "                filename = f.replace(\".html\",\"\")\n",
    "                html_content = str(\"\")\n",
    "                with open(dir+\"/\"+ f, 'r', encoding=\"utf-8\") as file:\n",
    "                    html_content = file.read()\n",
    "                    file.close()\n",
    "                text_content = h.handle(html_content)\n",
    "                text_content = text_content.lower()\n",
    "                text_content = text_content.replace(\"[1]\",\"\")\n",
    "                text_content = text_content.replace(\"[2]\",\"\")\n",
    "                text_content = text_content.replace(\"[3]\",\"\")\n",
    "                text_content = text_content.replace(\"[4]\",\"\")\n",
    "                text_content = text_content.replace(\"[5]\",\"\")\n",
    "                text_content = text_content.replace(\"[6]\",\"\")\n",
    "                text_content = text_content.replace(\"[7]\",\"\")\n",
    "                text_content = text_content.replace(\"[8]\",\"\")\n",
    "                text_content = text_content.replace(\"[9]\",\"\")\n",
    "                text_content = text_content.replace(\"[0]\",\"\")\n",
    "                text_content = text_content.replace(\"[\",\"\")\n",
    "                text_content = text_content.replace(\"]\",\"\")\n",
    "                text_content = text_content.replace(\"(\",\"\")\n",
    "                text_content = text_content.replace(\")\",\"\")\n",
    "                text_content = text_content.replace(\"*\",\"\")\n",
    "                text_content = text_content.replace(\"\\\"\",\"\")\n",
    "                text_content = text_content.replace(\"’s\",\"\")\n",
    "                text_content = text_content.replace(\"!\",\"\")\n",
    "                text_content = text_content.replace(\":\",\"\")\n",
    "                text_content = text_content.replace(\",\",\"\")\n",
    "                text_content = text_content.replace(\"\\n\",\" \")\n",
    "                text_content = text_content.replace(\"/\",\" \")\n",
    "                text_content = text_content.replace(\"-\",\" \")\n",
    "                words = text_content.split()\n",
    "                is_tech = False\n",
    "                for i in range(len(words)):\n",
    "                    if words[i] in tech_word_list:\n",
    "                        is_tech = True\n",
    "                        break\n",
    "                if is_tech == True:\n",
    "                    source_file = os.path.join(\"skill unclassified/not tech\", file_name)\n",
    "                    destination_file = os.path.join(\"skill unclassified/tech\", file_name)\n",
    "                    shutil.copy(source_file, destination_file)\n",
    "                    \n",
    "    def FindClassificationKeyword(self):\n",
    "        h = html2text.HTML2Text()\n",
    "        h.ignore_links = False\n",
    "        h.inline_links = False\n",
    "        h.reference_links = True\n",
    "        dir = 'skill classified/unknown'\n",
    "        filenames = [f for f in listdir(dir) if isfile(join(dir, f))]\n",
    "        one_word_dict_list = {}\n",
    "        two_word_dict_list = {}\n",
    "        three_word_dict_list = {}\n",
    "        ignore_word_list = [\"a\",\"an\",\"the\",\"of\",\"on\",\"as\",\"by\",\"to\",\"with\",\"for\",\"is\",\"are\",\"was\",\"were\", \"in\"]\n",
    "        for f in filenames:\n",
    "            words = f.rsplit(\".\")\n",
    "            extension = words[len(words)-1]\n",
    "            if extension ==\"html\":\n",
    "                filename = f.replace(\".html\",\"\")\n",
    "                html_content = str(\"\")\n",
    "                with open(dir+\"/\"+ f, 'r', encoding=\"utf-8\") as file:\n",
    "                    html_content = file.read()\n",
    "                    file.close()\n",
    "                text_content = h.handle(html_content)\n",
    "                text_content = text_content.lower()\n",
    "                text_content = text_content.replace(\"[1]\",\"\")\n",
    "                text_content = text_content.replace(\"[2]\",\"\")\n",
    "                text_content = text_content.replace(\"[3]\",\"\")\n",
    "                text_content = text_content.replace(\"[4]\",\"\")\n",
    "                text_content = text_content.replace(\"[5]\",\"\")\n",
    "                text_content = text_content.replace(\"[6]\",\"\")\n",
    "                text_content = text_content.replace(\"[7]\",\"\")\n",
    "                text_content = text_content.replace(\"[8]\",\"\")\n",
    "                text_content = text_content.replace(\"[9]\",\"\")\n",
    "                text_content = text_content.replace(\"[0]\",\"\")\n",
    "                text_content = text_content.replace(\"[\",\"\")\n",
    "                text_content = text_content.replace(\"]\",\"\")\n",
    "                text_content = text_content.replace(\"(\",\"\")\n",
    "                text_content = text_content.replace(\")\",\"\")\n",
    "                text_content = text_content.replace(\"*\",\"\")\n",
    "                text_content = text_content.replace(\"\\\"\",\"\")\n",
    "                text_content = text_content.replace(\"’s\",\"\")\n",
    "                text_content = text_content.replace(\"!\",\"\")\n",
    "                text_content = text_content.replace(\":\",\"\")\n",
    "                text_content = text_content.replace(\",\",\"\")\n",
    "                text_content = text_content.replace(\"\\n\",\" \")\n",
    "                text_content = text_content.replace(\"/\",\" \")\n",
    "                text_content = text_content.replace(\"-\",\" \")\n",
    "                words = text_content.split()\n",
    "                for i in range(len(words)):\n",
    "                    first_word = words[i]\n",
    "                    if '1.' in first_word:\n",
    "                        break\n",
    "                    if first_word.endswith('.'):\n",
    "                        first_word = first_word[:-1]\n",
    "                    if first_word in ignore_word_list:\n",
    "                        continue\n",
    "                    one_word = first_word\n",
    "                    if one_word not in one_word_dict_list:\n",
    "                        one_word_dict_list[one_word] = 0\n",
    "                    one_word_dict_list[one_word]+=1\n",
    "                    \n",
    "                    second_word = words[i+1]\n",
    "                    if second_word.endswith('.'):\n",
    "                        second_word = second_word[:-1] \n",
    "                    if second_word in ignore_word_list:\n",
    "                        continue\n",
    "                    two_word = first_word + \" \" + second_word\n",
    "                    if two_word not in two_word_dict_list:\n",
    "                        two_word_dict_list[two_word] = 0\n",
    "                    two_word_dict_list[two_word]+=1\n",
    "\n",
    "                    third_word = words[i+2]\n",
    "                    if third_word.endswith('.'):\n",
    "                        third_word = third_word[:-1] \n",
    "                    if third_word in ignore_word_list:\n",
    "                        continue\n",
    "                    three_word = first_word + \" \" + second_word + \" \" +third_word\n",
    "                    if three_word not in three_word_dict_list:\n",
    "                        three_word_dict_list[three_word] = 0\n",
    "                    three_word_dict_list[three_word]+=1\n",
    "        with open('count one word.txt', 'w', encoding=\"utf-8\" ) as f:\n",
    "            for s in sorted(one_word_dict_list, key=one_word_dict_list.get, reverse=True):\n",
    "                f.write(str(s) + \" - \" + str(one_word_dict_list[s]))\n",
    "                f.write('\\n')\n",
    "            file.close()\n",
    "        with open('count two word.txt', 'w', encoding=\"utf-8\") as f:\n",
    "            for s in sorted(two_word_dict_list, key=two_word_dict_list.get, reverse=True):\n",
    "                f.write(str(s) + \" - \" + str(two_word_dict_list[s]))\n",
    "                f.write('\\n')\n",
    "            file.close()\n",
    "        with open('count three word.txt', 'w', encoding=\"utf-8\") as f:\n",
    "            for s in sorted(three_word_dict_list, key=three_word_dict_list.get, reverse=True):\n",
    "                f.write(str(s) + \" - \" + str(three_word_dict_list[s]))\n",
    "                f.write('\\n')\n",
    "            file.close()\n",
    "\n",
    "    def InitLeetCodeCompanyNameDictList(self):\n",
    "        f = open(\"leetcode/companies.txt\", \"r\")\n",
    "        for c in f:\n",
    "            c = c.replace(\"\\n\", \"\")\n",
    "            key = c\n",
    "            key = key.lower()\n",
    "            self.leetcode_company_dict_list[key] = c\n",
    "        f.close\n",
    "\n",
    "    def InitLeetcodeOverallFrequencyDictList(self):\n",
    "        df = pd.read_csv(\"leetcode/Question List.csv\")\n",
    "        for index, row in df.iterrows():\n",
    "            self.leetcode_overall_frequency_dict_list[str(row[\"No\"])]=str(row[\"Frequency\"])\n",
    "          \n",
    "    def AllThisWillBeRemoveOnceFinalize(self):\n",
    "        \n",
    "        self.exact_match_replace_dict_list[\"aws\"]=\"amazon web services\"\n",
    "        self.exact_match_replace_dict_list[\"tdd\"]=\"testing\"\n",
    "        self.exact_match_replace_dict_list[\"webdriver\"]=\"web crawler\"\n",
    "        self.exact_match_replace_dict_list[\"vbnet\"]=\"visual basic .net\"\n",
    "        self.exact_match_replace_dict_list[\"vb.net\"]=\"visual basic .net\"\n",
    "        self.exact_match_replace_dict_list[\"vb\"]=\"visual basic\"\n",
    "        self.exact_match_replace_dict_list[\"html5\"]=\"html\"\n",
    "        self.exact_match_replace_dict_list[\"svn\"]=\"subversion\"\n",
    "        self.exact_match_replace_dict_list[\"rdbms\"]=\"relational\"\n",
    "        self.exact_match_replace_dict_list[\"unity3d\"]=\"unity\"\n",
    "        self.exact_match_replace_dict_list[\"mssql\"]=\"microsoft sql\"\n",
    "        self.exact_match_replace_dict_list[\"shaders\"]=\"shader\"\n",
    "        self.exact_match_replace_dict_list[\"uat\"]=\"testing\"\n",
    "        self.exact_match_replace_dict_list[\"mui\"]=\"material ui\"\n",
    "        self.exact_match_replace_dict_list[\"gui\"]=\"graphical user interface\"\n",
    "        self.exact_match_replace_dict_list[\"ui\"]=\"user interface\"\n",
    "        self.exact_match_replace_dict_list[\"mq\"]=\"message queue\"\n",
    "        self.exact_match_replace_dict_list[\"aliyun\"]=\"alibaba cloud\"\n",
    "        self.exact_match_replace_dict_list[\"ali-cloud\"]=\"alibaba cloud\"\n",
    "        \n",
    "        self.partial_match_replace_dict_list[\"ms\"]=\"microsoft\"\n",
    "        self.partial_match_replace_dict_list[\"vm\"]=\"virtual machine\"\n",
    "        self.partial_match_replace_dict_list[\"website\"]=\"web\"\n",
    "        self.partial_match_replace_dict_list[\"test\"]=\"testing\"\n",
    "        self.partial_match_replace_dict_list[\"networking\"]=\"network\"\n",
    "    \n",
    "\n",
    "     \n",
    "        \n",
    "        self.ExportMatchReplaceDictList()\n",
    "     \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3ea5afc-91a8-450e-b689-2f8610dbd841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PL/SQL also known as Pl Sql\n",
      "Cocoa Framework not found\n",
      "MSSQL also known as Microsoft Sql\n",
      "Apollo not found\n",
      "Polymer not found\n",
      "Amber not found\n",
      "Bamboo not found\n",
      "Mocha not found\n",
      "AWS Cloudwatch also known as Cloudwatch\n",
      "Enzyme not found\n",
      "Karma not found\n",
      "Gauge not found\n",
      "Charles not found\n",
      "Eclipse not found\n",
      "Axway Integration Broker(XIB) also known as Axway\n",
      "Sonar not found\n",
      "Nexus not found\n",
      "apache cordova also known as Apache\n",
      "apache cordova also known as Cordova\n",
      "Unity3D also known as Unity\n",
      "Dash not found\n",
      "Scout not found\n",
      "Segment not found\n",
      "Amplitude not found\n",
      "Code Climate not found\n",
      "Yii framework not found\n",
      "EDB not found\n",
      "Symfony 2 also known as Symfony\n",
      "ECR not found\n",
      "VIPER not found\n",
      "Combine not found\n",
      "Karate not found\n",
      "Dapresy not found\n",
      "Unicon not found\n",
      "Autonomy not found\n",
      "Crystal Report also known as Crystal\n",
      "IBM Cognos Analytics also known as Ibm\n",
      "IBM Cognos Analytics also known as Analytics\n",
      "Nose not found\n",
      "Insomnia not found\n",
      "EMS not found\n",
      "MUI also known as Material Ui\n",
      "Metal not found\n",
      "Modular not found\n",
      "MODE not found\n",
      "Aliyun also known as Alibaba Cloud\n",
      "Tencent not found\n",
      "Lit Element also known as Lit\n",
      "MQ also known as Message Queue\n",
      "ISS not found\n",
      "Solaris 11 also known as Solaris\n",
      "ABC not found\n",
      "Cloud-Bees Flow also known as Flow\n",
      "RDBMS also known as Relational\n",
      "Espresso not found\n",
      "SPA not found\n",
      "Trac not found\n",
      "KVS not found\n",
      "iReport not found\n",
      "Drone not found\n",
      "Hudson not found\n",
      "Dojo not found\n",
      "Mantis not found\n",
      "essage Queue also known as Queue\n",
      "Web Token also known as Web\n",
      "React Router also known as React\n",
      "Apache Nifi also known as Apache\n",
      "Apache Nifi also known as Nifi\n",
      "Apache Ambari also known as Apache\n",
      "Apache Ambari also known as Ambari\n",
      "IBM CPM also known as Ibm\n",
      "Ranger not found\n",
      "IBM Planning Analytics also known as Ibm\n",
      "IBM Planning Analytics also known as Analytics\n",
      "Hyperion not found\n",
      "ASP.NET MVC 5 also known as Asp.Net\n",
      "ASP.NET MVC 5 also known as Mvc\n",
      "Crystal Reports also known as Crystal\n",
      "Xray not found\n",
      "Zeppelin not found\n",
      "Apache Beam also known as Apache\n",
      "Apache Beam also known as Beam\n",
      "Spark Streaming also known as Spark\n",
      "MVP not found\n",
      "JWS not found\n",
      "Visual Studio C# also known as C#\n",
      "Graven not found\n",
      "AWS Sagemaker also known as Sagemaker\n",
      "Oracle VM also known as Oracle Virtual Machine\n",
      "Oracle VM also known as Oracle\n",
      "ArcGIS Server also known as Server\n",
      "Leaflet not found\n",
      "OneMap not found\n",
      "Test Studio also known as Testing Studio\n",
      "Test Studio also known as Testing\n",
      "Busted not found\n",
      "iPlanet Web Server also known as Web\n",
      "iPlanet Web Server also known as Server\n",
      "Cloud Dataprep also known as Cloud\n",
      "Entity not found\n",
      "Bourne not found\n",
      "GitLab CI also known as Gitlab\n",
      "GitLab CI also known as Ci\n",
      "Android NDK also known as Android\n",
      "Endur not found\n",
      "TCP/IP also known as Tcp Ip\n",
      "TCP/IP also known as Tcp\n",
      "Test Labs also known as Testing Labs\n",
      "Test Labs also known as Testing\n",
      "IBM Db2 also known as Ibm\n",
      "IBM Db2 also known as Db2\n",
      "Apache Common also known as Apache\n",
      "Fisheye not found\n",
      "Docker Swarm also known as Docker\n",
      "Docker Swarm also known as Swarm\n",
      "IBM MQBroker also known as Ibm\n",
      "Android Jetpack also known as Android\n",
      "Quartz not found\n",
      "Python scikit also known as Python\n",
      "Python scikit also known as Scikit\n",
      "React Hooks also known as React\n",
      "Appcelerator Titanium also known as Titanium\n",
      "Elastic Bean Stalk also known as Elastic\n",
      "AWS Mobile Hub also known as Mobile\n",
      "SR SAM 34/35 also known as Sr Sam 34 35\n",
      "JBoss Fuse also known as Jboss\n",
      "Oracle DB also known as Oracle\n",
      "Canal not found\n",
      "Google Big Query also known as Google\n",
      "Google Big Query also known as Query\n",
      "Apache Camel also known as Apache\n",
      "Apache Camel also known as Camel\n",
      "Concourse not found\n",
      "Amazon Neptune also known as Amazon\n",
      "Amazon Neptune also known as Neptune\n",
      "R Shiny also known as R\n",
      "Relay not found\n",
      "Web Workers also known as Web\n",
      "TICK stack also known as Stack\n",
      "JCR not found\n",
      "Fink not found\n",
      "Swing not found\n",
      "CVS not found\n",
      "Photoshop not found\n",
      "Bottle not found\n",
      "Kepler not found\n",
      "Spring AOP also known as Spring\n",
      "Spring Transaction management also known as Spring\n",
      "Quasar not found\n",
      "JERSEY not found\n",
      "Fresco not found\n",
      "Epoxy not found\n",
      "Fission not found\n",
      "OpenAuth not found\n",
      "LoRa not found\n",
      "Stripe not found\n",
      "Web Api also known as Web\n",
      "Web Api also known as Api\n",
      "Spring Reactor also known as Spring\n",
      "Rest API also known as Api\n",
      "AWS also known as Amazon Web Services\n",
      "VB.Net also known as Visual Basic .Net\n",
      "Pivotal Cloud Foundry also known as Cloud\n",
      "SVN also known as Subversion\n",
      "Spring Data also known as Spring\n",
      "Visual Studio IDE also known as Ide\n",
      "Bash scripting also known as Bash\n",
      "Retrofit not found\n",
      "Hooks not found\n",
      "Amazon S3 also known as Amazon\n",
      "Amazon S3 also known as S3\n",
      "Travis CI also known as Ci\n",
      "WebDriver also known as Web Crawler\n",
      "Dagger 2 also known as Dagger\n",
      "Dagger Android also known as Dagger\n",
      "Dagger Android also known as Android\n",
      "Kotlin Flow also known as Kotlin\n",
      "Kotlin Flow also known as Flow\n",
      "JPA 2 also known as Jpa\n",
      "Aquadata not found\n",
      "Flux not found\n",
      "Studs not found\n",
      "TDD also known as Testing\n",
      "Dat not found\n",
      "Graphite not found\n",
      "Ali-cloud also known as Alibaba Cloud\n",
      "Apache Ignite also known as Apache\n",
      "Apache Ignite also known as Ignite\n",
      "Oracle XE also known as Oracle\n",
      "Amazon RDS also known as Amazon\n",
      "Amazon RDS also known as Rds\n",
      "Arduino IDE also known as Ide\n",
      "JBoss Drools also known as Jboss\n",
      "JBoss Drools also known as Drools\n",
      "Fn not found\n",
      ".NET Microservice Framework also known as .Net\n",
      ".NET Microservice Framework also known as Microservice\n",
      "Apache Atlas also known as Apache\n",
      "Adobe XD not found\n",
      "Adobe Experience Cloud (AEC) also known as Adobe\n",
      "Adobe Experience Cloud (AEC) not found\n",
      "Cloud Firestore also known as Cloud\n",
      "Cloud Firestore also known as Firestore\n",
      "Dataframe API also known as Dataframe\n",
      "Dataframe API also known as Api\n",
      "Jackson not found\n",
      "C++ 11 also known as C++\n",
      "Docker Compose also known as Docker\n"
     ]
    }
   ],
   "source": [
    "test = TechStack()\n",
    "skills = {}\n",
    "f = open(\"nodeflair skill.txt\", \"r\")\n",
    "for c in f:\n",
    "    c = c.replace(\"\\n\", \"\")\n",
    "    skills[c] =c\n",
    "f.close\n",
    "troll = {}\n",
    "result = test.GenerateLearningResource(troll, skills, \"Google\")\n",
    "print(result[\"Skill Learning Resource Remarks\"])\n",
    "test.ExportNotFoundSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62e3f713-9098-41d2-ac39-22929b5945c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'flask'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflask\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Flask, request, jsonify, send_file, after_this_request\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mzipfile\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'flask'"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify, send_file, after_this_request\n",
    "import zipfile\n",
    "import os\n",
    "from io import BytesIO\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/get_zip', methods=['GET'])\n",
    "def get_zip():\n",
    "    # Retrieve parameters from the GET request\n",
    "    param1 = request.args.get('param1')\n",
    "    param2 = request.args.get('param2')\n",
    "\n",
    "    # Create a zip file in memory\n",
    "    memory_file = BytesIO()\n",
    "    with zipfile.ZipFile(memory_file, 'w') as zf:\n",
    "        # Add files to the zip file using the parameters\n",
    "        zf.writestr(f'{param1}.txt', f'Content for {param1}')\n",
    "        zf.writestr(f'{param2}.txt', f'Content for {param2}')\n",
    "    memory_file.seek(0)\n",
    "\n",
    "    # Define a function to remove the zip file after sending it\n",
    "    @after_this_request\n",
    "    def remove_file(response):\n",
    "        try:\n",
    "            os.remove(zip_path)\n",
    "        except Exception as error:\n",
    "            app.logger.error(\"Error removing or closing downloaded file handle\", error)\n",
    "        return response\n",
    "\n",
    "    # Send the zip file\n",
    "    response = send_file(memory_file, attachment_filename='files.zip', as_attachment=True)\n",
    "\n",
    "    # Return the JSON response with the download link\n",
    "    return jsonify({'success': True, 'message': 'Files are ready for download', 'download_link': '/get_zip'})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4c4910-cb60-4ca0-b94f-8bf01f758b75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
