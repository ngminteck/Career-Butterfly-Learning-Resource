{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a25ea2e0-da69-43df-ae53-d256c9ca2e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "import pandas as pd\n",
    "import pypandoc\n",
    "from pypandoc.pandoc_download import download_pandoc\n",
    "#download_pandoc()\n",
    "import csv\n",
    "import os\n",
    "from pathlib import Path\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import shutil\n",
    "import docx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "488066d5-b842-4fbe-9350-5056a0692885",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skill:\n",
    "    def __init__(self,name,dir, keyword , groups=None,prerequisites= None):\n",
    "        filename = name\n",
    "        filename = filename.replace('/','-')\n",
    "        filename = filename.replace(\"\\\\\",'-')\n",
    "        filename = filename + \".html\"\n",
    "        path = os.path.join(dir, filename)\n",
    "        path = path.replace(\"\\\\\",'/')\n",
    "        self.resource_path = path # for the resource path\n",
    "        self.keyword_search = keyword  # keyword for searching LLM\n",
    "        self.group_set = set()\n",
    "        if groups is not None:    \n",
    "            self.UpdateGroupSet(groups)\n",
    "        \n",
    "    def UpdateGroupSet(self,groups):\n",
    "        self.group_set.update(groups)\n",
    "        #print(\"skill group set updated.\")\n",
    "\n",
    "    def ChangeKeyword(self,keyword):\n",
    "        self.keyword_search = keyword\n",
    "\n",
    "    def ChangePath(self,name,dir):\n",
    "        filename = name\n",
    "        filename = filename.replace('/','-')\n",
    "        filename = filename.replace(\"\\\\\",'-')\n",
    "        filename = filename + \".html\"\n",
    "        path = os.path.join(dir, filename)\n",
    "        path = path.replace(\"\\\\\",'/')\n",
    "        self.resource_path = path\n",
    "      \n",
    "        \n",
    "class Group:\n",
    "    def __init__(self,name,skills):\n",
    "        filename = name\n",
    "        filename = filename.replace('/','-')\n",
    "        filename = filename.replace(\"\\\\\",'-')\n",
    "        filename = filename + \".html\"\n",
    "        path = os.path.join(\"group\", filename)\n",
    "        path = path.replace(\"\\\\\",'/')\n",
    "        self.resource_path = path # for the resource path\n",
    "        self.keyword_search = name + \" in tech\" # keyword for searching LLM\n",
    "        self.skill_set = skills\n",
    "\n",
    "    def UpdateSkillSet(self,skill):\n",
    "        self.skill_set.update(skill)\n",
    "        #print(\"group skill set updated.\")\n",
    "\n",
    "    def ChangeKeyword(self,keyword):\n",
    "        self.keyword_search = keyword\n",
    "\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b9b981e-d994-41b8-81d8-8ff213a69c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TechStack:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load('en_core_web_md')\n",
    "        self.skill_dict_list = {}\n",
    "        self.group_dict_list = {}\n",
    "        self.exact_match_replace_dict_list = {}\n",
    "        self.partial_match_replace_dict_list = {}\n",
    "        self.vector_group_dict_list = {}\n",
    "        self.ignore_set = set()\n",
    "        self.not_found_dict_list = {}\n",
    "        self.document_pepare_set = set()\n",
    "        self.three_word_skill_classification_set =set()\n",
    "        self.two_word_skill_classification_set =set()\n",
    "        self.one_word_skill_classification_set =set()\n",
    "        self.backup_keyword_dict_list={}\n",
    "        self.ImportIgnoreSet()\n",
    "        self.AllThisWillBeRemoveOnceFinalize()\n",
    "        self.ImportClassificationSet()\n",
    "        self.ImportSkillDictList()\n",
    "        self.GroupTextVectorization()\n",
    "\n",
    "               \n",
    "    def AddSkillDictList(self,name,path,keyword,groups=None):\n",
    "        if name not in self.skill_dict_list:\n",
    "            self.skill_dict_list[name] = Skill(name,path,keyword,groups)\n",
    "            #print(name,\"added in skill_dict_list.\")\n",
    "            if groups is not None:\n",
    "                for g in groups:\n",
    "                    if g in self.group_dict_list:\n",
    "                        self.group_dict_list.get(g).UpdateSkillSet({name})\n",
    "                        #print(name,\"added in\",g,\".\")\n",
    "                    else:\n",
    "                        self.group_dict_list[g] = Group(g,{name})\n",
    "                        #print(\"new group:\",g,\"have been created and added\",name,\".\")\n",
    "        else:\n",
    "            self.UpdateSkillDictList(name,groups)\n",
    "\n",
    "    def ReClassificationSkillDictList(self,name,path,keyword,groups):\n",
    "        search_keyword = keyword\n",
    "        if name in self.backup_keyword_dict_list:\n",
    "            search_keyword = self.backup_keyword_dict_list[name]\n",
    "        self.AddSkillDictList(name,path,search_keyword,groups)\n",
    "       \n",
    "                    \n",
    "    def UpdateSkillDictList(self,name,groups):\n",
    "        if name in self.skill_dict_list:\n",
    "            self.skill_dict_list[name].UpdateGroupSet(groups)\n",
    "\n",
    "    def AddGroupDictList(self,name,skills):\n",
    "        if skills is not None:\n",
    "            if name in self.group_dict_list:\n",
    "                self.UpdateGroupDictList(name,skills)\n",
    "            else:\n",
    "                found_set = set()\n",
    "                for s in skills:\n",
    "                    if s in self.skill_dict_list:\n",
    "                        self.skill_dict_list[s].UpdateGroupSet({name})\n",
    "                        found_set.add(s)  \n",
    "                        #print(s,\"added in\",name,\"group set.\")\n",
    "                self.group_dict_list[name] = Group(name,found_set)\n",
    "\n",
    "    def UpdateGroupDictList(self,name,skills):\n",
    "        if name in self.group_dict_list:\n",
    "            found_set = set()\n",
    "            for s in skills:\n",
    "                if s in self.skill_dict_list:\n",
    "                      found_set.add(s)  \n",
    "            self.group_dict_list[name].UpdateSkillSet(found_set)\n",
    "        else:\n",
    "            self.AddGroupDictList(name,skills)\n",
    "\n",
    "    def AddNotFoundDictList(self,name,keyword):\n",
    "        if name not in self.not_found_dict_list:\n",
    "            path = \"unclassified\"\n",
    "            self.not_found_dict_list[name] =  Skill(name,path,keyword,None)\n",
    "\n",
    "    def UpdateUnknownGroupInSkillDictList(self,name,new_group):\n",
    "        if name in self.skill_dict_list:\n",
    "            self.skill_dict_list[name].group_set.discard(\"unknown\")\n",
    "            self.skill_dict_list[name].group_set.add(new_group)\n",
    "        if \"unknown\" in self.group_dict_list:\n",
    "            self.group_dict_list[\"unknown\"].skill_set.discard(name)\n",
    "        if new_group in self.group_dict_list:\n",
    "            self.group_dict_list[new_group].skill_set.add(name)\n",
    "            \n",
    "    def UpdateSkillDictPath(self, name, new_dir):\n",
    "        print(name,new_dir)\n",
    "        if name in self.skill_dict_list:\n",
    "            self.skill_dict_list[name].ChangePath(name,new_dir)\n",
    "\n",
    "    def ImportIgnoreSet(self):\n",
    "        f = open(\"ignore.txt\", \"r\")\n",
    "        for c in f:\n",
    "            c = c.replace(\"\\n\", \"\")\n",
    "            self.ignore_set.add(c)\n",
    "        f.close()\n",
    "\n",
    "    def ImportClassificationSet(self):\n",
    "        f = open(\"three word skill classification.txt\", \"r\")\n",
    "        for l in f:\n",
    "            l = l.replace(\"\\n\", \"\")\n",
    "            self.three_word_skill_classification_set.add(l)\n",
    "        f.close()\n",
    "        f = open(\"two word skill classification.txt\", \"r\")\n",
    "        for l in f:\n",
    "            l = l.replace(\"\\n\", \"\")\n",
    "            self.two_word_skill_classification_set.add(l)\n",
    "        f.close()\n",
    "        f = open(\"one word skill classification.txt\", \"r\")\n",
    "        for l in f:\n",
    "            l = l.replace(\"\\n\", \"\")\n",
    "            self.one_word_skill_classification_set.add(l)\n",
    "        f.close()\n",
    "        \n",
    "    def ExportSkillDictList(self):\n",
    "        file_path = \"skills.csv\"\n",
    "        with open(file_path, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Name\", \"Search Keyword\",\"Resource Path\",\"Groups\"])\n",
    "            for key, value in self.skill_dict_list.items():\n",
    "                name=key\n",
    "                search = value.keyword_search\n",
    "                path = value.resource_path\n",
    "                groups =\"\"\n",
    "            \n",
    "                for g in value.group_set:\n",
    "                    groups += \"[\"\n",
    "                    groups += g\n",
    "                    groups +=\"]\"\n",
    "             \n",
    "                writer.writerow([name,search,path,groups])\n",
    "            file.close()\n",
    "        with open('skills.txt', 'w') as f:\n",
    "            for i in self.skill_dict_list:\n",
    "                f.write(i)\n",
    "                f.write(\"\\n\")\n",
    "            f.close()\n",
    "            \n",
    "    def ImportSkillDictList(self):\n",
    "        df = pd.read_csv(\"skills.csv\")\n",
    "        for index, row in df.iterrows():\n",
    "            name = str(row['Name'])\n",
    "            keyword = str(row['Search Keyword'])\n",
    "            path = str(row['Resource Path'])\n",
    "            dir_path = os.path.dirname(path)\n",
    "            groups = str(row['Groups'])\n",
    "            groups_set = None\n",
    "            groups = groups.replace('[', '')\n",
    "            groups_list = groups.split(']')\n",
    "            if len(groups_list) > 0 :\n",
    "                groups_list = groups_list[:-1]\n",
    "                groups_set = set()\n",
    "                for g in groups_list:\n",
    "                    groups_set.add(g)\n",
    "            # auto create group also\n",
    "            self.AddSkillDictList(name,dir_path,keyword,groups_set)\n",
    "                \n",
    "\n",
    "        \n",
    "    \n",
    "    def ExportGroupDictList(self):\n",
    "        file_path = \"groups.csv\"\n",
    "        with open(file_path, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Name\", \"Search Keyword\",\"Resource Path\",\"skills\"])\n",
    "            for key, value in self.group_dict_list.items():\n",
    "                name=key\n",
    "                search = value.keyword_search\n",
    "                path = value.resource_path\n",
    "                skills =\"\"\n",
    "                for s in value.skill_set:\n",
    "                    skills += \"[\"\n",
    "                    skills += s\n",
    "                    skills +=\"]\"\n",
    "                writer.writerow([name,search,path,skills])\n",
    "            file.close()\n",
    "\n",
    "\n",
    "\n",
    "    def ExportMatchReplaceDictList(self):\n",
    "        file_path = \"exact match.csv\"\n",
    "        with open(file_path, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Word\", \"Replace\"])\n",
    "            for key, value in self.exact_match_replace_dict_list.items():\n",
    "                writer.writerow([key,value])\n",
    "            file.close()\n",
    "        file_path = \"partial match.csv\"\n",
    "        with open(file_path, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Word\", \"Replace\"])\n",
    "            for key, value in self.partial_match_replace_dict_list.items():\n",
    "                writer.writerow([key,value])\n",
    "            file.close()\n",
    "\n",
    "\n",
    "\n",
    "    def ExportNotFoundSet(self):\n",
    "        file_path = \"not found.csv\"\n",
    "        with open(file_path, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Name\", \"Search Keyword\",\"Resource Path\",\"Groups\"])\n",
    "            for key, value in self.not_found_dict_list.items():\n",
    "                name=key\n",
    "                search = value.keyword_search\n",
    "                path = value.resource_path\n",
    "             \n",
    "                writer.writerow([name,search,path,\"\"])\n",
    "            file.close()\n",
    "     \n",
    "\n",
    "    def Filter(self, text):\n",
    "        text = text.lower()\n",
    "        text = text.replace(\"/\",\" \")\n",
    "        if text.find('(') != -1:\n",
    "            text = text.split(\"(\")[0]\n",
    "            text = text.rsplit()[0]\n",
    "            \n",
    "        words = text.split()\n",
    "    \n",
    "        if text in self.ignore_set:\n",
    "            return str(\"\")\n",
    "            \n",
    "        if text in self.exact_match_replace_dict_list:\n",
    "            text = self.exact_match_replace_dict_list.get(text)\n",
    "            \n",
    "        words = text.split()\n",
    "        new_text = \"\"\n",
    "        for word in words:\n",
    "            if word in self.partial_match_replace_dict_list:\n",
    "                new_text += self.partial_match_replace_dict_list.get(word) \n",
    "                new_text +=\" \"\n",
    "            else:\n",
    "                new_text += word \n",
    "                new_text +=\" \"\n",
    "        return new_text[:-1]\n",
    "\n",
    "    def Search(self,text):\n",
    "        if text in self.skill_dict_list:\n",
    "            self.document_pepare_set.add(text)\n",
    "            return True\n",
    "        if text in self.group_dict_list:\n",
    "            self.document_pepare_set.add(text)\n",
    "            return True\n",
    "        # check for . - space and .js js\n",
    "        for sdl in  self.skill_dict_list:\n",
    "            check1 = sdl\n",
    "            check1 = check1.replace(\".\",\"\")\n",
    "            check2 = text\n",
    "            check2 = check2.replace(\".\",\"\")\n",
    "            if check1 == check2:\n",
    "                self.document_pepare_set.add(sdl)\n",
    "                return True\n",
    "            check1 = sdl\n",
    "            check1 = check1.replace(\"-\",\" \")\n",
    "            check2 = text\n",
    "            check2 = check2.replace(\"-\",\" \")\n",
    "            if check1 == check2:\n",
    "                self.document_pepare_set.add(sdl)\n",
    "                return True\n",
    "            check1 = sdl\n",
    "            check1 = check1.replace(\" \",\"\")\n",
    "            check2 = text\n",
    "            check2 = check2.replace(\" \",\"\")\n",
    "            if check1 == check2:\n",
    "                self.document_pepare_set.add(sdl)\n",
    "                return True\n",
    "            check1 = sdl\n",
    "            check1 = check1.replace(\".js\",\"\")\n",
    "            check1 = check1.replace(\"js\",\"\")\n",
    "            check2 = text\n",
    "            check2 = check2.replace(\".js\",\"\")\n",
    "            check2 = check2.replace(\"js\",\"\")\n",
    "            if check1 == check2:\n",
    "                self.document_pepare_set.add(sdl)\n",
    "                return True\n",
    "   \n",
    "        found = False      \n",
    "        words = text.split()\n",
    "        # check word by word\n",
    "        for word in words:   \n",
    "            if word in self.skill_dict_list:\n",
    "                self.document_pepare_set.add(word)\n",
    "                found = True\n",
    "            elif word in self.group_dict_list:\n",
    "                self.document_pepare_set.add(word)\n",
    "                found = True\n",
    "        \n",
    "        return found \n",
    "        \n",
    "\n",
    "    def GenerateLearningResource(self,your_skills, job_skills):\n",
    "        skills = set()\n",
    "        if your_skills is not None:\n",
    "            skills =  job_skills -  your_skills \n",
    "        else:\n",
    "            skills = job_skills\n",
    "\n",
    "        if len(skills) == 0:\n",
    "            print(\"you are good.\")\n",
    "            return False\n",
    "        \n",
    "        self.document_pepare_set.clear()\n",
    "\n",
    "        for s in skills:\n",
    "            s = self.Filter(s)\n",
    "            if s != \"\":\n",
    "                found = self.Search(s)\n",
    "                if found == False:\n",
    "                    self.AddNotFoundDictList(s,s + \" in tech\")\n",
    "                    \n",
    "        if len(self.document_pepare_set) == 0 :\n",
    "            print(\"No any learning resource generated.\")\n",
    "            return False\n",
    "        print(self.document_pepare_set)\n",
    "        html_content = \"\"\n",
    "        for d in self.document_pepare_set:\n",
    "            path = \"\"\n",
    "            if d in self.skill_dict_list:\n",
    "                v = self.skill_dict_list.get(d)\n",
    "                path = v.resource_path\n",
    "            elif d in self.skill_dict_list:\n",
    "                v = self.group_dict_list.get(d)\n",
    "                path = v.resource_path\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            if os.path.isfile(path) == False:\n",
    "                print(d,\"not found in\",path)\n",
    "            else:\n",
    "                with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "                    title = d.title()\n",
    "                    html_content +=\"<h1><u><b>\"\n",
    "                    html_content += title\n",
    "                    html_content +=\"</b></u></h1>\"\n",
    "                    html_content += file.read()\n",
    "                file.close()\n",
    "        with open(\"learning resource.html\", 'w', encoding='utf-8') as file:\n",
    "            file.write(html_content)  \n",
    "            file.close()\n",
    "        output = pypandoc.convert_text(html_content, 'docx', format='html', outputfile='learning resource.docx')\n",
    "        if output == \"\":\n",
    "            print(\"Document output sucessfully.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Document output failed\")\n",
    "            return False\n",
    "\n",
    "    def GroupTextVectorization(self):\n",
    "        for word in self.group_dict_list:\n",
    "            if self.nlp.vocab[word].has_vector == True:\n",
    "                vector_word = self.nlp(word)\n",
    "                if vector_word not in self.vector_group_dict_list:\n",
    "                    self.vector_group_dict_list[vector_word] = set()\n",
    "                self.vector_group_dict_list[vector_word].add(word)\n",
    "\n",
    "    def VectorSearch(self, word):\n",
    "        if self.nlp.vocab[word].has_vector == True:\n",
    "            vector_word = self.nlp(word)\n",
    "            for vw in self.vector_group_dict_list:\n",
    "                similarity_score = vector_word.similarity(vw)\n",
    "                if similarity_score >= 0.9:\n",
    "                    for w in vector_group_dict_list[vw]:\n",
    "                        print(w)\n",
    "\n",
    "    def CopyReplaceFolder(self, source_dir ,dest_dir , filename): \n",
    "        keyword = \"\"\n",
    "        if dest_dir == \"unknown\":\n",
    "            keyword = filename + \" in tech\"\n",
    "        else:\n",
    "            keyword = filename\n",
    "        self.ReClassificationSkillDictList(filename,dest_dir, keyword , {dest_dir})\n",
    "        if not os.path.exists(dest_dir):\n",
    "            os.makedirs(dest_dir)\n",
    "        source_path_doc = source_dir + \"/\" + filename + \".docx\"\n",
    "        source_path_html = source_dir + \"/\" + filename + \".html\"\n",
    "        destination_path_doc = dest_dir + \"/\" + filename + \".docx\"\n",
    "        destination_path_html = dest_dir + \"/\" + filename + \".html\"\n",
    "        if source_path_doc != destination_path_doc:\n",
    "            shutil.copyfile(source_path_doc, destination_path_doc)\n",
    "        if source_path_html != destination_path_html:\n",
    "            shutil.copyfile(source_path_html, destination_path_html)\n",
    "\n",
    "    def CleanupUnknownFolder(self):\n",
    "        my_path = 'unknown'\n",
    "        unknown_files = [f for f in listdir(my_path) if isfile(join(my_path, f))]\n",
    "        dir_list = next(os.walk('.'))[1]\n",
    "        for dir in dir_list:\n",
    "            if dir == \"unknown\" or dir == \"Leetcode\" or dir == \"group\":\n",
    "                continue\n",
    "            filenames = next(os.walk(dir), (None, None, []))[2]\n",
    "            for f in filenames:\n",
    "                if f in unknown_files:\n",
    "                    if os.path.exists(\"unknown/\"+f):\n",
    "                        name = f\n",
    "                        name = name.replace(\".html\",\"\")\n",
    "                        name = name.replace(\".docx\",\"\")\n",
    "                        self.UpdateSkillDictPath(name,dir)\n",
    "                        self.UpdateUnknownGroupInSkillDictList(name,dir)\n",
    "                        os.remove(\"unknown/\"+ f)\n",
    "\n",
    "    def MakeDocsFromHtml(self):\n",
    "        dir = 'unclassified'\n",
    "        filenames = [f for f in listdir(dir) if isfile(join(dir, f))]\n",
    "        for f in filenames:\n",
    "            print(f)\n",
    "            words = f.rsplit(\".\")\n",
    "            extension = words[len(words)-1]\n",
    "            if extension ==\"html\":\n",
    "                filename = f.replace(\".html\",\"\")\n",
    "                output = pypandoc.convert_file(dir + \"/\" + f, 'docx', outputfile= dir + \"/\" + filename +\".docx\")\n",
    " \n",
    "\n",
    "    def SkillReClassification(self): \n",
    "        self.backup_keyword_dict_list.clear()\n",
    "        for s in self.skill_dict_list:\n",
    "            self.backup_keyword_dict_list[s] = self.skill_dict_list[s].keyword_search\n",
    "        \n",
    "        self.skill_dict_list.clear()\n",
    "        self.group_dict_list.clear()\n",
    "        self.vector_group_dict_list.clear()\n",
    "        dir_list = next(os.walk('.'))[1]\n",
    "        for dir in dir_list:\n",
    "            if dir == \"leetcode\" or dir == \"unclassified\" or dir == \"group\":\n",
    "                continue\n",
    "            filenames = next(os.walk(dir), (None, None, []))[2]\n",
    "            for f in filenames:\n",
    "                words = f.rsplit(\".\")\n",
    "                extension = words[len(words)-1]\n",
    "                if extension ==\"docx\":\n",
    "                    filename = f.replace(\".docx\",\"\")\n",
    "                    doc = docx.Document(dir + \"/\" + f)    \n",
    "                    doc_content = \"\\n\".join([paragraph.text for paragraph in doc.paragraphs])\n",
    "                    doc_content = doc_content.lower()\n",
    "                    doc_content = doc_content.replace(\"\\n\",\" \")\n",
    "                    doc_content = doc_content.replace(\"certainly\",\"\")\n",
    "                    doc_content = doc_content.replace(\"explore\",\"\")\n",
    "                    doc_content = doc_content.replace(\",\",\"\")\n",
    "                    doc_content = doc_content.replace(\"/\",\" \")\n",
    "                    doc_content = doc_content.replace(\"-\",\" \")\n",
    "                    doc_content = doc_content.replace(\"(\",\"\")\n",
    "                    doc_content = doc_content.replace(\")\",\"\")\n",
    "                    doc_content = doc_content.replace(\"!\",\"\")\n",
    "                    doc_content = doc_content.replace(\"\\\"\",\"\")\n",
    "                    doc_content = doc_content.replace(\"1\",\"\")\n",
    "                    doc_content = doc_content.replace(\"2\",\"\")\n",
    "                    doc_content = doc_content.replace(\"3\",\"\")\n",
    "                    doc_content = doc_content.replace(\". \",\" \")\n",
    "                    #print(doc_content)\n",
    "                    words = doc_content.split()\n",
    "                    have_classific = False\n",
    "                    for i in range(len(words)):\n",
    "                        if ':' in words[i]:\n",
    "                            break\n",
    "                        three_word = words[i] + \" \" + words[i+1] + \" \" +  words[i+2]\n",
    "                        if three_word in self.three_word_skill_classification_set:\n",
    "                            self.CopyReplaceFolder(dir,three_word,filename)\n",
    "                            have_classific = True\n",
    "        \n",
    "                        two_word = words[i] + \" \" + words[i+1]\n",
    "                        two_word = two_word.replace(\" servers\",\" server\")\n",
    "                        two_word = two_word.replace(\" services\",\" service\")\n",
    "                        two_word = two_word.replace(\" applications\",\" application\")\n",
    "                        two_word = two_word.replace(\" apps\",\" application\")\n",
    "                        two_word = two_word.replace(\" app\",\" application\")\n",
    "                        two_word = two_word.replace(\" databases\",\" database\")\n",
    "                        two_word = two_word.replace(\" machines\",\" machine\")\n",
    "                        two_word = two_word.replace(\"website\",\"web\")\n",
    "                      \n",
    "                        if two_word in self.two_word_skill_classification_set:\n",
    "                            self.CopyReplaceFolder(dir,two_word,filename)\n",
    "                            have_classific = True\n",
    "        \n",
    "                        one_word = words[i]\n",
    "                        one_word = one_word.replace(\"microservices\",\"microservice\")\n",
    "                        one_word = one_word.replace(\"protocols\",\"protocol\")\n",
    "                        one_word = one_word.replace(\"networks\",\"network\")\n",
    "                        one_word = one_word.replace(\"website\",\"web\")\n",
    "                        one_word = one_word.replace(\"test\",\"testing\")\n",
    "                        one_word = one_word.replace(\"visualizations\",\"visualization\")\n",
    "                        one_word = one_word.replace(\"aws\",\"amazon\")\n",
    "        \n",
    "                        if one_word in self.one_word_skill_classification_set:\n",
    "                            self.CopyReplaceFolder(dir,one_word,filename)\n",
    "                            have_classific = True\n",
    "        \n",
    "                        if one_word ==\"ai\":\n",
    "                            two_word = \"artificial intelligence\"\n",
    "                            if two_word in self.two_word_skill_classification_set:\n",
    "                                self.CopyReplaceFolder(dir,two_word,filename)\n",
    "                            have_classific = True\n",
    "                        if one_word ==\"api\":\n",
    "                            three_word = \"application programming interface\"\n",
    "                            if three_word in self.three_word_skill_classification_set:\n",
    "                                self.CopyReplaceFolder(dir,three_word,filename)\n",
    "                                have_classific = True\n",
    "                        if one_word ==\"nlp\":\n",
    "                            three_word = \"natural language processing\"\n",
    "                            if three_word in self.three_word_skill_classification_set:\n",
    "                                self.CopyReplaceFolder(dir,three_word,filename)\n",
    "                                have_classific = True\n",
    "        \n",
    "                    if have_classific == False:\n",
    "                        self.CopyReplaceFolder(dir,\"unknown\",filename)\n",
    "                        \n",
    "        self.CleanupUnknownFolder()\n",
    "        self.GroupTextVectorization()\n",
    "        self.ExportSkillDictList()\n",
    "        self.ExportGroupDictList()\n",
    "                        \n",
    "            \n",
    "    \n",
    "    def AllThisWillBeRemoveOnceFinalize(self):\n",
    "        \n",
    "        self.exact_match_replace_dict_list[\"aws\"]=\"amazon web services\"\n",
    "        self.exact_match_replace_dict_list[\"tdd\"]=\"testing\"\n",
    "        self.exact_match_replace_dict_list[\"webdriver\"]=\"web crawler\"\n",
    "        self.exact_match_replace_dict_list[\"vbnet\"]=\"visual basic .net\"\n",
    "        self.exact_match_replace_dict_list[\"vb.net\"]=\"visual basic .net\"\n",
    "        self.exact_match_replace_dict_list[\"vb\"]=\"visual basic\"\n",
    "        self.exact_match_replace_dict_list[\"html5\"]=\"html\"\n",
    "        self.exact_match_replace_dict_list[\"svn\"]=\"subversion\"\n",
    "        self.exact_match_replace_dict_list[\"rdbms\"]=\"relational\"\n",
    "        self.exact_match_replace_dict_list[\"unity3d\"]=\"unity\"\n",
    "        self.exact_match_replace_dict_list[\"mssql\"]=\"microsoft sql\"\n",
    "        self.exact_match_replace_dict_list[\"shaders\"]=\"shader\"\n",
    "        self.exact_match_replace_dict_list[\"uat\"]=\"testing\"\n",
    "        self.exact_match_replace_dict_list[\"mui\"]=\"material ui\"\n",
    "        self.exact_match_replace_dict_list[\"gui\"]=\"graphical user interface\"\n",
    "        self.exact_match_replace_dict_list[\"ui\"]=\"user interface\"\n",
    "        \n",
    "        self.partial_match_replace_dict_list[\"ms\"]=\"microsoft\"\n",
    "        self.partial_match_replace_dict_list[\"system\"]=\"systems\"\n",
    "        self.partial_match_replace_dict_list[\"window\"]=\"windows\"\n",
    "        self.partial_match_replace_dict_list[\"databases\"]=\"database\"\n",
    "        self.partial_match_replace_dict_list[\"website\"]=\"web\"\n",
    "        self.partial_match_replace_dict_list[\"test\"]=\"testing\"\n",
    "        self.partial_match_replace_dict_list[\"networking\"]=\"network\"\n",
    "        self.partial_match_replace_dict_list[\"solarwinds\"]=\"solarwind\"\n",
    "        \n",
    "        self.ExportMatchReplaceDictList()\n",
    "     \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee87e14-341a-414f-af4d-ce82b3d82d98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3ea5afc-91a8-450e-b689-2f8610dbd841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'msmq', 'gatsby', 'ggplot', 'pig', 'delta lake', 'redux', 'expressjs', 'sass', 'play', 'charts.js', 'zabbix', 'maximo', 'kvm', 'visual basic', 'data extraction', 'jee', 'mockito', 'maxscale', 'netegrity', 'soap', 'calabash', 'filebeat', 'splunk', 'sysdig', 'ember', 'clojure', 'cxf', 'codeigniter', 'nagios', 'shell script', 'kudu', 'blazor', 'tcp', 'python', 'tableau', 'etl', 'vault', 'hibernate', 'aws cdk', 'control m', 'knockoutjs', 'node.js', 'essbase', 'testng', 'javascript', 'mxnet', 'sonarlint', 'neptune', 'nextjs', 'google', 'logstash', 'sr sam 34 35', 'kdb', 'scipy', 'nats', 'spark', 'cloudflare', 'jboss', 'titanium', 'jmp', 'immutable.js', 'f#', 'workbench', 'liferay', 'opencv', 'salt', 'mailgun', 'es5', 'presto', 'chai', 'kylin', 'dataiku', 'circleci', 'sagemaker', 'vue.js', 'firebase', 'stata', 'datalake', 'php', 'qt', 'knn', 'activiti', 'elementor', 'udp', 'influxdb', 'mule', 'maxwell', 'elk', 'paw', 'kubernetes', 'scrapy', 'swift', 'buildforge', 'xampp', 'cloudwatch', 'timescaledb', 'afnetworking', 'material ui', 'jsf', 's3', 'sonatype nexus', 'cosmodb', 'koin', 'wicket', 'lando', 'lerna', 'oauth2', 'coreos', 'appcheck', 'unix', 'autoprefixer', 'netbeans', 'css', 'spacy', 'gatling', 'nestjs', 'jax-rs', 'mvrx', 'drupal', 'c', 'informatica', 'couchdb', 'es6', 'opentsdb', 'mercurial', 'javaee', 'webrtc', 'tcpflow', 'validata qs', 'tensorflow', 'ast', 'ibatis', 'iam', 'rhel', 'cypress', 'cucumber', 'digitalocean', 'solaris', 'gemfire', 'ignite', 'backbone', 'aiops', 'tanzu', 'c4.js', 'playcanvas', 'gemalto', 'kafka', 'squid proxy', 'high charts', 'core animation', 'rxkotlin', 'puppet', 'jwe', 'cnn', 'weka', 'storybook', 'rvest', 'gemnasium', 'labview', 'pandas', 'shell', 'alerta', 'gradle', 'cloudfoundry', 'codepipeline', 'tfs', 'dremio', 'pl sql', 'zigbee', 'windows', 'elb', 'greenplum', 'postman', 'jobserver', 'cakephp', 'ios sdk', 'sybase', 'vagrant', 'flaskapi', 'mlib', 'postcss', 'clair', 'gherkin', 'azure', 'casperjs', 'wpf', 'artifactory', 'hyperv', 'eslint', 'caffe', 'datadog', 'code commit', 'streamsets', 'jetty', 'bigtable', 'core data', 'rancher', 'cntk', 'xsd', 'plc', 'cloudformation', 'keras', 'dataproc', 'nhibernate', 'sap', 'sockets', 'dataframe', 'composer', 'documentdb', 'pycharm', 'varnish', 'servlets', 'esp32', 'go', 'queue', 'avro', 'vb script', 'boost', 'react', 'assertj', 'sensu', 'glide', 'dbaas', 'django', 'dax', 'ipython', 'gurobi', 'retrofit 2', 'android sdk', 'realm', 'factory', 'j2se', 'xen', 'pentaho', 'grafana', 'riak', 'papertrail', 'ribbon', 'fastapi', 'phonegap', 'elasticsearch', 'rxjava', 'eks', 'jasmine', 'gocd', 'jfrog', 'teradata', 'rspec', 'redux-saga', 'codedeploy', 'oracle sql', 'sentry', 'jndi', 'anaconda', 'apache', 'pixijs', 'cpanel', 'structs', 'analytics', 'mqtt', 'mapreduce', 'relational', 'appkit', 'ant', 'bitrise', 'scalding', 'ajax', 'pullreview', 'experian', 'datastage', 'dapper', 'visual studio', 'mootools', 'microstrategy', 'mahout', 'rpc', 'mobx', 'dask', 'uikit', 'luigi', 'hbase', 'microservice', 'spinnaker', 'mixpanel', 'restassured', 'winform', 'amcharts', 'spock', 'rwd', 'oracle', 'phabricator', 'azkaban', 'locust', 'geronimo', 'icinga', 'jade template', 'mybatis', 'fastlane', 'vreazlise', 'mvt', 'webpack', 'ecmascript', 'pyqt', 'sparkml', 'intelij idea', 'es2015', 'sns', 'ios', 'elixir', 'nixos', 'signalr', 'lightgbm', 'corda', 'checkmarx', 'bootstrap', 'wpbakery', 'strategy', 'snowflake', 'weblogic', 'konga', 'ecs', 'buildkite', 'maven', 'axis', 'cloudstack', 'directx', 'scss', 'xcode', 'undertow', 'unity', 'swagger', 'jsdoc', 'raspberry pi', 'fargate', 'dbeaver', 'nuxtjs', 'sketch', 'celery', 'amqp', 'jslint', 'dubbo', 'stm32wl', 'helm', 'sqoop', 'packer', 'jruby', 'carthage', 'c++', 'opengl', 'spring cloud', 'ide', 'sprint', 'spotfire', 'vkey', 'zenoss', 'aerospike', 'pug', 'spring boot', 'cloud', 'mobile', 'svelte', 'dynatrace', 'hana', 'pyspark', 'seaborn', 'azure data lake', 'clickhouse', 'awk', 'route53', 'yarn', 'linux', 'angular', 'ambari', 'ionic', 'elasticip', 'ovirt', 'sqs', 'web3.js', 'ftp', 'crystal', 'pagerduty', 'ado.net', 'singleton', 'podman', 'glassfish', 'firestore', 'openstack', 'jinja', 'buddy', 'openssl', 'graylog', 'impala', 'new relic', 'eventbus', 'dhtml', 'haskell', 'xmlrpc', 'html', 'kedro', 'erlang', 'metastore', 'hortonworks', 'xamarin', 'ssrs', 'vert.x', 'perl', 'quorum', 'red hat fuse', 'kaggle', 'jpa', 'rust', 'caches', 'ejb', 'qilkview', 'tosca', 'voldemort', 'web crawler', 'soa', 'jaeger', 'cognos tm1', 'emr', 'stack', '.net', 'shiro', 'd3', 'kubeflow', 'airflow', 'oop', 'druid', 'samza', 'rackspace', 'heroku', 'envoy', 'rundeck', 'ble', 'objective c', 'statsd', 'jakarta ee', 'ubuntu', 'zuul', 'power bi', 'datax', 'elastic', 'tfx', 'toplink', 'magento', 'udb', 'iis', 'cft', 'beam', 'foundationdb', 'oidc', 'specflow', 'coredns', 'qlikview', 'ses', 'aks', 'sqlalchemy', 'alamofire', 'ranorex', 'sinonjs', 'rabbitmq', 'sisense', 'saml', 'terraform', 'query', 'parquet', 'activemq', 'json', 'camel', 'mechanicalsoup', 'devexpress', 'akka', 'sqlite', 'java', 'google cloud', 'mapr', 'opencart', 'datastax', 'yaml', 'elastalert', 'telegraf', 'jshint', 'h2o', 'mariadb', 'dataflow', 'bugzilla', 'soapui', 'twistlock', 'sparksql', 'subversion', 'groovy', 'webgl', 'qubole', 'protractor', 'aurora', 'open vpn', 'mysql', 'consul', 'elasticcache', 'reactivecocoa', 'sling', 'x-pack', 'drools', 'pwa', 'guava', 'flow', 'phpunit', 'fortify', 'aws device farm', 'ansible', 'geoserver', 'jasper', 'rxswift', 'whitesource', 'crashlytics', 'posix', 'gstreamer', 'gulp', 'electron', 'jupyter', 'phpspec', 'cassandra', 'jquery', 'codebuild', 'vba', 'flume', 'alluxio', 'superset', 'hdfs', 'babylon.js', 'mapbox', 'amplify', 'dds', 'react native', 'okhttp', 'jena', 'lambda', 'browserstack', 'perforce', 'junit', 'alfresco', 'pytest', 'grpc', 'camunda', 'pyspider', 'openshift', 'hilt', 'htmlunit', 'osgi', 'parcel', 'jamstack', 'ehcache', 'hive', 'talend', 'mojolicious', 'axios', 'istio', 'axway', 'odata', 'three.js', 'siebel', 'phaser', 'kms', 'teamcity', 'appium', 'grunt', 'dagger', 'xarray', 'vpc', 'athena', 'spring', 'matplotlib', 'cesiumjs', 'mcv', 'blackduck', 'kotlin', 'jwt', 'aspx', 'metricbeat', 'datarobot', 'mongodb', 'ie10+', 'meteor', 'testing', 'messagepack', 'security', 'ec2', 'alteryx', 'aws appsync', 'uml', 'pinpoint', 'android', 'windows server', 'mvc', 'wordpress', 'haproxy', 'ws02 apim', 'kinesis', 'http', 'fcm', 'vertica', 'aws sam', 'chakra ui', 'zookeeper', 'openvz', 'xgboost', 'conan', 'c#', 'periscope', 'vmware', 'graph', 'bower', 'axiom', 'openapi', 'postgresql', 'ethereum', 'dynamodb', 'hashicorp', 'websockets', 'cloudfront', 'hazelcast', 'wsdl', 'qlik', 'prototype', 'theano', 'codepush', 'glusterfs', 'canvasjs', 'jmeter', 'redis', 'yugabytedb', 'postgis', 'odoo', 'syslog', 'scikit-image', 'orm', 'laravel', 'kapacitor', 'ood', 'j2ee', 'scala', 'databricks', 'robot', 'nginx', 'qliksense', 'message queue', 'cocoa touch', '.net core', 'arkit', 'gke', 'titan', 'grails', 'aquasec', 'mesos', 'dom', 'stylus', 'matlab', 'pcf', 'microsoft', 'hysterix', 'extjs', 'figma', 'centos', 'pillow', 'inversify', 'jms', 'xslt', 'jest', 'flask', 'tibco', 'zeplin', 'api gateway', 'ceph', 'visual basic .net', 'butterknife', 'nomad', 'respondjs', 'vsts', 'hadoop', 'typescript', 'wildfly', 'ocaml', 'coroutines', 'amazon', 'knative', 'kustomize', 'portainer', 'babel', 'durandaljs', 'plotly', 'fiddler', 'jsp', 'linkerd', 'loopback', 'memcached', 'mvicore', 'silverlight', 'numpy', 'dotnetnuke', 'redshift', 'tiered', 'intellij', 'git', 'gitlab', 'ada', 'ebs', 'skimage', 'apigee', 'esxi', 'prometheus', 'zend', 'nunit', 'vanillajs', 'birt report', 'nethereum', 'orc', 'geopandas', 'tomcat', 'tensorrt', 'octave', 'nativescript', 'powershell', 'oauth', 'symfony', 'sparkr', 'fabric', 'gephi', 'ui automator', 'boomi', 'synapse', 'opsgenie', 'strata', 'vsphere', 'hdp', 'httpunit', 'android studio', 'polly.js', 'newsql', 'sailsjs', 'koajs', 'openid', 'socketio', 'testrail', 'pytorch', 'nuget', 'mvvm', 'websphere', 'virtuozzo', 'argocd', 'swarm', 'google compute engine', 'reactivex', 'dart', 'asyncio', 'spss', 'vercel', 'neo4j', 'bitbucket', 'kibana', 'fedora', 'lake formation', 'percona xtradb', 'storm', 'ruby', 'etcd', 'web', 'beelin', 'xmpp', 'leakcanary', 'karaf', 'docker', 'gwt', 'hpux', 'bazel', 'zipkin', 'knime', 'cmake', 'glue', 'ibm', 'ci', 'mermaid', 'lucene', 'webhooks', 'less', 'beautiful soup', 'rnn', 'clearcase', 'flutter', 'cloudera', 'graphql', 'cordova', 'dl4j', 'scylladb', 'sonar qube', 'rollup.js', 'sas', 'ldap', 'r', 'sql', 'bigquery', 'mstest', 'struts', 'antd', 'hal', 'viz', 'lit', 'chef', 'instana', 'attunity', 'dataset', 'gogs', 'rapidminer', 'liquibase', 'server', 'toad', 'wcf', 'solace', 'data mart', 'debian', 'geojson', 'markdown', 'filezilla', 'greensock', 'ssis', 'db2', 'nifi', 'selenium', 'asp.mvc', 'bash', 'wsk', 'pax', 'amazon web services', 'behat', 'efs', 'es7', 'jdbc', 'pojo', 'garden', 'esri-leaflet', 'cognito', 'flink', 'npm', 'xml', 'octopus deploy', 'asp.net', 'cplex', 'xhtml', 'ruby on rails', 'bsd', 'scikit', 'sonarcloud', 'netezza', 'fluentd', 'solr', 'geneos', 'jmx', 'hd insights', 'rds', 'oozie', 'nosql', 'glacier', 'api', 'featherjs', 'jenkins', 'clearquest', 'appdynamics'}\n",
      "Document output sucessfully.\n"
     ]
    }
   ],
   "source": [
    "test = TechStack()\n",
    "f = open(\"nodeflair skill.txt\", \"r\")\n",
    "skills = set()\n",
    "for c in f:\n",
    "    c = c.replace(\"\\n\", \"\")\n",
    "    if c == 'x':\n",
    "        continue\n",
    "    skills.add(c)\n",
    "f.close\n",
    "result = test.GenerateLearningResource(None, skills)\n",
    "test.ExportNotFoundSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc6e9d6-5b6b-48ba-a152-71f5b2d11f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#openauth not found in unknown/openauth.html\n",
    "#elastic bean stalk not found in unknown/elastic bean stalk.html\n",
    "#https://stackoverflow.com/questions/75475470/how-to-extract-the-all-hyperlink-and-their-text-from-a-word-document-using-pytho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "242a69d0-06d5-4df6-a57f-866c3b58c8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "![C++][1]\n",
      "\n",
      "![C++][2]\n",
      "\n",
      "Explore\n",
      "\n",
      "Certainly! **C++** is a **cross-platform programming language** that extends\n",
      "the capabilities of the C language, providing high control over system\n",
      "resources and memory. [It’s widely used for creating high-performance\n",
      "applications, operating systems, and embedded systems][3][1][3][2][4][3][5].\n",
      "\n",
      "Here are **five free resources** where you can learn C++:\n",
      "\n",
      "  1. [****][3]**[W3Schools C++ Introduction][3]** : This tutorial covers the basics of C++, including syntax, variables, and development[1][3].\n",
      "  2. [****][3]**[LearnCpp.com][6]** : A comprehensive website with step-by-step tutorials, examples, and quizzes to help you master C++ programming[4][6].\n",
      "  3. [****][3]**[Programiz C++ Tutorial][7]** : Offers interactive lessons, examples, and references for learning C++[5][7].\n",
      "  4. [****][3]**[Codecademy C++ Course][8]** : A beginner-friendly course that covers C++ essentials for software development[6][8].\n",
      "  5. **Official C++ Documentation** : The official documentation provides in-depth information about C++ features, syntax, and libraries. You can find it on the C++ Standard website.\n",
      "\n",
      "Happy learning! 🚀👩‍💻\n",
      "\n",
      "   [1]:\n",
      "https://www.bing.com/th?id=OSK.830992e6b8f0c7bc66cd3d6fa3db36b4&pid=cdx&w=320&h=189&c=7&rs=1\n",
      "\n",
      "   [2]:\n",
      "https://www.bing.com/th?id=OSK.830992e6b8f0c7bc66cd3d6fa3db36b4&pid=cdx&w=168&h=189&c=7\n",
      "\n",
      "   [3]: https://www.w3schools.com/cpp/cpp_intro.asp\n",
      "\n",
      "   [4]: https://en.wikipedia.org/wiki/C%2B%2B\n",
      "\n",
      "   [5]: https://www.geeksforgeeks.org/introduction-to-c-programming-language/\n",
      "\n",
      "   [6]: https://www.learncpp.com/\n",
      "\n",
      "   [7]: https://www.programiz.com/cpp-programming\n",
      "\n",
      "   [8]: https://www.codecademy.com/learn/learn-c-plus-plus\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import html2text\n",
    "\n",
    "html_content = str(\"\")\n",
    "with open('c++/c++.html', 'r', encoding=\"utf-8\") as file:\n",
    "    html_content = file.read()\n",
    "file.close()\n",
    "\n",
    "h = html2text.HTML2Text()\n",
    "\n",
    "h.inline_links = False\n",
    "h.reference_links = True\n",
    "\n",
    "# Convert HTML to text with separated links\n",
    "text_content = h.handle(html_content)\n",
    "\n",
    "print(text_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e3f713-9098-41d2-ac39-22929b5945c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify, send_file, after_this_request\n",
    "import zipfile\n",
    "import os\n",
    "from io import BytesIO\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/get_zip', methods=['GET'])\n",
    "def get_zip():\n",
    "    # Retrieve parameters from the GET request\n",
    "    param1 = request.args.get('param1')\n",
    "    param2 = request.args.get('param2')\n",
    "\n",
    "    # Create a zip file in memory\n",
    "    memory_file = BytesIO()\n",
    "    with zipfile.ZipFile(memory_file, 'w') as zf:\n",
    "        # Add files to the zip file using the parameters\n",
    "        zf.writestr(f'{param1}.txt', f'Content for {param1}')\n",
    "        zf.writestr(f'{param2}.txt', f'Content for {param2}')\n",
    "    memory_file.seek(0)\n",
    "\n",
    "    # Define a function to remove the zip file after sending it\n",
    "    @after_this_request\n",
    "    def remove_file(response):\n",
    "        try:\n",
    "            os.remove(zip_path)\n",
    "        except Exception as error:\n",
    "            app.logger.error(\"Error removing or closing downloaded file handle\", error)\n",
    "        return response\n",
    "\n",
    "    # Send the zip file\n",
    "    response = send_file(memory_file, attachment_filename='files.zip', as_attachment=True)\n",
    "\n",
    "    # Return the JSON response with the download link\n",
    "    return jsonify({'success': True, 'message': 'Files are ready for download', 'download_link': '/get_zip'})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba39eb0-2936-4fa2-a1ed-a2e88a6baf98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
