{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9b981e-d994-41b8-81d8-8ff213a69c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://localhost:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [29/Mar/2024 18:00:21] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [29/Mar/2024 18:00:26] \"GET /generate_skill_match_score HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c c++ c# java python javascript typescript\n",
      "['c', 'c++', 'c#', 'java', 'python', 'javascript', 'typescript']\n",
      "responsibilities collaborate with business stakeholders to understand their data needs and objectives collect clean and preprocess data from various sources for analysis perform exploratory data analysis to identify trends patterns and correlations develop and implement predictive models and machine learning algorithms to solve business challenges apply statistical analysis techniques to analyze complex datasets and draw meaningful conclusions create data visualizations and reports to communicate insights effectively to non-technical audiences collaborate with data engineers to optimize data pipelines for efficient data processing conduct a b testing and experimentation to evaluate the effectiveness of different strategies stay up-to-date with advancements in data science machine learning and artificial intelligence assist in the development and deployment of machine learning models into production environments provide data-driven insights and recommendations to support strategic decision-making collaborate with other data scientists analysts and cross-functional teams to drive data initiatives requirements bachelor's degree in data science computer science statistics mathematics or a related field or equivalent practical experience proven experience as a data scientist or similar role with a portfolio of data science projects that demonstrate your analytical skills proficiency in programming languages such as python or r for data manipulation and analysis strong understanding of statistical analysis machine learning algorithms and data visualization techniques experience with machine learning frameworks and libraries  scikit-learn tensorflow pytorch familiarity with data manipulation libraries  pandas numpy and data visualization tools  matplotlib seaborn solid understanding of sql and database concepts for querying and extracting data excellent problem-solving skills and the ability to work with complex unstructured datasets effective communication skills to explain technical concepts to non-technical stakeholders experience with big data technologies  hadoop spark is a plus knowledge of cloud platforms and services for data analysis  aws azure is advantageous familiarity with natural language processing nlp and text analysis is a plus advanced degree master's or phd in a related field is beneficial but not required.\n",
      "['responsibilities', 'collaborate', 'with', 'business', 'stakeholders', 'to', 'understand', 'their', 'data', 'needs', 'and', 'objectives', 'collect', 'clean', 'and', 'preprocess', 'data', 'from', 'various', 'sources', 'for', 'analysis', 'perform', 'exploratory', 'data', 'analysis', 'to', 'identify', 'trends', 'patterns', 'and', 'correlations', 'develop', 'and', 'implement', 'predictive', 'models', 'and', 'machine', 'learning', 'algorithms', 'to', 'solve', 'business', 'challenges', 'apply', 'statistical', 'analysis', 'techniques', 'to', 'analyze', 'complex', 'datasets', 'and', 'draw', 'meaningful', 'conclusions', 'create', 'data', 'visualizations', 'and', 'reports', 'to', 'communicate', 'insights', 'effectively', 'to', 'non-technical', 'audiences', 'collaborate', 'with', 'data', 'engineers', 'to', 'optimize', 'data', 'pipelines', 'for', 'efficient', 'data', 'processing', 'conduct', 'a', 'b', 'testing', 'and', 'experimentation', 'to', 'evaluate', 'the', 'effectiveness', 'of', 'different', 'strategies', 'stay', 'up-to-date', 'with', 'advancements', 'in', 'data', 'science', 'machine', 'learning', 'and', 'artificial', 'intelligence', 'assist', 'in', 'the', 'development', 'and', 'deployment', 'of', 'machine', 'learning', 'models', 'into', 'production', 'environments', 'provide', 'data-driven', 'insights', 'and', 'recommendations', 'to', 'support', 'strategic', 'decision-making', 'collaborate', 'with', 'other', 'data', 'scientists', 'analysts', 'and', 'cross-functional', 'teams', 'to', 'drive', 'data', 'initiatives', 'requirements', \"bachelor's\", 'degree', 'in', 'data', 'science', 'computer', 'science', 'statistics', 'mathematics', 'or', 'a', 'related', 'field', 'or', 'equivalent', 'practical', 'experience', 'proven', 'experience', 'as', 'a', 'data', 'scientist', 'or', 'similar', 'role', 'with', 'a', 'portfolio', 'of', 'data', 'science', 'projects', 'that', 'demonstrate', 'your', 'analytical', 'skills', 'proficiency', 'in', 'programming', 'languages', 'such', 'as', 'python', 'or', 'r', 'for', 'data', 'manipulation', 'and', 'analysis', 'strong', 'understanding', 'of', 'statistical', 'analysis', 'machine', 'learning', 'algorithms', 'and', 'data', 'visualization', 'techniques', 'experience', 'with', 'machine', 'learning', 'frameworks', 'and', 'libraries', 'scikit-learn', 'tensorflow', 'pytorch', 'familiarity', 'with', 'data', 'manipulation', 'libraries', 'pandas', 'numpy', 'and', 'data', 'visualization', 'tools', 'matplotlib', 'seaborn', 'solid', 'understanding', 'of', 'sql', 'and', 'database', 'concepts', 'for', 'querying', 'and', 'extracting', 'data', 'excellent', 'problem-solving', 'skills', 'and', 'the', 'ability', 'to', 'work', 'with', 'complex', 'unstructured', 'datasets', 'effective', 'communication', 'skills', 'to', 'explain', 'technical', 'concepts', 'to', 'non-technical', 'stakeholders', 'experience', 'with', 'big', 'data', 'technologies', 'hadoop', 'spark', 'is', 'a', 'plus', 'knowledge', 'of', 'cloud', 'platforms', 'and', 'services', 'for', 'data', 'analysis', 'aws', 'azure', 'is', 'advantageous', 'familiarity', 'with', 'natural', 'language', 'processing', 'nlp', 'and', 'text', 'analysis', 'is', 'a', 'plus', 'advanced', 'degree', \"master's\", 'or', 'phd', 'in', 'a', 'related', 'field', 'is', 'beneficial', 'but', 'not', 'required.']\n",
      "data analysis\n",
      "data visualization\n",
      "aws\n",
      "big data\n",
      "testing\n",
      "r\n",
      "matplotlib\n",
      "data processing\n",
      "pytorch\n",
      "tensorflow\n",
      "exploratory data analysis\n",
      "seaborn\n",
      "machine learning\n",
      "pandas\n",
      "scikit-learn\n",
      "visualization\n",
      "artificial intelligence\n",
      "unstructured datasets\n",
      "natural language processing\n",
      "machine learning algorithms\n",
      "azure\n",
      "spark\n",
      "programming\n",
      "database\n",
      "hadoop\n",
      "text analysis\n",
      "data science\n",
      "cloud\n",
      "statistical analysis\n",
      "python\n",
      "numpy\n",
      "sql\n",
      "PL/SQL also known as Pl Sql\n",
      "MSSQL also known as Microsoft Sql\n",
      "Apollo not found\n",
      "Polymer not found\n",
      "Amber not found\n",
      "Bamboo not found\n",
      "Mocha not found\n",
      "AWS Cloudwatch also known as Aws\n",
      "AWS Cloudwatch also known as Cloudwatch\n",
      "Enzyme not found\n",
      "Karma not found\n",
      "Gauge not found\n",
      "Charles not found\n",
      "Eclipse not found\n",
      "Axway Integration Broker(XIB) also known as Axway\n",
      "Sonar not found\n",
      "Nexus not found\n",
      "apache cordova also known as Cordova\n",
      "Unity3D also known as Unity\n",
      "Dash not found\n",
      "Scout not found\n",
      "Segment not found\n",
      "Amplitude not found\n",
      "Code Climate not found\n",
      "Yii framework not found\n",
      "EDB not found\n",
      "Symfony 2 also known as Symfony\n",
      "ECR not found\n",
      "VIPER not found\n",
      "Combine not found\n",
      "Karate not found\n",
      "Dapresy not found\n",
      "Unicon not found\n",
      "Autonomy not found\n",
      "Crystal Report also known as Crystal\n",
      "Nose not found\n",
      "Insomnia not found\n",
      "EMS not found\n",
      "MUI also known as Material Ui\n",
      "Metal not found\n",
      "Modular not found\n",
      "MODE not found\n",
      "Aliyun also known as Alibaba Cloud\n",
      "Tencent not found\n",
      "Lit Element also known as Lit\n",
      "ISS not found\n",
      "Solaris 11 also known as Solaris\n",
      "ABC not found\n",
      "SPA not found\n",
      "Trac not found\n",
      "KVS not found\n",
      "iReport not found\n",
      "Drone not found\n",
      "Hudson not found\n",
      "Dojo not found\n",
      "Mantis not found\n",
      "essage Queue also known as Queue\n",
      "Apache Nifi also known as Nifi\n",
      "Apache Ambari also known as Ambari\n",
      "IBM CPM not found\n",
      "Ranger not found\n",
      "Hyperion not found\n",
      "ASP.NET MVC 5 also known as Asp.Net Mvc\n",
      "Crystal Reports also known as Crystal\n",
      "Xray not found\n",
      "Zeppelin not found\n",
      "Apache Beam also known as Beam\n",
      "MVP not found\n",
      "JWS not found\n",
      "Visual Studio C# also known as C#\n",
      "Graven not found\n",
      "AWS Sagemaker also known as Aws\n",
      "AWS Sagemaker also known as Sagemaker\n",
      "Oracle VM not found\n",
      "Leaflet not found\n",
      "OneMap not found\n",
      "Busted not found\n",
      "Entity not found\n",
      "Bourne not found\n",
      "GitLab CI also known as Gitlab\n",
      "GitLab CI also known as Ci\n",
      "Endur not found\n",
      "TCP/IP also known as Tcp Ip\n",
      "TCP/IP also known as Tcp\n",
      "Fisheye not found\n",
      "Android Jetpack also known as Android\n",
      "Quartz not found\n",
      "Python scikit also known as Python\n",
      "Python scikit also known as Scikit\n",
      "Appcelerator Titanium also known as Titanium\n",
      "SR SAM 34/35 also known as Sr Sam 34 35\n",
      "JBoss Fuse also known as Jboss\n",
      "Oracle DB also known as Oracle Database\n",
      "Canal not found\n",
      "Apache Camel also known as Camel\n",
      "Concourse not found\n",
      "Amazon Neptune also known as Neptune\n",
      "Relay not found\n",
      "JCR not found\n",
      "Fink not found\n",
      "Swing not found\n",
      "CVS not found\n",
      "Photoshop not found\n",
      "Bottle not found\n",
      "Kepler not found\n",
      "Quasar not found\n",
      "JERSEY not found\n",
      "Fresco not found\n",
      "Epoxy not found\n",
      "Fission not found\n",
      "OpenAuth not found\n",
      "LoRa not found\n",
      "Stripe not found\n",
      "Rest API also known as Api\n",
      "VB.Net also known as Visual Basic .Net\n",
      "SVN also known as Subversion\n",
      "Visual Studio IDE also known as Ide\n",
      "Retrofit not found\n",
      "Hooks not found\n",
      "Amazon S3 also known as S3\n",
      "WebDriver also known as Web Crawler\n",
      "Dagger 2 also known as Dagger\n",
      "Dagger Android also known as Dagger\n",
      "Dagger Android also known as Android\n",
      "Kotlin Flow also known as Kotlin\n",
      "Kotlin Flow also known as Flow\n",
      "JPA 2 also known as Jpa\n",
      "Aquadata not found\n",
      "Flux not found\n",
      "Studs not found\n",
      "TDD also known as Testing\n",
      "Dat not found\n",
      "Graphite not found\n",
      "Ali-cloud also known as Alibaba Cloud\n",
      "Apache Ignite also known as Ignite\n",
      "Oracle XE not found\n",
      "Amazon RDS also known as Rds\n",
      "JBoss Drools also known as Jboss\n",
      "JBoss Drools also known as Drools\n",
      "Fn not found\n",
      ".NET Microservice Framework also known as .Net\n",
      ".NET Microservice Framework also known as Microservice\n",
      "Adobe XD not found\n",
      "Adobe Experience Cloud (AEC) also known as Adobe\n",
      "Adobe Experience Cloud (AEC) not found\n",
      "Cloud Firestore also known as Cloud\n",
      "Cloud Firestore also known as Firestore\n",
      "Dataframe API also known as Dataframe\n",
      "Dataframe API also known as Api\n",
      "Jackson not found\n",
      "C++ 11 also known as C++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [29/Mar/2024 18:04:40] \"GET /generate_learning_resource HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import pypandoc\n",
    "import csv\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import shutil\n",
    "import zipfile\n",
    "import html2text\n",
    "import json\n",
    "import re\n",
    "from flask import Flask, send_file, jsonify\n",
    "from werkzeug.serving import run_simple\n",
    "\n",
    "\n",
    "class Skill:\n",
    "\n",
    "    def __init__(self, name, keyword, groups=None):\n",
    "        filename = name\n",
    "        filename = filename.replace('/', '-')\n",
    "        filename = filename.replace(\"\\\\\", '-')\n",
    "        filename = filename + \".html\"\n",
    "        path = os.path.join(\"skill\", filename)\n",
    "        path = path.replace(\"\\\\\", '/')\n",
    "        self.resource_path = path  # for the resource path\n",
    "        self.keyword_search = keyword  # keyword for searching LLM\n",
    "        self.group_set = set()\n",
    "        if groups is not None:\n",
    "            self.UpdateGroupSet(groups)\n",
    "\n",
    "    def UpdateGroupSet(self, groups):\n",
    "        self.group_set.update(groups)\n",
    "        # print(\"skill group set updated.\")\n",
    "\n",
    "    def ChangeKeyword(self, keyword):\n",
    "        self.keyword_search = keyword\n",
    "\n",
    "\n",
    "class Group:\n",
    "    def __init__(self, name, skills):\n",
    "        filename = name\n",
    "        filename = filename.replace('/', '-')\n",
    "        filename = filename.replace(\"\\\\\", '-')\n",
    "        filename = filename + \".html\"\n",
    "        path = os.path.join(\"group\", filename)\n",
    "        path = path.replace(\"\\\\\", '/')\n",
    "        self.resource_path = path  # for the resource path\n",
    "        self.keyword_search = name + \" in tech\"  # keyword for searching LLM\n",
    "        self.skill_set = skills\n",
    "\n",
    "    def UpdateSkillSet(self, skill):\n",
    "        self.skill_set.update(skill)\n",
    "        # print(\"group skill set updated.\")\n",
    "\n",
    "    def ChangeKeyword(self, keyword):\n",
    "        self.keyword_search = keyword\n",
    "\n",
    "\n",
    "class TechStack:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load('en_core_web_md')\n",
    "        self.skill_dict_list = {}\n",
    "        self.group_dict_list = {}\n",
    "        self.exact_match_replace_dict_list = {}\n",
    "        self.partial_match_replace_dict_list = {}\n",
    "        self.vector_group_dict_list = {}\n",
    "        self.ignore_set = set()\n",
    "        self.not_found_dict_list = {}\n",
    "        self.three_word_skill_classification_set = set()\n",
    "        self.two_word_skill_classification_set = set()\n",
    "        self.one_word_skill_classification_set = set()\n",
    "        self.backup_keyword_dict_list = {}\n",
    "        self.partial_search_ignore_list = [\"apache\", \"microsoft\", \"google\", \"amazon\", \"apple\", \"vmware\", \"ibm\",\n",
    "                                           \"oracle\", \"sap\"]\n",
    "        self.leetcode_list = [\"c++\", \"c\", \"c#\", \"python\", \"java\", \"javascript\", \"typescript\", \"php\", \"swift\", \"kotlin\",\n",
    "                              \"go\", \"ruby\", \"scala\", \"rust\", \"racket\"]\n",
    "        self.one_keyword_dict_list = {}\n",
    "        self.two_keyword_dict_list = {}\n",
    "        self.three_keyword_dict_list = {}\n",
    "        self.leetcode_company_dict_list = {}\n",
    "        self.leetcode_overall_frequency_dict_list = {}\n",
    "        self.ImportIgnoreSet()\n",
    "        self.AllThisWillBeRemoveOnceFinalize()\n",
    "        self.ImportClassificationSet()\n",
    "        self.ImportSkillDictList()\n",
    "        self.GroupTextVectorization()\n",
    "        self.InitKeywordDictList()\n",
    "        self.InitLeetCodeCompanyNameDictList()\n",
    "        self.InitLeetcodeOverallFrequencyDictList()\n",
    "        self.request_queue_no = 0\n",
    "\n",
    "    def GenerateSkillMatchScore(self, your_skill, job_skill):\n",
    "        result_dict = {\"Your Skills List\": None, \"Job Skills List\": None, \"Match Score\": None}\n",
    "        your_skill_set = self.ExtractSkillKeyword(your_skill)\n",
    "        job_skill_set = self.ExtractSkillKeyword(job_skill)\n",
    "        result_dict[\"Your Skills List\"] = list(your_skill_set)\n",
    "        result_dict[\"Job Skills List\"] = list(job_skill_set)\n",
    "        match_score = {}\n",
    "        for js in result_dict[\"Job Skills List\"]:\n",
    "            if js in result_dict[\"Your Skills List\"]:\n",
    "                match_score[js] = 1\n",
    "            else:\n",
    "                match_score[js] = 0\n",
    "        result_dict[\"Match Score\"] = match_score\n",
    "        return result_dict\n",
    "\n",
    "    def ExtractSkillKeyword(self, text):\n",
    "        skill_set = set()\n",
    "        text = text.lower()\n",
    "        text = text.replace(\"e.g.,\", \"\")\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        text = text.replace(\"!\", \"\")\n",
    "        text = text.replace(\",\", \"\")\n",
    "        text = text.replace(\"(\", \"\")\n",
    "        text = text.replace(\")\", \"\")\n",
    "        text = text.replace(\":\", \"\")\n",
    "        text = text.replace(\"\\\"\", \" \")\n",
    "        text = text.replace(\"/\", \" \")\n",
    "        text = text.replace(\". \", \" \")\n",
    "        print(text)\n",
    "        words = text.split()\n",
    "        # = []\n",
    "  \n",
    "        for i in range(2, len(words)): \n",
    "            search_word = words[i - 2] + \" \" + words[i - 1] + \" \" + words[i]\n",
    "            if search_word in self.three_keyword_dict_list:\n",
    "                skill_set.add(self.three_keyword_dict_list[search_word])\n",
    "                #remove_index.append(i -2)\n",
    "                #remove_index.append(i -1)\n",
    "                #remove_index.append(i)\n",
    "        #for i in remove_index:\n",
    "          #  del words[i]\n",
    "        #remove_index.clear()\n",
    "        for i in range(1, len(words)): \n",
    "            search_word = words[i - 1] + \" \" + words[i]\n",
    "            if search_word in self.two_keyword_dict_list:\n",
    "                skill_set.add(self.two_keyword_dict_list[search_word])\n",
    "                #remove_index.append(i -1)\n",
    "                #remove_index.append(i)\n",
    "      #  for i in remove_index:\n",
    "         #   del words[i]\n",
    "        #remove_index.clear()\n",
    "        for i in range(len(words)):\n",
    "            if words[i] in self.one_keyword_dict_list:\n",
    "                skill_set.add(self.one_keyword_dict_list[words[i]])\n",
    "                #remove_index.append(i)\n",
    "       # for i in remove_index:\n",
    "          #  del words[i]\n",
    "        print(words)\n",
    "        return skill_set\n",
    "\n",
    "    def GetRequestQueueNo(self):\n",
    "        self.request_queue_no += 1\n",
    "        return self.request_queue_no\n",
    "\n",
    "    def GenerateLearningResource(self, your_skills, job_skills, company_name, generated_directory):\n",
    "        result_dict = {\"Leetcode Question\": None, \"Skill Learning Resource Content\": None,\n",
    "                       \"Skill Learning Resource Remarks\": str(\"\")}\n",
    "        if not os.path.exists(\"learning resource/\" + generated_directory):\n",
    "            os.makedirs(\"learning resource/\" + generated_directory)\n",
    "\n",
    "        for key in job_skills:\n",
    "            text = key\n",
    "            text = text.lower()\n",
    "            if text in self.leetcode_list:\n",
    "                result_dict[\"Leetcode Question\"] = self.GenerateLeetcodeResource(company_name, generated_directory)\n",
    "                break\n",
    "        difference_skill_dict_list = {}\n",
    "        # difference_skill_dict_list = [dict_ for dict_ in job_skills if not any(dict_ == dict2 for dict2 in your_skills)]\n",
    "\n",
    "        difference_skill_dict_list = job_skills\n",
    "        if len(difference_skill_dict_list) != 0:\n",
    "            skill_result_dict = self.GenerateSkillResource(difference_skill_dict_list, generated_directory)\n",
    "            result_dict[\"Skill Learning Resource Content\"] = skill_result_dict[\"Skill Learning Resource Content\"]\n",
    "            result_dict[\"Skill Learning Resource Remarks\"] = skill_result_dict[\"Skill Learning Resource Remarks\"]\n",
    "\n",
    "        filename = \"learning resource/\" + generated_directory + \"/response.json\"\n",
    "        print(result_dict[\"Skill Learning Resource Remarks\"])\n",
    "\n",
    "        # Serialize and write the list of dictionaries to a file\n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(result_dict, file, indent=4)\n",
    "\n",
    "        self.ZipLearningResource(generated_directory)\n",
    "\n",
    "    @staticmethod\n",
    "    def ZipLearningResource(generated_directory):\n",
    "        directory_path = \"learning resource/\" + generated_directory\n",
    "        zip_filename = \"learning resource/\" + generated_directory + \"/learning resource.zip\"\n",
    "        valid_extensions = ('.html', '.docx', '.csv', '.json')\n",
    "\n",
    "        with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "            for folder_name, sub_folders, filenames in os.walk(directory_path):\n",
    "                for filename in filenames:\n",
    "                    if filename.endswith(valid_extensions):\n",
    "                        file_path = os.path.join(folder_name, filename)\n",
    "                        zipf.write(file_path, arcname=filename)\n",
    "\n",
    "    def GenerateLeetcodeResource(self, company, generated_directory):\n",
    "        leetcode_dict_list = {}\n",
    "        check_company = company\n",
    "        check_company = check_company.lower()\n",
    "        company_name_to_search = str(\"\")\n",
    "        for c in self.leetcode_company_dict_list:\n",
    "            check = c.lower()\n",
    "            if check == check_company:\n",
    "                company_name_to_search = c\n",
    "                break\n",
    "        if company_name_to_search == \"\":\n",
    "            shutil.copyfile(\"leetcode/leetcode learning resource.html\",\n",
    "                            \"learning resource/\" + generated_directory + \"/leetcode learning resource.html\")\n",
    "            shutil.copyfile(\"leetcode/leetcode learning resource.docx\",\n",
    "                            \"learning resource/\" + generated_directory + \"/leetcode learning resource.docx\")\n",
    "            df = pd.read_csv(\"leetcode/Top 100 Question List.csv\")\n",
    "            questions_content = str(\"\")\n",
    "            for index, row in df.iterrows():\n",
    "                no = str(row['No'])\n",
    "                title = str(row['Title'])\n",
    "                link = str(row['Link'])\n",
    "                path = \"leetcode/Question/\" + no + \".html\"\n",
    "                if os.path.isfile(path):\n",
    "                    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "                        file_content = file.read()\n",
    "\n",
    "                        questions_content += \"<h1><u><b>\"\n",
    "                        questions_content += no\n",
    "                        questions_content += \". \"\n",
    "                        questions_content += title\n",
    "                        questions_content += \"</b></u></h1>\\n\"\n",
    "                        questions_content += link\n",
    "                        questions_content += \"\\n\"\n",
    "                        questions_content += file_content\n",
    "\n",
    "                        h = html2text.HTML2Text()\n",
    "                        h.ignore_links = False\n",
    "                        h.inline_links = False\n",
    "                        h.reference_links = False\n",
    "                        string_format = h.handle(file_content)\n",
    "                        string_format = string_format.replace(\"**\", \"\")\n",
    "                        leetcode_dict_list[no] = no + \". \" + title + \"\\n\" + link + \"\\n\\n\" + string_format\n",
    "                        file.close()\n",
    "\n",
    "            with open(\"learning resource/\" + generated_directory + \"/leetcode question.html\", 'w',\n",
    "                      encoding='utf-8') as file:\n",
    "                file.write(questions_content)\n",
    "                file.close()\n",
    "            pypandoc.convert_text(questions_content, 'docx', format='html',\n",
    "                                  outputfile='learning resource/' + generated_directory + '/leetcode question.docx')\n",
    "            df[company + \" Company Frequency\"] = 0\n",
    "            df[\"Overall Frequency\"] = df[\"Frequency\"]\n",
    "            df = df.drop(columns=['Frequency'])\n",
    "            df.to_csv(\"learning resource/\" + generated_directory + \"/leetcode question list.csv\", encoding='utf-8',\n",
    "                      index=False)\n",
    "            return leetcode_dict_list\n",
    "        else:\n",
    "            html_content = \"\"\n",
    "            title = \"<h1><u><b>\" + company + \" Leetcode Tag Type Appear in the Question Count</b></u></h1>\\n\"\n",
    "            html_content += title\n",
    "            html_content += \"<table>\\n\"\n",
    "            html_content += \"<tr>\\n\"\n",
    "            html_content += \"  <th>Tag</th>\\n\"\n",
    "            html_content += \"  <th>Count</th>\\n\"\n",
    "            html_content += \"</tr>\\n\"\n",
    "            df = pd.read_csv(\"leetcode/Top Tag/\" + company_name_to_search + \".csv\")\n",
    "            for index, row in df.iterrows():\n",
    "                html_content += \"<tr>\\n\"\n",
    "                tag_html = \"  <td>\" + str(row[\"Tag\"]) + \"</td>\\n\"\n",
    "                count_html = \"  <td>\" + str(row[\"Appearance\"]) + \"</td>\\n\"\n",
    "                html_content += tag_html\n",
    "                html_content += count_html\n",
    "                html_content += \"</tr>\\n\"\n",
    "            html_content += \"</table>\\n\"\n",
    "            with open(\"leetcode/leetcode learning resource.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "                html_content += file.read()\n",
    "                file.close()\n",
    "            with open(\"learning resource/\" + generated_directory + \"/leetcode learning resource.html\", 'w',\n",
    "                      encoding='utf-8') as file:\n",
    "                file.write(html_content)\n",
    "                file.close()\n",
    "            pypandoc.convert_text(html_content, 'docx', format='html',\n",
    "                                  outputfile=\"learning resource/\" + generated_directory +\n",
    "                                             \"/leetcode learning resource.docx\")\n",
    "            df1 = pd.read_csv(\"leetcode/Companies Leetcode/\" + company_name_to_search + \".csv\")\n",
    "            df1[company + \" Company Frequency\"] = df1[\"Frequency\"]\n",
    "            df1 = df1.drop(columns=['Frequency'])\n",
    "            df1[\"Overall Frequency\"] = str(\"\")\n",
    "            for index, row in df1.iterrows():\n",
    "                no = str(row['No'])\n",
    "                if no in self.leetcode_overall_frequency_dict_list:\n",
    "                    df1.at[index, \"Overall Frequency\"] = self.leetcode_overall_frequency_dict_list[no]\n",
    "            df = pd.read_csv(\"leetcode/Top 100 Question List.csv\")\n",
    "            df[company + \" Company Frequency\"] = 0\n",
    "            df[\"Overall Frequency\"] = df['Frequency']\n",
    "            df = df.drop(columns=['Frequency'])\n",
    "            appended_df = pd.concat([df1, df], ignore_index=True)\n",
    "            appended_df = appended_df.drop_duplicates(keep='first')\n",
    "            final_df = appended_df.head(100).copy()\n",
    "            final_df.to_csv(\"learning resource/\" + generated_directory + \"/leetcode question list.csv\",\n",
    "                            encoding='utf-8', index=False)\n",
    "            questions_content = \"\"\n",
    "            for index, row in final_df.iterrows():\n",
    "                no = str(row['No'])\n",
    "                title = str(row['Title'])\n",
    "                link = str(row['Link'])\n",
    "                path = \"leetcode/Question/\" + no + \".html\"\n",
    "                if os.path.isfile(path):\n",
    "                    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "                        file_content = file.read()\n",
    "\n",
    "                        questions_content += \"<h1><u><b>\"\n",
    "                        questions_content += no\n",
    "                        questions_content += \". \"\n",
    "                        questions_content += title\n",
    "                        questions_content += \"</b></u></h1>\\n\"\n",
    "                        questions_content += link\n",
    "                        questions_content += \"\\n\"\n",
    "                        questions_content += file_content\n",
    "\n",
    "                        h = html2text.HTML2Text()\n",
    "                        h.ignore_links = False\n",
    "                        h.inline_links = False\n",
    "                        h.reference_links = False\n",
    "                        string_format = h.handle(file_content)\n",
    "                        string_format = string_format.replace(\"**\", \"\")\n",
    "                        leetcode_dict_list[no] = no + \". \" + title + \"\\n\" + link + \"\\n\\n\" + string_format\n",
    "                        file.close()\n",
    "\n",
    "            with (open(\"learning resource/\" + generated_directory + \"/leetcode question.html\", 'w', encoding='utf-8')\n",
    "                  as file):\n",
    "                file.write(questions_content)\n",
    "                file.close()\n",
    "            pypandoc.convert_text(questions_content, 'docx', format='html',\n",
    "                                  outputfile=\"learning resource/\" + generated_directory + \"/leetcode question.docx\")\n",
    "            return leetcode_dict_list\n",
    "\n",
    "    def GenerateSkillResource(self, skills, generated_directory):\n",
    "        result_dict = {\"Skill Learning Resource Content\": None, \"Skill Learning Resource Remarks\": str(\"\")}\n",
    "\n",
    "        result_dict[\"Skill Learning Resource Remarks\"], document_prepare_set = self.GenerateSkillResourcePreProcessing(\n",
    "            skills, result_dict[\"Skill Learning Resource Remarks\"])\n",
    "        if len(document_prepare_set) == 0:\n",
    "            return result_dict\n",
    "\n",
    "        result_dict[\"Skill Learning Resource Remarks\"], result_dict[\"Skill Learning Resource Content\"] =\\\n",
    "            self.GenerateSkillResourceContent(skills, document_prepare_set,\n",
    "                                              result_dict[\"Skill Learning Resource Remarks\"], generated_directory)\n",
    "        return result_dict\n",
    "\n",
    "    def GenerateSkillResourcePreProcessing(self, skills, remarks):\n",
    "        document_prepare_set = set()\n",
    "\n",
    "        for key, value in skills.items():\n",
    "            remarks, skills[key] = self.SkillLearningResourceFilter(key, value, remarks)\n",
    "            if skills[key] != \"\":\n",
    "                remarks, document_prepare_set = self.SkillLearningResourceSearch(key, skills[key],\n",
    "                                                                                 document_prepare_set, remarks)\n",
    "        return remarks, document_prepare_set\n",
    "\n",
    "    def GenerateSkillResourceContent(self, skills, document_prepare_set, remarks, generated_directory):\n",
    "        skill_dict = {}\n",
    "        html_content = \"\"\n",
    "        for d in document_prepare_set:\n",
    "            if d in self.skill_dict_list:\n",
    "                v = self.skill_dict_list.get(d)\n",
    "                path = v.resource_path\n",
    "            elif d in self.skill_dict_list:\n",
    "                v = self.group_dict_list.get(d)\n",
    "                path = v.resource_path\n",
    "            else:\n",
    "                continue\n",
    "            if not os.path.isfile(path):\n",
    "                if len(remarks) != 0:\n",
    "                    remarks += \"\\n\"\n",
    "                remarks += \"can't generate content for \"\n",
    "                remarks += d.title()\n",
    "            else:\n",
    "                with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "                    title = d.title()\n",
    "                    html_content += \"<h1><u><b>\"\n",
    "                    html_content += title\n",
    "                    html_content += \"</b></u></h1>\"\n",
    "                    file_content = file.read()\n",
    "                    html_content += file_content\n",
    "                    h = html2text.HTML2Text()\n",
    "                    h.ignore_links = False\n",
    "                    h.inline_links = False\n",
    "                    h.reference_links = True\n",
    "                    skill_dict[title] = h.handle(file_content)\n",
    "                file.close()\n",
    "        with open(\"learning resource/\" + generated_directory + \"/skill learning resource.html\", 'w',\n",
    "                  encoding='utf-8') as file:\n",
    "            file.write(html_content)\n",
    "            file.close()\n",
    "        pypandoc.convert_text(html_content, 'docx', format='html',\n",
    "                              outputfile=\"learning resource/\" + generated_directory + \"/skill learning resource.docx\")\n",
    "        return remarks, skill_dict\n",
    "\n",
    "    def SkillLearningResourceFilter(self, key, text, remarks):\n",
    "        text = text.lower()\n",
    "        text = text.replace(\"/\", \" \")\n",
    "        if text.find('(') != -1:\n",
    "            text = text.split(\"(\")[0]\n",
    "            text = text.rsplit()[0]\n",
    "        if text in self.ignore_set:\n",
    "            if len(remarks) != 0:\n",
    "                remarks += \"\\n\"\n",
    "            remarks += key\n",
    "            remarks += \" not found\"\n",
    "            return remarks, str(\"\")\n",
    "        if text in self.exact_match_replace_dict_list:\n",
    "            text = self.exact_match_replace_dict_list.get(text)\n",
    "        words = text.split()\n",
    "        new_text = \"\"\n",
    "        for word in words:\n",
    "            if word in self.partial_match_replace_dict_list:\n",
    "                new_text += self.partial_match_replace_dict_list.get(word)\n",
    "                new_text += \" \"\n",
    "            else:\n",
    "                new_text += word\n",
    "                new_text += \" \"\n",
    "        new_text = new_text[:-1]\n",
    "        lower_key = key.lower()\n",
    "        if lower_key != new_text:\n",
    "            if len(remarks) != 0:\n",
    "                remarks += \"\\n\"\n",
    "            remarks += key\n",
    "            remarks += \" also known as \"\n",
    "            remarks += new_text.title()\n",
    "        return remarks, new_text\n",
    "\n",
    "    def SkillLearningResourceSearch(self, key, text, document_prepare_set, remarks):\n",
    "        if text in self.skill_dict_list:\n",
    "            document_prepare_set.add(text)\n",
    "            return remarks, document_prepare_set\n",
    "\n",
    "        if text in self.group_dict_list:\n",
    "            document_prepare_set.add(text)\n",
    "            return remarks, document_prepare_set\n",
    "\n",
    "        # check for . - space and .js\n",
    "        for sdl in self.skill_dict_list:\n",
    "\n",
    "            check1 = sdl\n",
    "            check1 = re.sub(r'\\b(\\w+)s\\b', r'\\1', check1)\n",
    "            check2 = text\n",
    "            check2 = re.sub(r'\\b(\\w+)s\\b', r'\\1', check2)\n",
    "            if check1 == check2:\n",
    "                document_prepare_set.add(sdl)\n",
    "                return remarks, document_prepare_set\n",
    "            check1 = sdl\n",
    "            check1 = check1.replace(\".\", \"\")\n",
    "            check2 = text\n",
    "            check2 = check2.replace(\".\", \"\")\n",
    "            if check1 == check2:\n",
    "                document_prepare_set.add(sdl)\n",
    "                return remarks, document_prepare_set\n",
    "            check1 = sdl\n",
    "            check1 = check1.replace(\"-\", \" \")\n",
    "            check2 = text\n",
    "            check2 = check2.replace(\"-\", \" \")\n",
    "            if check1 == check2:\n",
    "                document_prepare_set.add(sdl)\n",
    "                return remarks, document_prepare_set\n",
    "            check1 = sdl\n",
    "            check1 = check1.replace(\" \", \"\")\n",
    "            check2 = text\n",
    "            check2 = check2.replace(\" \", \"\")\n",
    "            if check1 == check2:\n",
    "                document_prepare_set.add(sdl)\n",
    "                return remarks, document_prepare_set\n",
    "            check1 = sdl\n",
    "            check1 = check1.replace(\".js\", \"\")\n",
    "            check1 = check1.replace(\"js\", \"\")\n",
    "            check2 = text\n",
    "            check2 = check2.replace(\".js\", \"\")\n",
    "            check2 = check2.replace(\"js\", \"\")\n",
    "            if check1 == check2:\n",
    "                document_prepare_set.add(sdl)\n",
    "                return remarks, document_prepare_set\n",
    "\n",
    "        found = False\n",
    "        words = text.split()\n",
    "        # check word by word\n",
    "        for word in words:\n",
    "            if word not in self.partial_search_ignore_list:\n",
    "                if word in self.skill_dict_list:\n",
    "                    document_prepare_set.add(word)\n",
    "                    if len(remarks) != 0:\n",
    "                        remarks += \"\\n\"\n",
    "                    remarks += key\n",
    "                    remarks += \" also known as \"\n",
    "                    remarks += word.title()\n",
    "                    found = True\n",
    "                elif word in self.group_dict_list:\n",
    "                    document_prepare_set.add(word)\n",
    "                    if len(remarks) != 0:\n",
    "                        remarks += \"\\n\"\n",
    "                    remarks += key\n",
    "                    remarks += \" also known as \"\n",
    "                    remarks += word.title()\n",
    "                    found = True\n",
    "\n",
    "        if not found:\n",
    "            if len(remarks) != 0:\n",
    "                remarks += \"\\n\"\n",
    "            remarks += key\n",
    "            remarks += \" not found\"\n",
    "        return remarks, document_prepare_set\n",
    "\n",
    "    def AddSkillDictList(self, name, keyword, groups=None):\n",
    "        if name not in self.skill_dict_list:\n",
    "            self.skill_dict_list[name] = Skill(name, keyword, groups)\n",
    "            # print(name,\"added in skill_dict_list.\")\n",
    "            if groups is not None:\n",
    "                for g in groups:\n",
    "                    if g in self.group_dict_list:\n",
    "                        self.group_dict_list.get(g).UpdateSkillSet({name})\n",
    "                        # print(name,\"added in\",g,\".\")\n",
    "                    else:\n",
    "                        self.group_dict_list[g] = Group(g, {name})\n",
    "                        # print(\"new group:\",g,\"have been created and added\",name,\".\")\n",
    "        else:\n",
    "            self.UpdateSkillDictList(name, groups)\n",
    "\n",
    "    def ReClassificationSkillDictList(self, name, keyword, groups):\n",
    "        search_keyword = keyword\n",
    "        if name in self.backup_keyword_dict_list:\n",
    "            search_keyword = self.backup_keyword_dict_list[name]\n",
    "        self.AddSkillDictList(name, search_keyword, groups)\n",
    "\n",
    "    def UpdateSkillDictList(self, name, groups):\n",
    "        if name in self.skill_dict_list:\n",
    "            self.skill_dict_list[name].UpdateGroupSet(groups)\n",
    "\n",
    "    def AddGroupDictList(self, name, skills):\n",
    "        if skills is not None:\n",
    "            if name in self.group_dict_list:\n",
    "                self.UpdateGroupDictList(name, skills)\n",
    "            else:\n",
    "                found_set = set()\n",
    "                for s in skills:\n",
    "                    if s in self.skill_dict_list:\n",
    "                        self.skill_dict_list[s].UpdateGroupSet({name})\n",
    "                        found_set.add(s)\n",
    "                        # print(s,\"added in\",name,\"group set.\")\n",
    "                self.group_dict_list[name] = Group(name, found_set)\n",
    "\n",
    "    def UpdateGroupDictList(self, name, skills):\n",
    "        if name in self.group_dict_list:\n",
    "            found_set = set()\n",
    "            for s in skills:\n",
    "                if s in self.skill_dict_list:\n",
    "                    found_set.add(s)\n",
    "            self.group_dict_list[name].UpdateSkillSet(found_set)\n",
    "        else:\n",
    "            self.AddGroupDictList(name, skills)\n",
    "\n",
    "    def AddNotFoundDictList(self, name, keyword):\n",
    "        if name not in self.not_found_dict_list:\n",
    "            path = \"unclassified\"\n",
    "            self.not_found_dict_list[name] = Skill(name, path, keyword)\n",
    "\n",
    "    def GroupTextVectorization(self):\n",
    "        for word in self.group_dict_list:\n",
    "            if self.nlp.vocab[word].has_vector:\n",
    "                vector_word = self.nlp(word)\n",
    "                if vector_word not in self.vector_group_dict_list:\n",
    "                    self.vector_group_dict_list[vector_word] = set()\n",
    "                self.vector_group_dict_list[vector_word].add(word)\n",
    "\n",
    "    def VectorSearch(self, word):\n",
    "        if self.nlp.vocab[word].has_vector:\n",
    "            vector_word = self.nlp(word)\n",
    "            for vw in self.vector_group_dict_list:\n",
    "                similarity_score = vector_word.similarity(vw)\n",
    "                if similarity_score >= 0.9:\n",
    "                    for w in self.vector_group_dict_list[vw]:\n",
    "                        print(w)\n",
    "\n",
    "    def CopyReplaceFolder(self, source_dir, dest_dir, filename):\n",
    "        if dest_dir == \"unknown\":\n",
    "            keyword = filename + \" in tech\"\n",
    "        else:\n",
    "            keyword = filename\n",
    "        self.ReClassificationSkillDictList(filename, keyword, {dest_dir})\n",
    "        dest_dir = \"skill classified/\" + dest_dir\n",
    "        if not os.path.exists(dest_dir):\n",
    "            os.makedirs(dest_dir)\n",
    "        source_path_doc = source_dir + \"/\" + filename + \".docx\"\n",
    "        source_path_html = source_dir + \"/\" + filename + \".html\"\n",
    "        destination_path_doc = dest_dir + \"/\" + filename + \".docx\"\n",
    "        destination_path_html = dest_dir + \"/\" + filename + \".html\"\n",
    "        if source_path_doc != destination_path_doc:\n",
    "            shutil.copyfile(source_path_doc, destination_path_doc)\n",
    "        if source_path_html != destination_path_html:\n",
    "            shutil.copyfile(source_path_html, destination_path_html)\n",
    "\n",
    "    @staticmethod\n",
    "    def MakeDocsFromHtml():\n",
    "        directory = 'skill unclassified/not tech/'\n",
    "        filenames = [f for f in listdir(directory) if isfile(join(directory, f))]\n",
    "        for f in filenames:\n",
    "            print(f)\n",
    "            words = f.rsplit(\".\")\n",
    "            extension = words[len(words) - 1]\n",
    "            if extension == \"html\":\n",
    "                filename = f.replace(\".html\", \"\")\n",
    "                pypandoc.convert_file(directory + \"/\" + f, 'docx', outputfile=directory + \"/\" + filename + \".docx\")\n",
    "\n",
    "    def DeleteAllSkillFile(self):\n",
    "        for directory in self.three_word_skill_classification_set:\n",
    "            path = \"skill classified/\" + directory\n",
    "            if os.path.isdir(path):\n",
    "                for filename in os.listdir(path):\n",
    "                    file_path = os.path.join(path, filename)\n",
    "                    try:\n",
    "                        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                            os.unlink(file_path)\n",
    "                        elif os.path.isdir(file_path):\n",
    "                            shutil.rmtree(file_path)\n",
    "                    except Exception as e:\n",
    "                        print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "        for directory in self.two_word_skill_classification_set:\n",
    "            path = \"skill classified/\" + directory\n",
    "            if os.path.isdir(directory):\n",
    "                for filename in os.listdir(path):\n",
    "                    file_path = os.path.join(path, filename)\n",
    "                    try:\n",
    "                        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                            os.unlink(file_path)\n",
    "                        elif os.path.isdir(file_path):\n",
    "                            shutil.rmtree(file_path)\n",
    "                    except Exception as e:\n",
    "                        print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "        for directory in self.one_word_skill_classification_set:\n",
    "            path = \"skill classified/\" + directory\n",
    "            if os.path.isdir(path):\n",
    "                for filename in os.listdir(path):\n",
    "                    file_path = os.path.join(path, filename)\n",
    "                    try:\n",
    "                        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                            os.unlink(file_path)\n",
    "                        elif os.path.isdir(file_path):\n",
    "                            shutil.rmtree(file_path)\n",
    "                    except Exception as e:\n",
    "                        print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "        source_dir = 'skill'\n",
    "        destination_dir = 'skill classified/unknown'\n",
    "        os.makedirs(destination_dir, exist_ok=True)\n",
    "\n",
    "        for file_name in os.listdir(source_dir):\n",
    "            source_file = os.path.join(source_dir, file_name)\n",
    "            destination_file = os.path.join(destination_dir, file_name)\n",
    "            shutil.copy(source_file, destination_file)\n",
    "\n",
    "    @staticmethod\n",
    "    def FilterHtmlContent(text_content):\n",
    "        text_content = text_content.lower()\n",
    "        text_content = text_content.replace(\"[1]\", \"\")\n",
    "        text_content = text_content.replace(\"[2]\", \"\")\n",
    "        text_content = text_content.replace(\"[3]\", \"\")\n",
    "        text_content = text_content.replace(\"[4]\", \"\")\n",
    "        text_content = text_content.replace(\"[5]\", \"\")\n",
    "        text_content = text_content.replace(\"[6]\", \"\")\n",
    "        text_content = text_content.replace(\"[7]\", \"\")\n",
    "        text_content = text_content.replace(\"[8]\", \"\")\n",
    "        text_content = text_content.replace(\"[9]\", \"\")\n",
    "        text_content = text_content.replace(\"[0]\", \"\")\n",
    "        text_content = text_content.replace(\"[\", \"\")\n",
    "        text_content = text_content.replace(\"]\", \"\")\n",
    "        text_content = text_content.replace(\"(\", \"\")\n",
    "        text_content = text_content.replace(\")\", \"\")\n",
    "        text_content = text_content.replace(\"*\", \"\")\n",
    "        text_content = text_content.replace(\"\\\"\", \"\")\n",
    "        text_content = text_content.replace(\"â€™s\", \"\")\n",
    "        text_content = text_content.replace(\"!\", \"\")\n",
    "        text_content = text_content.replace(\":\", \"\")\n",
    "        text_content = text_content.replace(\",\", \"\")\n",
    "        text_content = text_content.replace(\"\\n\", \" \")\n",
    "        text_content = text_content.replace(\"/\", \" \")\n",
    "        text_content = text_content.replace(\"-\", \" \")\n",
    "        return text_content\n",
    "\n",
    "    def SkillReClassification(self):\n",
    "        self.backup_keyword_dict_list.clear()\n",
    "        for s in self.skill_dict_list:\n",
    "            self.backup_keyword_dict_list[s] = self.skill_dict_list[s].keyword_search\n",
    "\n",
    "        self.skill_dict_list.clear()\n",
    "        self.group_dict_list.clear()\n",
    "        self.vector_group_dict_list.clear()\n",
    "        self.DeleteAllSkillFile()\n",
    "        h = html2text.HTML2Text()\n",
    "        h.ignore_links = False\n",
    "        h.inline_links = False\n",
    "        h.reference_links = True\n",
    "        directory = 'skill classified/unknown'\n",
    "        filenames = [f for f in listdir(directory) if isfile(join(directory, f))]\n",
    "\n",
    "        for f in filenames:\n",
    "            words = f.rsplit(\".\")\n",
    "            extension = words[len(words) - 1]\n",
    "            if extension == \"html\":\n",
    "                filename = f.replace(\".html\", \"\")\n",
    "                with open(directory + \"/\" + f, 'r', encoding=\"utf-8\") as file:\n",
    "                    html_content = file.read()\n",
    "                    file.close()\n",
    "                text_content = self.FilterHtmlContent(h.handle(html_content))\n",
    "                words = text_content.split()\n",
    "                have_classified = False\n",
    "                for i in range(len(words)):\n",
    "                    first_word = words[i]\n",
    "                    if first_word.endswith('.'):\n",
    "                        first_word = first_word[:-1]\n",
    "\n",
    "                    one_word = first_word\n",
    "                    one_word = one_word.replace(\"microservices\", \"microservice\")\n",
    "                    one_word = one_word.replace(\"protocols\", \"protocol\")\n",
    "                    one_word = one_word.replace(\"networks\", \"network\")\n",
    "                    one_word = one_word.replace(\"website\", \"web\")\n",
    "                    one_word = one_word.replace(\"test\", \"testing\")\n",
    "                    one_word = one_word.replace(\"visualizations\", \"visualization\")\n",
    "                    one_word = one_word.replace(\"aws\", \"amazon\")\n",
    "\n",
    "                    if one_word in self.one_word_skill_classification_set:\n",
    "                        self.CopyReplaceFolder(directory, one_word, filename)\n",
    "                        have_classified = True\n",
    "\n",
    "                    if one_word == \"ai\":\n",
    "                        one_word = \"artificial intelligence\"\n",
    "                        if one_word in self.two_word_skill_classification_set:\n",
    "                            self.CopyReplaceFolder(directory, one_word, filename)\n",
    "                            have_classified = True\n",
    "                    if one_word == \"api\":\n",
    "                        one_word = \"application programming interface\"\n",
    "                        if one_word in self.three_word_skill_classification_set:\n",
    "                            self.CopyReplaceFolder(directory, one_word, filename)\n",
    "                            have_classified = True\n",
    "                    if one_word == \"nlp\":\n",
    "                        one_word = \"natural language processing\"\n",
    "                        if one_word in self.three_word_skill_classification_set:\n",
    "                            self.CopyReplaceFolder(directory, one_word, filename)\n",
    "                            have_classified = True\n",
    "\n",
    "                    if i + 1 >= len(words):\n",
    "                        break\n",
    "                    second_word = words[i + 1]\n",
    "                    if second_word.endswith('.'):\n",
    "                        second_word = second_word[:-1]\n",
    "\n",
    "                    two_word = first_word + \" \" + second_word\n",
    "                    two_word = two_word.replace(\" servers\", \" server\")\n",
    "                    two_word = two_word.replace(\" services\", \" service\")\n",
    "                    two_word = two_word.replace(\" applications\", \" application\")\n",
    "                    two_word = two_word.replace(\" apps\", \" application\")\n",
    "                    two_word = two_word.replace(\" app\", \" application\")\n",
    "                    two_word = two_word.replace(\" databases\", \" database\")\n",
    "                    two_word = two_word.replace(\" machines\", \" machine\")\n",
    "                    two_word = two_word.replace(\"website\", \"web\")\n",
    "\n",
    "                    if two_word in self.two_word_skill_classification_set:\n",
    "                        self.CopyReplaceFolder(directory, two_word, filename)\n",
    "                        have_classified = True\n",
    "\n",
    "                    if i + 2 >= len(words):\n",
    "                        break\n",
    "                    third_word = words[i + 2]\n",
    "                    if third_word.endswith('.'):\n",
    "                        third_word = third_word[:-1]\n",
    "                    three_word = first_word + \" \" + second_word + \" \" + third_word\n",
    "                    if three_word in self.three_word_skill_classification_set:\n",
    "                        self.CopyReplaceFolder(directory, three_word, filename)\n",
    "                        have_classified = True\n",
    "\n",
    "                if have_classified:\n",
    "                    file_path = directory + \"/\" + filename + \".html\"\n",
    "                    if os.path.isfile(file_path):\n",
    "                        os.remove(file_path)\n",
    "                    else:\n",
    "                        print(f\"The file {file_path} does not exist.\")\n",
    "                    file_path = directory + \"/\" + filename + \".docx\"\n",
    "                    if os.path.isfile(file_path):\n",
    "                        os.remove(file_path)\n",
    "                    else:\n",
    "                        print(f\"The file {file_path} does not exist.\")\n",
    "                else:\n",
    "                    self.ReClassificationSkillDictList(filename, filename + \" in tech\", {\"unknown\"})\n",
    "\n",
    "        self.GroupTextVectorization()\n",
    "        self.InitKeywordDictList()\n",
    "        self.ExportSkillDictList()\n",
    "        self.ExportGroupDictList()\n",
    "\n",
    "    def ClassificationUnClassifiedSkill(self):\n",
    "        h = html2text.HTML2Text()\n",
    "        h.ignore_links = False\n",
    "        h.inline_links = False\n",
    "        h.reference_links = True\n",
    "        directory = 'skill unclassified/not tech'\n",
    "        filenames = [f for f in listdir(directory) if isfile(join(directory, f))]\n",
    "        tech_word_list = [\"software\", \"application\", \"applications\", \"platform\", \"platforms\", \"api\", \"web\", \"website\",\n",
    "                          \"network\", \"networks\", \"security\", \"architecture\", \"development\", \"system\", \"systems\",\n",
    "                          \"language\", \"cloud\", \"data\", \"open\", \"source\", \"windows\"]\n",
    "        for f in filenames:\n",
    "            words = f.rsplit(\".\")\n",
    "            extension = words[len(words) - 1]\n",
    "            if extension == \"html\":\n",
    "                with open(directory + \"/\" + f, 'r', encoding=\"utf-8\") as file:\n",
    "                    html_content = file.read()\n",
    "                    file.close()\n",
    "                text_content = self.FilterHtmlContent(h.handle(html_content))\n",
    "                words = text_content.split()\n",
    "                is_tech = False\n",
    "                for i in range(len(words)):\n",
    "                    if words[i] in tech_word_list:\n",
    "                        is_tech = True\n",
    "                        break\n",
    "                if is_tech:\n",
    "                    source_file = os.path.join(\"skill unclassified/not tech\", f)\n",
    "                    destination_file = os.path.join(\"skill unclassified/tech\", f)\n",
    "                    shutil.copy(source_file, destination_file)\n",
    "\n",
    "    def FindClassificationKeyword(self):\n",
    "        h = html2text.HTML2Text()\n",
    "        h.ignore_links = False\n",
    "        h.inline_links = False\n",
    "        h.reference_links = True\n",
    "        directory = 'skill classified/unknown'\n",
    "        filenames = [f for f in listdir(directory) if isfile(join(directory, f))]\n",
    "        one_word_dict_list = {}\n",
    "        two_word_dict_list = {}\n",
    "        three_word_dict_list = {}\n",
    "        ignore_word_list = [\"a\", \"an\", \"the\", \"of\", \"on\", \"as\", \"by\", \"to\", \"with\", \"for\", \"is\", \"are\", \"was\", \"were\",\n",
    "                            \"in\"]\n",
    "        for f in filenames:\n",
    "            words = f.rsplit(\".\")\n",
    "            extension = words[len(words) - 1]\n",
    "            if extension == \"html\":\n",
    "                with open(directory + \"/\" + f, 'r', encoding=\"utf-8\") as file:\n",
    "                    html_content = file.read()\n",
    "                    file.close()\n",
    "                text_content = self.FilterHtmlContent(h.handle(html_content))\n",
    "                words = text_content.split()\n",
    "                for i in range(len(words)):\n",
    "                    first_word = words[i]\n",
    "                    if '1.' in first_word:\n",
    "                        break\n",
    "                    if first_word.endswith('.'):\n",
    "                        first_word = first_word[:-1]\n",
    "                    if first_word in ignore_word_list:\n",
    "                        continue\n",
    "                    one_word = first_word\n",
    "                    if one_word not in one_word_dict_list:\n",
    "                        one_word_dict_list[one_word] = 0\n",
    "                    one_word_dict_list[one_word] += 1\n",
    "\n",
    "                    second_word = words[i + 1]\n",
    "                    if second_word.endswith('.'):\n",
    "                        second_word = second_word[:-1]\n",
    "                    if second_word in ignore_word_list:\n",
    "                        continue\n",
    "                    two_word = first_word + \" \" + second_word\n",
    "                    if two_word not in two_word_dict_list:\n",
    "                        two_word_dict_list[two_word] = 0\n",
    "                    two_word_dict_list[two_word] += 1\n",
    "\n",
    "                    third_word = words[i + 2]\n",
    "                    if third_word.endswith('.'):\n",
    "                        third_word = third_word[:-1]\n",
    "                    if third_word in ignore_word_list:\n",
    "                        continue\n",
    "                    three_word = first_word + \" \" + second_word + \" \" + third_word\n",
    "                    if three_word not in three_word_dict_list:\n",
    "                        three_word_dict_list[three_word] = 0\n",
    "                    three_word_dict_list[three_word] += 1\n",
    "        with open('count one word.txt', 'w', encoding=\"utf-8\") as f:\n",
    "            for s in sorted(one_word_dict_list, key=one_word_dict_list.get, reverse=True):\n",
    "                f.write(str(s) + \" - \" + str(one_word_dict_list[s]))\n",
    "                f.write('\\n')\n",
    "            file.close()\n",
    "        with open('count two word.txt', 'w', encoding=\"utf-8\") as f:\n",
    "            for s in sorted(two_word_dict_list, key=two_word_dict_list.get, reverse=True):\n",
    "                f.write(str(s) + \" - \" + str(two_word_dict_list[s]))\n",
    "                f.write('\\n')\n",
    "            file.close()\n",
    "        with open('count three word.txt', 'w', encoding=\"utf-8\") as f:\n",
    "            for s in sorted(three_word_dict_list, key=three_word_dict_list.get, reverse=True):\n",
    "                f.write(str(s) + \" - \" + str(three_word_dict_list[s]))\n",
    "                f.write('\\n')\n",
    "            file.close()\n",
    "\n",
    "    def ImportIgnoreSet(self):\n",
    "        f = open(\"ignore.txt\", \"r\")\n",
    "        for c in f:\n",
    "            c = c.replace(\"\\n\", \"\")\n",
    "            self.ignore_set.add(c)\n",
    "        f.close()\n",
    "\n",
    "    def ImportClassificationSet(self):\n",
    "        file = open(\"three word skill classification.txt\", \"r\")\n",
    "        for word in file:\n",
    "            word = word.replace(\"\\n\", \"\")\n",
    "            self.three_word_skill_classification_set.add(word)\n",
    "        file.close()\n",
    "        file = open(\"two word skill classification.txt\", \"r\")\n",
    "        for word in file:\n",
    "            word = word.replace(\"\\n\", \"\")\n",
    "            self.two_word_skill_classification_set.add(word)\n",
    "        file.close()\n",
    "        file = open(\"one word skill classification.txt\", \"r\")\n",
    "        for word in file:\n",
    "            word = word.replace(\"\\n\", \"\")\n",
    "            self.one_word_skill_classification_set.add(word)\n",
    "        file.close()\n",
    "\n",
    "    def ExportSkillDictList(self):\n",
    "        file_path = \"skills.csv\"\n",
    "        with open(file_path, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Name\", \"Search Keyword\", \"Resource Path\", \"Groups\"])\n",
    "            for key, value in self.skill_dict_list.items():\n",
    "                name = key\n",
    "                search = value.keyword_search\n",
    "                path = value.resource_path\n",
    "                groups = \"\"\n",
    "\n",
    "                for g in value.group_set:\n",
    "                    groups += \"[\"\n",
    "                    groups += g\n",
    "                    groups += \"]\"\n",
    "\n",
    "                writer.writerow([name, search, path, groups])\n",
    "            file.close()\n",
    "        with open('skills.txt', 'w') as f:\n",
    "            for i in self.skill_dict_list:\n",
    "                f.write(i)\n",
    "                f.write(\"\\n\")\n",
    "            f.close()\n",
    "\n",
    "    def ImportSkillDictList(self):\n",
    "        df = pd.read_csv(\"skills.csv\")\n",
    "        for index, row in df.iterrows():\n",
    "            name = str(row['Name'])\n",
    "            keyword = str(row['Search Keyword'])\n",
    "            groups = str(row['Groups'])\n",
    "            groups_set = None\n",
    "            groups = groups.replace('[', '')\n",
    "            groups_list = groups.split(']')\n",
    "            if len(groups_list) > 0:\n",
    "                groups_list = groups_list[:-1]\n",
    "                groups_set = set()\n",
    "                for g in groups_list:\n",
    "                    groups_set.add(g)\n",
    "            # auto create group also\n",
    "            self.AddSkillDictList(name, keyword, groups_set)\n",
    "\n",
    "    def ExportGroupDictList(self):\n",
    "        file_path = \"groups.csv\"\n",
    "        with open(file_path, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Name\", \"Search Keyword\", \"Resource Path\", \"skills\"])\n",
    "            for key, value in self.group_dict_list.items():\n",
    "                name = key\n",
    "                search = value.keyword_search\n",
    "                path = value.resource_path\n",
    "                skills = \"\"\n",
    "                for s in value.skill_set:\n",
    "                    skills += \"[\"\n",
    "                    skills += s\n",
    "                    skills += \"]\"\n",
    "                writer.writerow([name, search, path, skills])\n",
    "            file.close()\n",
    "\n",
    "    def ExportMatchReplaceDictList(self):\n",
    "        file_path = \"exact match.csv\"\n",
    "        with open(file_path, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Word\", \"Replace\"])\n",
    "            for key, value in self.exact_match_replace_dict_list.items():\n",
    "                writer.writerow([key, value])\n",
    "            file.close()\n",
    "        file_path = \"partial match.csv\"\n",
    "        with open(file_path, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Word\", \"Replace\"])\n",
    "            for key, value in self.partial_match_replace_dict_list.items():\n",
    "                writer.writerow([key, value])\n",
    "            file.close()\n",
    "\n",
    "    def ExportNotFoundSet(self):\n",
    "        file_path = \"not found.csv\"\n",
    "        with open(file_path, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Name\", \"Search Keyword\", \"Resource Path\"])\n",
    "            for key, value in self.not_found_dict_list.items():\n",
    "                name = key\n",
    "                search = value.keyword_search\n",
    "                path = \"skill unclassified/not tech/\" + name + \".html\"\n",
    "\n",
    "                writer.writerow([name, search, path])\n",
    "            file.close()\n",
    "\n",
    "    def InitKeywordDictList(self):\n",
    "        self.one_keyword_dict_list.clear()\n",
    "        self.two_keyword_dict_list.clear()\n",
    "        self.three_keyword_dict_list.clear()\n",
    "        for s in self.skill_dict_list:\n",
    "            words = s.split()  \n",
    "            if len(words) == 1:\n",
    "                self.one_keyword_dict_list[s] = s\n",
    "            elif len(words) == 2:\n",
    "                self.two_keyword_dict_list[s] = s\n",
    "            else:\n",
    "                self.three_keyword_dict_list[s] = s\n",
    "        for s in self.group_dict_list:\n",
    "            words = s.split()\n",
    "            if len(words) == 1:\n",
    "                self.one_keyword_dict_list[s] = s\n",
    "            elif len(words) == 2:\n",
    "                self.two_keyword_dict_list[s] = s\n",
    "            else:\n",
    "                self.three_keyword_dict_list[s] = s\n",
    "\n",
    "    def InitLeetCodeCompanyNameDictList(self):\n",
    "        f = open(\"leetcode/companies.txt\", \"r\")\n",
    "\n",
    "        for c in f:\n",
    "            c = c.replace(\"\\n\", \"\")\n",
    "            key = c\n",
    "            key = key.lower()\n",
    "            self.leetcode_company_dict_list[key] = c\n",
    "        f.close()\n",
    "\n",
    "    def InitLeetcodeOverallFrequencyDictList(self):\n",
    "        df = pd.read_csv(\"leetcode/Question List.csv\")\n",
    "        for index, row in df.iterrows():\n",
    "            self.leetcode_overall_frequency_dict_list[str(row[\"No\"])] = str(row[\"Frequency\"])\n",
    "\n",
    "    def AllThisWillBeRemoveOnceFinalize(self):\n",
    "\n",
    "        self.exact_match_replace_dict_list[\"tdd\"] = \"testing\"\n",
    "        self.exact_match_replace_dict_list[\"webdriver\"] = \"web crawler\"\n",
    "        self.exact_match_replace_dict_list[\"vbnet\"] = \"visual basic .net\"\n",
    "        self.exact_match_replace_dict_list[\"vb.net\"] = \"visual basic .net\"\n",
    "        self.exact_match_replace_dict_list[\"vb\"] = \"visual basic\"\n",
    "        self.exact_match_replace_dict_list[\"html5\"] = \"html\"\n",
    "        self.exact_match_replace_dict_list[\"svn\"] = \"subversion\"\n",
    "        self.exact_match_replace_dict_list[\"unity3d\"] = \"unity\"\n",
    "        self.exact_match_replace_dict_list[\"mssql\"] = \"microsoft sql\"\n",
    "        self.exact_match_replace_dict_list[\"shaders\"] = \"shader\"\n",
    "        self.exact_match_replace_dict_list[\"uat\"] = \"testing\"\n",
    "        self.exact_match_replace_dict_list[\"mui\"] = \"material ui\"\n",
    "        self.exact_match_replace_dict_list[\"gui\"] = \"graphical user interface\"\n",
    "        self.exact_match_replace_dict_list[\"ui\"] = \"user interface\"\n",
    "        self.exact_match_replace_dict_list[\"aliyun\"] = \"alibaba cloud\"\n",
    "        self.exact_match_replace_dict_list[\"ali-cloud\"] = \"alibaba cloud\"\n",
    "        self.exact_match_replace_dict_list[\"asp.net mvc 5\"] = \"asp.net mvc\"\n",
    "\n",
    "        self.partial_match_replace_dict_list[\"ms\"] = \"microsoft\"\n",
    "        self.partial_match_replace_dict_list[\"db\"] = \"database\"\n",
    "\n",
    "        self.ExportMatchReplaceDictList()\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "learning_resource = TechStack()\n",
    "#learning_resource.MakeDocsFromHtml()\n",
    "#learning_resource.SkillReClassification()\n",
    "\n",
    "\n",
    "@app.route('/')\n",
    "def hello():\n",
    "    return 'Hello, World!'\n",
    "\n",
    "\n",
    "@app.route('/generate_skill_match_score', methods=['GET'])\n",
    "def generate_skill_match_score():\n",
    "    your_skill = \"c, c++, c#, java, python, javascript, typescript\"\n",
    "    job_skill = \"Responsibilities:\\nCollaborate with business stakeholders to understand their data needs and objectives.\\nCollect, clean, and preprocess data from various sources for analysis.\\nPerform exploratory data analysis to identify trends, patterns, and correlations.\\nDevelop and implement predictive models and machine learning algorithms to solve business challenges.\\nApply statistical analysis techniques to analyze complex datasets and draw meaningful conclusions.\\nCreate data visualizations and reports to communicate insights effectively to non-technical audiences.\\nCollaborate with data engineers to optimize data pipelines for efficient data processing.\\nConduct A/B testing and experimentation to evaluate the effectiveness of different strategies.\\nStay up-to-date with advancements in data science, machine learning, and artificial intelligence.\\nAssist in the development and deployment of machine learning models into production environments.\\nProvide data-driven insights and recommendations to support strategic decision-making.\\nCollaborate with other data scientists, analysts, and cross-functional teams to drive data initiatives.\\nRequirements:\\nBachelor's degree in Data Science, Computer Science, Statistics, Mathematics, or a related field (or equivalent practical experience).\\nProven experience as a Data Scientist or similar role, with a portfolio of data science projects that demonstrate your analytical skills.\\nProficiency in programming languages such as Python or R for data manipulation and analysis.\\nStrong understanding of statistical analysis, machine learning algorithms, and data visualization techniques.\\nExperience with machine learning frameworks and libraries (e.g., scikit-learn, TensorFlow, PyTorch).\\nFamiliarity with data manipulation libraries (e.g., Pandas, NumPy) and data visualization tools (e.g., Matplotlib, Seaborn).\\nSolid understanding of SQL and database concepts for querying and extracting data.\\nExcellent problem-solving skills and the ability to work with complex, unstructured datasets.\\nEffective communication skills to explain technical concepts to non-technical stakeholders.\\nExperience with big data technologies (e.g., Hadoop, Spark) is a plus.\\nKnowledge of cloud platforms and services for data analysis (e.g., AWS, Azure) is advantageous.\\nFamiliarity with natural language processing (NLP) and text analysis is a plus.\\nAdvanced degree (Master's or PhD) in a related field is beneficial but not required.\"\n",
    "    result = learning_resource.GenerateSkillMatchScore(your_skill, job_skill)\n",
    "\n",
    "    for i in result[\"Job Skills List\"]:\n",
    "        print(i)\n",
    "    \n",
    "    return jsonify(result)\n",
    "\n",
    "\n",
    "@app.route('/generate_learning_resource', methods=['GET'])\n",
    "def generate_learning_resource():\n",
    "    nodeflair = {}\n",
    "    file = open(\"nodeflair skill.txt\", \"r\")\n",
    "    for s in file:\n",
    "        s = s.replace(\"\\n\", \"\")\n",
    "        nodeflair[s] =s\n",
    "    file.close()\n",
    "    generated_directory = str(learning_resource.GetRequestQueueNo())\n",
    "    learning_resource.GenerateLearningResource(None, nodeflair, \"JPMorgan\", generated_directory)\n",
    "    learning_resource_zip_path = \"learning resource/\" + generated_directory + \"/learning resource.zip\"\n",
    "\n",
    "    return send_file(learning_resource_zip_path, as_attachment=True, download_name='learning resource.zip')\n",
    "\n",
    "\n",
    "# To run the Flask app with Werkzeug's run_simple function:\n",
    "if __name__ == '__main__':\n",
    "    run_simple('localhost', 5000, app)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7515a9f5-a5a8-4be8-ba4d-a9b8a1af7db3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f6de61-80b4-4201-9e88-411926f180f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
