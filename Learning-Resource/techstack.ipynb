{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9b981e-d994-41b8-81d8-8ff213a69c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading full_matcher ...\n",
      "loading abv_matcher ...\n",
      "loading full_uni_matcher ...\n",
      "loading low_form_matcher ...\n",
      "loading token_matcher ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://localhost:5000\n",
      "Press CTRL+C to quit\n",
      "C:\\Users\\ng_mi\\anaconda3\\lib\\site-packages\\skillNer\\utils.py:99: UserWarning: [W008] Evaluating Token.similarity based on empty vectors.\n",
      "  vec_similarity = token1.similarity(token2)\n",
      "127.0.0.1 - - [30/Mar/2024 20:18:21] \"GET /generate_skill_match_score HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data processing\n",
      "cross functional teams\n",
      "database\n",
      "machine learn\n",
      "testing\n",
      "decision make\n",
      "pytorch\n",
      "libraries\n",
      "statistical analysis\n",
      "pandas\n",
      "programming languages\n",
      "communication skills\n",
      "computer science\n",
      "machine learning algorithms\n",
      "problem solve\n",
      "exploratory datum analysis\n",
      "aws\n",
      "unstructured datasets\n",
      "predictive models\n",
      "science computer\n",
      "tools\n",
      "data manipulation\n",
      "tensorflow\n",
      "preprocess data\n",
      "algorithms\n",
      "a b testing\n",
      "strategic decision make\n",
      "big data\n",
      "data pipeline\n",
      "numpy\n",
      "experimentation\n",
      "datasets\n",
      "data visualization\n",
      "scikit-learn\n",
      "azure\n",
      "communicate\n",
      "statistics mathematics\n",
      "visualization\n",
      "cloud platforms\n",
      "matplotlib\n",
      "hadoop\n",
      "python\n",
      "machine learn algorithm\n",
      "natural language processing\n",
      "programming\n",
      "scikit learn\n",
      "extracting data\n",
      "data analysis\n",
      "analytical skill\n",
      "data science\n",
      "cloud\n",
      "seaborn\n",
      "analysis perform\n",
      "sql\n",
      "draw\n",
      "text analysis\n",
      "data technologies\n",
      "r\n",
      "artificial intelligence\n",
      "exploratory data analysis\n",
      "collaborate\n",
      "sources\n",
      "collect\n",
      "machine learning\n",
      "spark\n",
      "data engineers\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import pypandoc\n",
    "import csv\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import shutil\n",
    "import zipfile\n",
    "import html2text\n",
    "import json\n",
    "import re\n",
    "from flask import Flask, send_file, jsonify\n",
    "from werkzeug.serving import run_simple\n",
    "\n",
    "\n",
    "from spacy.matcher import PhraseMatcher\n",
    "# load default skills data base\n",
    "from skillNer.general_params import SKILL_DB\n",
    "# import skill extractor\n",
    "from skillNer.skill_extractor_class import SkillExtractor\n",
    "\n",
    "class Skill:\n",
    "\n",
    "    def __init__(self, name, keyword, groups=None):\n",
    "        filename = name\n",
    "        filename = filename.replace('/', '-')\n",
    "        filename = filename.replace(\"\\\\\", '-')\n",
    "        filename = filename + \".html\"\n",
    "        path = os.path.join(\"skill\", filename)\n",
    "        path = path.replace(\"\\\\\", '/')\n",
    "        self.resource_path = path  # for the resource path\n",
    "        self.keyword_search = keyword  # keyword for searching LLM\n",
    "        self.group_set = set()\n",
    "        if groups is not None:\n",
    "            self.UpdateGroupSet(groups)\n",
    "\n",
    "    def UpdateGroupSet(self, groups):\n",
    "        self.group_set.update(groups)\n",
    "        # print(\"skill group set updated.\")\n",
    "\n",
    "    def ChangeKeyword(self, keyword):\n",
    "        self.keyword_search = keyword\n",
    "\n",
    "\n",
    "class Group:\n",
    "    def __init__(self, name, skills):\n",
    "        filename = name\n",
    "        filename = filename.replace('/', '-')\n",
    "        filename = filename.replace(\"\\\\\", '-')\n",
    "        filename = filename + \".html\"\n",
    "        path = os.path.join(\"group\", filename)\n",
    "        path = path.replace(\"\\\\\", '/')\n",
    "        self.resource_path = path  # for the resource path\n",
    "        self.keyword_search = name + \" in tech\"  # keyword for searching LLM\n",
    "        self.skill_set = skills\n",
    "\n",
    "    def UpdateSkillSet(self, skill):\n",
    "        self.skill_set.update(skill)\n",
    "        # print(\"group skill set updated.\")\n",
    "\n",
    "    def ChangeKeyword(self, keyword):\n",
    "        self.keyword_search = keyword\n",
    "\n",
    "\n",
    "class TechStack:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load('en_core_web_lg')\n",
    "        self.skill_extractor = SkillExtractor(self.nlp, SKILL_DB, PhraseMatcher)\n",
    "        self.skill_dict_list = {}\n",
    "        self.group_dict_list = {}\n",
    "        self.exact_match_replace_dict_list = {}\n",
    "        self.partial_match_replace_dict_list = {}\n",
    "        self.not_found_dict_list = {}\n",
    "        self.three_word_skill_classification_set = set()\n",
    "        self.two_word_skill_classification_set = set()\n",
    "        self.one_word_skill_classification_set = set()\n",
    "        self.backup_keyword_dict_list = {}\n",
    "        self.partial_search_ignore_list = [\"apache\", \"microsoft\", \"google\", \"amazon\", \"apple\", \"vmware\", \"ibm\",\n",
    "                                           \"oracle\", \"sap\"]\n",
    "        self.leetcode_list = [\"c++\", \"c\", \"c#\", \"python\", \"java\", \"javascript\", \"typescript\", \"php\", \"swift\", \"kotlin\",\n",
    "                              \"go\", \"ruby\", \"scala\", \"rust\", \"racket\"]\n",
    "        self.one_keyword_dict_list = {}\n",
    "        self.two_keyword_dict_list = {}\n",
    "        self.three_keyword_dict_list = {}\n",
    "        self.leetcode_company_dict_list = {}\n",
    "        self.leetcode_overall_frequency_dict_list = {}\n",
    "        self.AllThisWillBeRemoveOnceFinalize()\n",
    "        self.ImportClassificationSet()\n",
    "        self.ImportSkillDictList()\n",
    "        self.InitKeywordDictList()\n",
    "        self.InitLeetCodeCompanyNameDictList()\n",
    "        self.InitLeetcodeOverallFrequencyDictList()\n",
    "        self.request_queue_no = 0\n",
    "\n",
    "    def GenerateSkillMatchScore(self, your_skill, job_skill):\n",
    "        result_dict = {\"Your Skills List\": None, \"Job Skills List\": None, \"Match Score\": None}\n",
    "        your_skill_set = self.ExtractSkillKeyword(your_skill)\n",
    "        job_skill_set = self.ExtractSkillKeyword(job_skill)\n",
    "        result_dict[\"Your Skills List\"] = list(your_skill_set)\n",
    "        result_dict[\"Job Skills List\"] = list(job_skill_set)\n",
    "        match_score = {}\n",
    "        for js in result_dict[\"Job Skills List\"]:\n",
    "            if js in result_dict[\"Your Skills List\"]:\n",
    "                match_score[js] = 1\n",
    "            else:\n",
    "                match_score[js] = 0\n",
    "        result_dict[\"Match Score\"] = match_score\n",
    "        return result_dict\n",
    "\n",
    "    def ExtractSkillKeyword(self, text):\n",
    "        skill_set = set()\n",
    "        text = text.lower()\n",
    "        text = text.replace(\"e.g.,\", \"\")\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        text = text.replace(\"!\", \"\")\n",
    "        text = text.replace(\",\", \"\")\n",
    "        text = text.replace(\"(\", \"\")\n",
    "        text = text.replace(\")\", \"\")\n",
    "        text = text.replace(\":\", \"\")\n",
    "        text = text.replace(\"\\\"\", \" \")\n",
    "        text = text.replace(\"/\", \" \")\n",
    "        text = text.replace(\". \", \" \")\n",
    "        words = text.split()\n",
    "  \n",
    "        for i in range(2, len(words)): \n",
    "            search_word = words[i - 2] + \" \" + words[i - 1] + \" \" + words[i]\n",
    "            if search_word in self.three_keyword_dict_list:\n",
    "                skill_set.add(self.three_keyword_dict_list[search_word])\n",
    "        for i in range(1, len(words)): \n",
    "            search_word = words[i - 1] + \" \" + words[i]\n",
    "            if search_word in self.two_keyword_dict_list:\n",
    "                skill_set.add(self.two_keyword_dict_list[search_word])\n",
    "        for i in range(len(words)):\n",
    "            if words[i] in self.one_keyword_dict_list:\n",
    "                skill_set.add(self.one_keyword_dict_list[words[i]])\n",
    "                \n",
    "        annotations =self.skill_extractor.annotate(text)\n",
    "\n",
    "        #self.skill_extractor.describe(annotations)\n",
    "\n",
    "        result = annotations[\"results\"]\n",
    "        skill_list_1 = result[\"full_matches\"]\n",
    "        skill_list_2 = result[\"ngram_scored\"]\n",
    "\n",
    "        for i in range(len(skill_list_1)):\n",
    "            info = skill_list_1[i]\n",
    "            skill = info[\"doc_node_value\"]\n",
    "            skill = skill.lower()\n",
    "            skill_set.add(skill)\n",
    "        for i in range(len(skill_list_2)):\n",
    "            info = skill_list_2[i]\n",
    "            skill = info[\"doc_node_value\"]\n",
    "            skill = skill.lower()\n",
    "            skill_set.add(skill)\n",
    "\n",
    "        return skill_set\n",
    "\n",
    "    def GetRequestQueueNo(self):\n",
    "        self.request_queue_no += 1\n",
    "        return self.request_queue_no\n",
    "\n",
    "    def GenerateLearningResource(self, your_skills, job_skills, company_name, generated_directory):\n",
    "        result_dict = {\"Skill Learning Resource Content\": None,\n",
    "                       \"Skill Learning Resource Remarks\": str(\"\")}\n",
    "        if not os.path.exists(\"learning resource/\" + generated_directory):\n",
    "            os.makedirs(\"learning resource/\" + generated_directory)\n",
    "\n",
    "        for key in job_skills:\n",
    "            text = key\n",
    "            text = text.lower()\n",
    "            if text in self.leetcode_list:\n",
    "                self.GenerateLeetcodeResource(company_name, generated_directory)\n",
    "                break\n",
    "        difference_skill_dict_list = {}\n",
    "        # difference_skill_dict_list = [dict_ for dict_ in job_skills if not any(dict_ == dict2 for dict2 in your_skills)]\n",
    "\n",
    "        difference_skill_dict_list = job_skills\n",
    "        if len(difference_skill_dict_list) != 0:\n",
    "            skill_result_dict = self.GenerateSkillResource(difference_skill_dict_list, generated_directory)\n",
    "            result_dict[\"Skill Learning Resource Content\"] = skill_result_dict[\"Skill Learning Resource Content\"]\n",
    "            result_dict[\"Skill Learning Resource Remarks\"] = skill_result_dict[\"Skill Learning Resource Remarks\"]\n",
    "\n",
    "        filename = \"learning resource/\" + generated_directory + \"/response.json\"\n",
    "        #print(result_dict[\"Skill Learning Resource Remarks\"])\n",
    "\n",
    "        for key, value in result_dict[\"Leetcode Question\"].items():\n",
    "            print(key,value)\n",
    "            \n",
    "        # Serialize and write the list of dictionaries to a file\n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(result_dict, file, indent=4)\n",
    "\n",
    "        self.ZipLearningResource(generated_directory)\n",
    "\n",
    "    @staticmethod\n",
    "    def ZipLearningResource(generated_directory):\n",
    "        directory_path = \"learning resource/\" + generated_directory\n",
    "        zip_filename = \"learning resource/\" + generated_directory + \"/learning resource.zip\"\n",
    "        valid_extensions = ('.html', '.docx', '.csv', '.json')\n",
    "\n",
    "        with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "            for folder_name, sub_folders, filenames in os.walk(directory_path):\n",
    "                for filename in filenames:\n",
    "                    if filename.endswith(valid_extensions):\n",
    "                        file_path = os.path.join(folder_name, filename)\n",
    "                        zipf.write(file_path, arcname=filename)\n",
    "\n",
    "    def GenerateLeetcodeResource(self, company, generated_directory):\n",
    "        check_company = company\n",
    "        check_company = check_company.lower()\n",
    "        company_name_to_search = str(\"\")\n",
    "        for c in self.leetcode_company_dict_list:\n",
    "            check = c.lower()\n",
    "            if check == check_company:\n",
    "                company_name_to_search = c\n",
    "                break\n",
    "        if company_name_to_search == \"\":\n",
    "            shutil.copyfile(\"leetcode/leetcode learning resource.html\",\n",
    "                            \"learning resource/\" + generated_directory + \"/leetcode learning resource.html\")\n",
    "            shutil.copyfile(\"leetcode/leetcode learning resource.docx\",\n",
    "                            \"learning resource/\" + generated_directory + \"/leetcode learning resource.docx\")\n",
    "            df = pd.read_csv(\"leetcode/Top 100 Question List.csv\")\n",
    "            df[company + \" Company Frequency\"] = 0\n",
    "            df[\"Overall Frequency\"] = df[\"Frequency\"]\n",
    "            df = df.drop(columns=['Frequency'])\n",
    "            df.to_csv(\"learning resource/\" + generated_directory + \"/leetcode question list.csv\", encoding='utf-8',\n",
    "                      index=False)\n",
    "        else:\n",
    "            html_content = \"\"\n",
    "            title = \"<h1><u><b>\" + company + \" Leetcode Tag Type Appear in the Question Count</b></u></h1>\\n\"\n",
    "            html_content += title\n",
    "            html_content += \"<table>\\n\"\n",
    "            html_content += \"<tr>\\n\"\n",
    "            html_content += \"  <th>Tag</th>\\n\"\n",
    "            html_content += \"  <th>Count</th>\\n\"\n",
    "            html_content += \"</tr>\\n\"\n",
    "            df = pd.read_csv(\"leetcode/Top Tag/\" + company_name_to_search + \".csv\")\n",
    "            for index, row in df.iterrows():\n",
    "                html_content += \"<tr>\\n\"\n",
    "                tag_html = \"  <td>\" + str(row[\"Tag\"]) + \"</td>\\n\"\n",
    "                count_html = \"  <td>\" + str(row[\"Appearance\"]) + \"</td>\\n\"\n",
    "                html_content += tag_html\n",
    "                html_content += count_html\n",
    "                html_content += \"</tr>\\n\"\n",
    "            html_content += \"</table>\\n\"\n",
    "            with open(\"leetcode/leetcode learning resource.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "                html_content += file.read()\n",
    "                file.close()\n",
    "            with open(\"learning resource/\" + generated_directory + \"/leetcode learning resource.html\", 'w',\n",
    "                      encoding='utf-8') as file:\n",
    "                file.write(html_content)\n",
    "                file.close()\n",
    "            pypandoc.convert_text(html_content, 'docx', format='html',\n",
    "                                  outputfile=\"learning resource/\" + generated_directory +\n",
    "                                             \"/leetcode learning resource.docx\")\n",
    "            df1 = pd.read_csv(\"leetcode/Companies Leetcode/\" + company_name_to_search + \".csv\")\n",
    "            df1[company + \" Company Frequency\"] = df1[\"Frequency\"]\n",
    "            df1 = df1.drop(columns=['Frequency'])\n",
    "            df1[\"Overall Frequency\"] = str(\"\")\n",
    "            for index, row in df1.iterrows():\n",
    "                no = str(row['No'])\n",
    "                if no in self.leetcode_overall_frequency_dict_list:\n",
    "                    df1.at[index, \"Overall Frequency\"] = self.leetcode_overall_frequency_dict_list[no]\n",
    "            df = pd.read_csv(\"leetcode/Top 100 Question List.csv\")\n",
    "            df[company + \" Company Frequency\"] = 0\n",
    "            df[\"Overall Frequency\"] = df['Frequency']\n",
    "            df = df.drop(columns=['Frequency'])\n",
    "            appended_df = pd.concat([df1, df], ignore_index=True)\n",
    "            appended_df = appended_df.drop_duplicates(keep='first')\n",
    "            final_df = appended_df.head(100).copy()\n",
    "            final_df.to_csv(\"learning resource/\" + generated_directory + \"/leetcode question list.csv\",\n",
    "                            encoding='utf-8', index=False)\n",
    "           \n",
    "\n",
    "    def GenerateSkillResource(self, skills, generated_directory):\n",
    "        result_dict = {\"Skill Learning Resource Content\": None, \"Skill Learning Resource Remarks\": str(\"\")}\n",
    "\n",
    "        result_dict[\"Skill Learning Resource Remarks\"], document_prepare_set = self.GenerateSkillResourcePreProcessing(\n",
    "            skills, result_dict[\"Skill Learning Resource Remarks\"])\n",
    "        if len(document_prepare_set) == 0:\n",
    "            return result_dict\n",
    "\n",
    "        result_dict[\"Skill Learning Resource Remarks\"], result_dict[\"Skill Learning Resource Content\"] =\\\n",
    "            self.GenerateSkillResourceContent(skills, document_prepare_set,\n",
    "                                              result_dict[\"Skill Learning Resource Remarks\"], generated_directory)\n",
    "        return result_dict\n",
    "\n",
    "    def GenerateSkillResourcePreProcessing(self, skills, remarks):\n",
    "        document_prepare_set = set()\n",
    "\n",
    "        for key, value in skills.items():\n",
    "            remarks, skills[key] = self.SkillLearningResourceFilter(key, value, remarks)\n",
    "            if skills[key] != \"\":\n",
    "                remarks, document_prepare_set = self.SkillLearningResourceSearch(key, skills[key],\n",
    "                                                                                 document_prepare_set, remarks)\n",
    "        return remarks, document_prepare_set\n",
    "\n",
    "    def GenerateSkillResourceContent(self, skills, document_prepare_set, remarks, generated_directory):\n",
    "        skill_dict = {}\n",
    "        html_content = \"\"\n",
    "        for d in document_prepare_set:\n",
    "            if d in self.skill_dict_list:\n",
    "                v = self.skill_dict_list.get(d)\n",
    "                path = v.resource_path\n",
    "            elif d in self.skill_dict_list:\n",
    "                v = self.group_dict_list.get(d)\n",
    "                path = v.resource_path\n",
    "            else:\n",
    "                continue\n",
    "            if not os.path.isfile(path):\n",
    "                if len(remarks) != 0:\n",
    "                    remarks += \"\\n\"\n",
    "                remarks += \"can't generate content for \"\n",
    "                remarks += d.title()\n",
    "            else:\n",
    "                with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "                    title = d.title()\n",
    "                    html_content += \"<h1><u><b>\"\n",
    "                    html_content += title\n",
    "                    html_content += \"</b></u></h1>\"\n",
    "                    file_content = file.read()\n",
    "                    html_content += file_content\n",
    "                    h = html2text.HTML2Text()\n",
    "                    h.ignore_links = False\n",
    "                    h.inline_links = False\n",
    "                    h.reference_links = True\n",
    "                    clean_text = h.handle(file_content)\n",
    "                    clean_text = clean_text.replace(\"[1]\",\"\")\n",
    "                    clean_text = clean_text.replace(\"[2]\",\"\")\n",
    "                    clean_text = clean_text.replace(\"[3]\",\"\")\n",
    "                    clean_text = clean_text.replace(\"[4]\",\"\")\n",
    "                    clean_text = clean_text.replace(\"[5]\",\"\")\n",
    "                    clean_text = clean_text.replace(\"[6]\",\"\")\n",
    "                    clean_text = clean_text.replace(\"[7]\",\"\")\n",
    "                    clean_text = clean_text.replace(\"[8]\",\"\")\n",
    "                    clean_text = clean_text.replace(\"[9]\",\"\")\n",
    "                    clean_text = clean_text.replace(\"**\",\"\")\n",
    "                    skill_dict[title] = clean_text\n",
    "                file.close()\n",
    "        with open(\"learning resource/\" + generated_directory + \"/skill learning resource.html\", 'w',\n",
    "                  encoding='utf-8') as file:\n",
    "            file.write(html_content)\n",
    "            file.close()\n",
    "        pypandoc.convert_text(html_content, 'docx', format='html',\n",
    "                              outputfile=\"learning resource/\" + generated_directory + \"/skill learning resource.docx\")\n",
    "        return remarks, skill_dict\n",
    "\n",
    "    def SkillLearningResourceFilter(self, key, text, remarks):\n",
    "        text = text.lower()\n",
    "        text = text.replace(\"/\", \" \")\n",
    "        if text.find('(') != -1:\n",
    "            text = text.split(\"(\")[0]\n",
    "            text = text.rsplit()[0]\n",
    "       \n",
    "        if text in self.exact_match_replace_dict_list:\n",
    "            text = self.exact_match_replace_dict_list.get(text)\n",
    "        words = text.split()\n",
    "        new_text = \"\"\n",
    "        for word in words:\n",
    "            if word in self.partial_match_replace_dict_list:\n",
    "                new_text += self.partial_match_replace_dict_list.get(word)\n",
    "                new_text += \" \"\n",
    "            else:\n",
    "                new_text += word\n",
    "                new_text += \" \"\n",
    "        new_text = new_text[:-1]\n",
    "        lower_key = key.lower()\n",
    "        if lower_key != new_text:\n",
    "            if len(remarks) != 0:\n",
    "                remarks += \"\\n\"\n",
    "            remarks += key\n",
    "            remarks += \" also known as \"\n",
    "            remarks += new_text.title()\n",
    "        return remarks, new_text\n",
    "\n",
    "    def SkillLearningResourceSearch(self, key, text, document_prepare_set, remarks):\n",
    "        if text in self.skill_dict_list:\n",
    "            document_prepare_set.add(text)\n",
    "            return remarks, document_prepare_set\n",
    "\n",
    "        if text in self.group_dict_list:\n",
    "            document_prepare_set.add(text)\n",
    "            return remarks, document_prepare_set\n",
    "\n",
    "        # check for . - space and .js\n",
    "        for sdl in self.skill_dict_list:\n",
    "\n",
    "            check1 = sdl\n",
    "            check1 = re.sub(r'\\b(\\w+)s\\b', r'\\1', check1)\n",
    "            check2 = text\n",
    "            check2 = re.sub(r'\\b(\\w+)s\\b', r'\\1', check2)\n",
    "            if check1 == check2:\n",
    "                document_prepare_set.add(sdl)\n",
    "                return remarks, document_prepare_set\n",
    "            check1 = sdl\n",
    "            check1 = check1.replace(\".\", \"\")\n",
    "            check2 = text\n",
    "            check2 = check2.replace(\".\", \"\")\n",
    "            if check1 == check2:\n",
    "                document_prepare_set.add(sdl)\n",
    "                return remarks, document_prepare_set\n",
    "            check1 = sdl\n",
    "            check1 = check1.replace(\"-\", \" \")\n",
    "            check2 = text\n",
    "            check2 = check2.replace(\"-\", \" \")\n",
    "            if check1 == check2:\n",
    "                document_prepare_set.add(sdl)\n",
    "                return remarks, document_prepare_set\n",
    "            check1 = sdl\n",
    "            check1 = check1.replace(\" \", \"\")\n",
    "            check2 = text\n",
    "            check2 = check2.replace(\" \", \"\")\n",
    "            if check1 == check2:\n",
    "                document_prepare_set.add(sdl)\n",
    "                return remarks, document_prepare_set\n",
    "            check1 = sdl\n",
    "            check1 = check1.replace(\".js\", \"\")\n",
    "            check1 = check1.replace(\"js\", \"\")\n",
    "            check2 = text\n",
    "            check2 = check2.replace(\".js\", \"\")\n",
    "            check2 = check2.replace(\"js\", \"\")\n",
    "            if check1 == check2:\n",
    "                document_prepare_set.add(sdl)\n",
    "                return remarks, document_prepare_set\n",
    "\n",
    "        found = False\n",
    "        words = text.split()\n",
    "        # check word by word\n",
    "        for word in words:\n",
    "            if word not in self.partial_search_ignore_list:\n",
    "                if word in self.skill_dict_list:\n",
    "                    document_prepare_set.add(word)\n",
    "                    if len(remarks) != 0:\n",
    "                        remarks += \"\\n\"\n",
    "                    remarks += key\n",
    "                    remarks += \" also known as \"\n",
    "                    remarks += word.title()\n",
    "                    found = True\n",
    "                elif word in self.group_dict_list:\n",
    "                    document_prepare_set.add(word)\n",
    "                    if len(remarks) != 0:\n",
    "                        remarks += \"\\n\"\n",
    "                    remarks += key\n",
    "                    remarks += \" also known as \"\n",
    "                    remarks += word.title()\n",
    "                    found = True\n",
    "\n",
    "        if not found:\n",
    "            if len(remarks) != 0:\n",
    "                remarks += \"\\n\"\n",
    "            remarks += key\n",
    "            remarks += \" not found\"\n",
    "        return remarks, document_prepare_set\n",
    "\n",
    "    def AddSkillDictList(self, name, keyword, groups=None):\n",
    "        if name not in self.skill_dict_list:\n",
    "            self.skill_dict_list[name] = Skill(name, keyword, groups)\n",
    "            # print(name,\"added in skill_dict_list.\")\n",
    "            if groups is not None:\n",
    "                for g in groups:\n",
    "                    if g in self.group_dict_list:\n",
    "                        self.group_dict_list.get(g).UpdateSkillSet({name})\n",
    "                        # print(name,\"added in\",g,\".\")\n",
    "                    else:\n",
    "                        self.group_dict_list[g] = Group(g, {name})\n",
    "                        # print(\"new group:\",g,\"have been created and added\",name,\".\")\n",
    "        else:\n",
    "            self.UpdateSkillDictList(name, groups)\n",
    "\n",
    "    def ReClassificationSkillDictList(self, name, keyword, groups):\n",
    "        search_keyword = keyword\n",
    "        if name in self.backup_keyword_dict_list:\n",
    "            search_keyword = self.backup_keyword_dict_list[name]\n",
    "        self.AddSkillDictList(name, search_keyword, groups)\n",
    "\n",
    "    def UpdateSkillDictList(self, name, groups):\n",
    "        if name in self.skill_dict_list:\n",
    "            self.skill_dict_list[name].UpdateGroupSet(groups)\n",
    "\n",
    "    def AddGroupDictList(self, name, skills):\n",
    "        if skills is not None:\n",
    "            if name in self.group_dict_list:\n",
    "                self.UpdateGroupDictList(name, skills)\n",
    "            else:\n",
    "                found_set = set()\n",
    "                for s in skills:\n",
    "                    if s in self.skill_dict_list:\n",
    "                        self.skill_dict_list[s].UpdateGroupSet({name})\n",
    "                        found_set.add(s)\n",
    "                        # print(s,\"added in\",name,\"group set.\")\n",
    "                self.group_dict_list[name] = Group(name, found_set)\n",
    "\n",
    "    def UpdateGroupDictList(self, name, skills):\n",
    "        if name in self.group_dict_list:\n",
    "            found_set = set()\n",
    "            for s in skills:\n",
    "                if s in self.skill_dict_list:\n",
    "                    found_set.add(s)\n",
    "            self.group_dict_list[name].UpdateSkillSet(found_set)\n",
    "        else:\n",
    "            self.AddGroupDictList(name, skills)\n",
    "\n",
    "    def AddNotFoundDictList(self, name, keyword):\n",
    "        if name not in self.not_found_dict_list:\n",
    "            path = \"unclassified\"\n",
    "            self.not_found_dict_list[name] = Skill(name, path, keyword)\n",
    "\n",
    "\n",
    "    def CopyReplaceFolder(self, source_dir, dest_dir, filename):\n",
    "        if dest_dir == \"unknown\":\n",
    "            keyword = filename + \" in tech\"\n",
    "        else:\n",
    "            keyword = filename\n",
    "        self.ReClassificationSkillDictList(filename, keyword, {dest_dir})\n",
    "        dest_dir = \"skill classified/\" + dest_dir\n",
    "        if not os.path.exists(dest_dir):\n",
    "            os.makedirs(dest_dir)\n",
    "        source_path_doc = source_dir + \"/\" + filename + \".docx\"\n",
    "        source_path_html = source_dir + \"/\" + filename + \".html\"\n",
    "        destination_path_doc = dest_dir + \"/\" + filename + \".docx\"\n",
    "        destination_path_html = dest_dir + \"/\" + filename + \".html\"\n",
    "        if source_path_doc != destination_path_doc:\n",
    "            shutil.copyfile(source_path_doc, destination_path_doc)\n",
    "        if source_path_html != destination_path_html:\n",
    "            shutil.copyfile(source_path_html, destination_path_html)\n",
    "\n",
    "    @staticmethod\n",
    "    def MakeDocsFromHtml():\n",
    "        directory = 'skill unclassified/not tech/'\n",
    "        filenames = [f for f in listdir(directory) if isfile(join(directory, f))]\n",
    "        for f in filenames:\n",
    "            print(f)\n",
    "            words = f.rsplit(\".\")\n",
    "            extension = words[len(words) - 1]\n",
    "            if extension == \"html\":\n",
    "                filename = f.replace(\".html\", \"\")\n",
    "                pypandoc.convert_file(directory + \"/\" + f, 'docx', outputfile=directory + \"/\" + filename + \".docx\")\n",
    "\n",
    "    def DeleteAllSkillFile(self):\n",
    "        for directory in self.three_word_skill_classification_set:\n",
    "            path = \"skill classified/\" + directory\n",
    "            if os.path.isdir(path):\n",
    "                for filename in os.listdir(path):\n",
    "                    file_path = os.path.join(path, filename)\n",
    "                    try:\n",
    "                        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                            os.unlink(file_path)\n",
    "                        elif os.path.isdir(file_path):\n",
    "                            shutil.rmtree(file_path)\n",
    "                    except Exception as e:\n",
    "                        print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "        for directory in self.two_word_skill_classification_set:\n",
    "            path = \"skill classified/\" + directory\n",
    "            if os.path.isdir(directory):\n",
    "                for filename in os.listdir(path):\n",
    "                    file_path = os.path.join(path, filename)\n",
    "                    try:\n",
    "                        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                            os.unlink(file_path)\n",
    "                        elif os.path.isdir(file_path):\n",
    "                            shutil.rmtree(file_path)\n",
    "                    except Exception as e:\n",
    "                        print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "        for directory in self.one_word_skill_classification_set:\n",
    "            path = \"skill classified/\" + directory\n",
    "            if os.path.isdir(path):\n",
    "                for filename in os.listdir(path):\n",
    "                    file_path = os.path.join(path, filename)\n",
    "                    try:\n",
    "                        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                            os.unlink(file_path)\n",
    "                        elif os.path.isdir(file_path):\n",
    "                            shutil.rmtree(file_path)\n",
    "                    except Exception as e:\n",
    "                        print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "        source_dir = 'skill'\n",
    "        destination_dir = 'skill classified/unknown'\n",
    "        os.makedirs(destination_dir, exist_ok=True)\n",
    "\n",
    "        for file_name in os.listdir(source_dir):\n",
    "            source_file = os.path.join(source_dir, file_name)\n",
    "            destination_file = os.path.join(destination_dir, file_name)\n",
    "            shutil.copy(source_file, destination_file)\n",
    "\n",
    "    @staticmethod\n",
    "    def FilterHtmlContent(text_content):\n",
    "        text_content = text_content.lower()\n",
    "        text_content = text_content.replace(\"[1]\", \"\")\n",
    "        text_content = text_content.replace(\"[2]\", \"\")\n",
    "        text_content = text_content.replace(\"[3]\", \"\")\n",
    "        text_content = text_content.replace(\"[4]\", \"\")\n",
    "        text_content = text_content.replace(\"[5]\", \"\")\n",
    "        text_content = text_content.replace(\"[6]\", \"\")\n",
    "        text_content = text_content.replace(\"[7]\", \"\")\n",
    "        text_content = text_content.replace(\"[8]\", \"\")\n",
    "        text_content = text_content.replace(\"[9]\", \"\")\n",
    "        text_content = text_content.replace(\"[0]\", \"\")\n",
    "        text_content = text_content.replace(\"[\", \"\")\n",
    "        text_content = text_content.replace(\"]\", \"\")\n",
    "        text_content = text_content.replace(\"(\", \"\")\n",
    "        text_content = text_content.replace(\")\", \"\")\n",
    "        text_content = text_content.replace(\"*\", \"\")\n",
    "        text_content = text_content.replace(\"\\\"\", \"\")\n",
    "        text_content = text_content.replace(\"â€™s\", \"\")\n",
    "        text_content = text_content.replace(\"!\", \"\")\n",
    "        text_content = text_content.replace(\":\", \"\")\n",
    "        text_content = text_content.replace(\",\", \"\")\n",
    "        text_content = text_content.replace(\"\\n\", \" \")\n",
    "        text_content = text_content.replace(\"/\", \" \")\n",
    "        text_content = text_content.replace(\"-\", \" \")\n",
    "        return text_content\n",
    "\n",
    "    def SkillReClassification(self):\n",
    "        self.backup_keyword_dict_list.clear()\n",
    "        for s in self.skill_dict_list:\n",
    "            self.backup_keyword_dict_list[s] = self.skill_dict_list[s].keyword_search\n",
    "        self.skill_dict_list.clear()\n",
    "        self.group_dict_list.clear()\n",
    "        self.DeleteAllSkillFile()\n",
    "        h = html2text.HTML2Text()\n",
    "        h.ignore_links = False\n",
    "        h.inline_links = False\n",
    "        h.reference_links = True\n",
    "        directory = 'skill classified/unknown'\n",
    "        filenames = [f for f in listdir(directory) if isfile(join(directory, f))]\n",
    "\n",
    "        for f in filenames:\n",
    "            words = f.rsplit(\".\")\n",
    "            extension = words[len(words) - 1]\n",
    "            if extension == \"html\":\n",
    "                filename = f.replace(\".html\", \"\")\n",
    "                with open(directory + \"/\" + f, 'r', encoding=\"utf-8\") as file:\n",
    "                    html_content = file.read()\n",
    "                    file.close()\n",
    "                text_content = self.FilterHtmlContent(h.handle(html_content))\n",
    "                words = text_content.split()\n",
    "                have_classified = False\n",
    "                for i in range(len(words)):\n",
    "                    first_word = words[i]\n",
    "                    if first_word.endswith('.'):\n",
    "                        first_word = first_word[:-1]\n",
    "\n",
    "                    one_word = first_word\n",
    "                    one_word = one_word.replace(\"microservices\", \"microservice\")\n",
    "                    one_word = one_word.replace(\"protocols\", \"protocol\")\n",
    "                    one_word = one_word.replace(\"networks\", \"network\")\n",
    "                    one_word = one_word.replace(\"website\", \"web\")\n",
    "                    one_word = one_word.replace(\"test\", \"testing\")\n",
    "                    one_word = one_word.replace(\"visualizations\", \"visualization\")\n",
    "                    one_word = one_word.replace(\"aws\", \"amazon\")\n",
    "\n",
    "                    if one_word in self.one_word_skill_classification_set:\n",
    "                        self.CopyReplaceFolder(directory, one_word, filename)\n",
    "                        have_classified = True\n",
    "\n",
    "                    if one_word == \"ai\":\n",
    "                        one_word = \"artificial intelligence\"\n",
    "                        if one_word in self.two_word_skill_classification_set:\n",
    "                            self.CopyReplaceFolder(directory, one_word, filename)\n",
    "                            have_classified = True\n",
    "                    if one_word == \"api\":\n",
    "                        one_word = \"application programming interface\"\n",
    "                        if one_word in self.three_word_skill_classification_set:\n",
    "                            self.CopyReplaceFolder(directory, one_word, filename)\n",
    "                            have_classified = True\n",
    "                    if one_word == \"nlp\":\n",
    "                        one_word = \"natural language processing\"\n",
    "                        if one_word in self.three_word_skill_classification_set:\n",
    "                            self.CopyReplaceFolder(directory, one_word, filename)\n",
    "                            have_classified = True\n",
    "\n",
    "                    if i + 1 >= len(words):\n",
    "                        break\n",
    "                    second_word = words[i + 1]\n",
    "                    if second_word.endswith('.'):\n",
    "                        second_word = second_word[:-1]\n",
    "\n",
    "                    two_word = first_word + \" \" + second_word\n",
    "                    two_word = two_word.replace(\" servers\", \" server\")\n",
    "                    two_word = two_word.replace(\" services\", \" service\")\n",
    "                    two_word = two_word.replace(\" applications\", \" application\")\n",
    "                    two_word = two_word.replace(\" apps\", \" application\")\n",
    "                    two_word = two_word.replace(\" app\", \" application\")\n",
    "                    two_word = two_word.replace(\" databases\", \" database\")\n",
    "                    two_word = two_word.replace(\" machines\", \" machine\")\n",
    "                    two_word = two_word.replace(\"website\", \"web\")\n",
    "\n",
    "                    if two_word in self.two_word_skill_classification_set:\n",
    "                        self.CopyReplaceFolder(directory, two_word, filename)\n",
    "                        have_classified = True\n",
    "\n",
    "                    if i + 2 >= len(words):\n",
    "                        break\n",
    "                    third_word = words[i + 2]\n",
    "                    if third_word.endswith('.'):\n",
    "                        third_word = third_word[:-1]\n",
    "                    three_word = first_word + \" \" + second_word + \" \" + third_word\n",
    "                    if three_word in self.three_word_skill_classification_set:\n",
    "                        self.CopyReplaceFolder(directory, three_word, filename)\n",
    "                        have_classified = True\n",
    "\n",
    "                if have_classified:\n",
    "                    file_path = directory + \"/\" + filename + \".html\"\n",
    "                    if os.path.isfile(file_path):\n",
    "                        os.remove(file_path)\n",
    "                    else:\n",
    "                        print(f\"The file {file_path} does not exist.\")\n",
    "                    file_path = directory + \"/\" + filename + \".docx\"\n",
    "                    if os.path.isfile(file_path):\n",
    "                        os.remove(file_path)\n",
    "                    else:\n",
    "                        print(f\"The file {file_path} does not exist.\")\n",
    "                else:\n",
    "                    self.ReClassificationSkillDictList(filename, filename + \" in tech\", {\"unknown\"})\n",
    "\n",
    "        self.InitKeywordDictList()\n",
    "        self.ExportSkillDictList()\n",
    "        self.ExportGroupDictList()\n",
    "\n",
    "    def ClassificationUnClassifiedSkill(self):\n",
    "        h = html2text.HTML2Text()\n",
    "        h.ignore_links = False\n",
    "        h.inline_links = False\n",
    "        h.reference_links = True\n",
    "        directory = 'skill unclassified/not tech'\n",
    "        filenames = [f for f in listdir(directory) if isfile(join(directory, f))]\n",
    "        tech_word_list = [\"software\", \"application\", \"applications\", \"platform\", \"platforms\", \"api\", \"web\", \"website\",\n",
    "                          \"network\", \"networks\", \"security\", \"architecture\", \"development\", \"system\", \"systems\",\n",
    "                          \"language\", \"cloud\", \"data\", \"open\", \"source\", \"windows\"]\n",
    "        for f in filenames:\n",
    "            words = f.rsplit(\".\")\n",
    "            extension = words[len(words) - 1]\n",
    "            if extension == \"html\":\n",
    "                with open(directory + \"/\" + f, 'r', encoding=\"utf-8\") as file:\n",
    "                    html_content = file.read()\n",
    "                    file.close()\n",
    "                text_content = self.FilterHtmlContent(h.handle(html_content))\n",
    "                words = text_content.split()\n",
    "                is_tech = False\n",
    "                for i in range(len(words)):\n",
    "                    if words[i] in tech_word_list:\n",
    "                        is_tech = True\n",
    "                        break\n",
    "                if is_tech:\n",
    "                    source_file = os.path.join(\"skill unclassified/not tech\", f)\n",
    "                    destination_file = os.path.join(\"skill unclassified/tech\", f)\n",
    "                    shutil.copy(source_file, destination_file)\n",
    "\n",
    "    def FindClassificationKeyword(self):\n",
    "        h = html2text.HTML2Text()\n",
    "        h.ignore_links = False\n",
    "        h.inline_links = False\n",
    "        h.reference_links = True\n",
    "        directory = 'skill'\n",
    "        filenames = [f for f in listdir(directory) if isfile(join(directory, f))]\n",
    "        one_word_dict_list = {}\n",
    "        two_word_dict_list = {}\n",
    "        three_word_dict_list = {}\n",
    "        ignore_word_list = [\"a\", \"an\", \"the\", \"of\", \"on\", \"as\", \"by\", \"to\", \"with\", \"for\", \"is\", \"are\", \"was\", \"were\",\n",
    "                            \"in\", \"you\", \"and\", \"or\"]\n",
    "        for f in filenames:\n",
    "            words = f.rsplit(\".\")\n",
    "            extension = words[len(words) - 1]\n",
    "            if extension == \"html\":\n",
    "                with open(directory + \"/\" + f, 'r', encoding=\"utf-8\") as file:\n",
    "                    html_content = file.read()\n",
    "                    file.close()\n",
    "                text_content = self.FilterHtmlContent(h.handle(html_content))\n",
    "                words = text_content.split()\n",
    "                for i in range(len(words)):\n",
    "                    first_word = words[i]\n",
    "                    #if '1.' in first_word:\n",
    "                        #break\n",
    "                    if first_word.endswith('.'):\n",
    "                        first_word = first_word[:-1]\n",
    "                    if first_word in ignore_word_list:\n",
    "                        continue\n",
    "                    one_word = first_word\n",
    "                    if one_word not in one_word_dict_list:\n",
    "                        one_word_dict_list[one_word] = 0\n",
    "                    one_word_dict_list[one_word] += 1\n",
    "\n",
    "                    if i + 1 >= len(words):\n",
    "                        break\n",
    "\n",
    "                    second_word = words[i + 1]\n",
    "                    if second_word.endswith('.'):\n",
    "                        second_word = second_word[:-1]\n",
    "                    if second_word in ignore_word_list:\n",
    "                        continue\n",
    "                    two_word = first_word + \" \" + second_word\n",
    "                    if two_word not in two_word_dict_list:\n",
    "                        two_word_dict_list[two_word] = 0\n",
    "                    two_word_dict_list[two_word] += 1\n",
    "\n",
    "                    if i + 2 >= len(words):\n",
    "                        break\n",
    "                        \n",
    "                    third_word = words[i + 2]\n",
    "                    if third_word.endswith('.'):\n",
    "                        third_word = third_word[:-1]\n",
    "                    if third_word in ignore_word_list:\n",
    "                        continue\n",
    "                    three_word = first_word + \" \" + second_word + \" \" + third_word\n",
    "                    if three_word not in three_word_dict_list:\n",
    "                        three_word_dict_list[three_word] = 0\n",
    "                    three_word_dict_list[three_word] += 1\n",
    "        with open('word classification/count one word.txt', 'w', encoding=\"utf-8\") as f:\n",
    "            for s in sorted(one_word_dict_list, key=one_word_dict_list.get, reverse=True):\n",
    "                f.write(str(s) + \" - \" + str(one_word_dict_list[s]))\n",
    "                f.write('\\n')\n",
    "            file.close()\n",
    "        with open('word classification/count two word.txt', 'w', encoding=\"utf-8\") as f:\n",
    "            for s in sorted(two_word_dict_list, key=two_word_dict_list.get, reverse=True):\n",
    "                f.write(str(s) + \" - \" + str(two_word_dict_list[s]))\n",
    "                f.write('\\n')\n",
    "            file.close()\n",
    "        with open('word classification/count three word.txt', 'w', encoding=\"utf-8\") as f:\n",
    "            for s in sorted(three_word_dict_list, key=three_word_dict_list.get, reverse=True):\n",
    "                f.write(str(s) + \" - \" + str(three_word_dict_list[s]))\n",
    "                f.write('\\n')\n",
    "            file.close()\n",
    "\n",
    "    def ImportIgnoreSet(self):\n",
    "        f = open(\"ignore.txt\", \"r\")\n",
    "        for c in f:\n",
    "            c = c.replace(\"\\n\", \"\")\n",
    "            self.ignore_set.add(c)\n",
    "        f.close()\n",
    "\n",
    "    def ImportClassificationSet(self):\n",
    "        file = open(\"word classification/three word skill classification.txt\", \"r\")\n",
    "        for word in file:\n",
    "            word = word.replace(\"\\n\", \"\")\n",
    "            self.three_word_skill_classification_set.add(word)\n",
    "        file.close()\n",
    "        file = open(\"word classification/two word skill classification.txt\", \"r\")\n",
    "        for word in file:\n",
    "            word = word.replace(\"\\n\", \"\")\n",
    "            self.two_word_skill_classification_set.add(word)\n",
    "        file.close()\n",
    "        file = open(\"word classification/one word skill classification.txt\", \"r\")\n",
    "        for word in file:\n",
    "            word = word.replace(\"\\n\", \"\")\n",
    "            self.one_word_skill_classification_set.add(word)\n",
    "        file.close()\n",
    "\n",
    "    def ExportSkillDictList(self):\n",
    "        file_path = \"skills.csv\"\n",
    "        with open(file_path, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Name\", \"Search Keyword\", \"Resource Path\", \"Groups\"])\n",
    "            for key, value in self.skill_dict_list.items():\n",
    "                name = key\n",
    "                search = value.keyword_search\n",
    "                path = value.resource_path\n",
    "                groups = \"\"\n",
    "\n",
    "                for g in value.group_set:\n",
    "                    groups += \"[\"\n",
    "                    groups += g\n",
    "                    groups += \"]\"\n",
    "\n",
    "                writer.writerow([name, search, path, groups])\n",
    "            file.close()\n",
    "        with open('skills.txt', 'w') as f:\n",
    "            for i in self.skill_dict_list:\n",
    "                f.write(i)\n",
    "                f.write(\"\\n\")\n",
    "            f.close()\n",
    "\n",
    "    def ImportSkillDictList(self):\n",
    "        df = pd.read_csv(\"skills.csv\")\n",
    "        for index, row in df.iterrows():\n",
    "            name = str(row['Name'])\n",
    "            keyword = str(row['Search Keyword'])\n",
    "            groups = str(row['Groups'])\n",
    "            groups_set = None\n",
    "            groups = groups.replace('[', '')\n",
    "            groups_list = groups.split(']')\n",
    "            if len(groups_list) > 0:\n",
    "                groups_list = groups_list[:-1]\n",
    "                groups_set = set()\n",
    "                for g in groups_list:\n",
    "                    groups_set.add(g)\n",
    "            # auto create group also\n",
    "            self.AddSkillDictList(name, keyword, groups_set)\n",
    "\n",
    "    def ExportGroupDictList(self):\n",
    "        file_path = \"groups.csv\"\n",
    "        with open(file_path, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Name\", \"Search Keyword\", \"Resource Path\", \"skills\"])\n",
    "            for key, value in self.group_dict_list.items():\n",
    "                name = key\n",
    "                search = value.keyword_search\n",
    "                path = value.resource_path\n",
    "                skills = \"\"\n",
    "                for s in value.skill_set:\n",
    "                    skills += \"[\"\n",
    "                    skills += s\n",
    "                    skills += \"]\"\n",
    "                writer.writerow([name, search, path, skills])\n",
    "            file.close()\n",
    "\n",
    "\n",
    "    def ExportNotFoundSet(self):\n",
    "        file_path = \"not found.csv\"\n",
    "        with open(file_path, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Name\", \"Search Keyword\", \"Resource Path\"])\n",
    "            for key, value in self.not_found_dict_list.items():\n",
    "                name = key\n",
    "                search = value.keyword_search\n",
    "                path = \"skill unclassified/not tech/\" + name + \".html\"\n",
    "\n",
    "                writer.writerow([name, search, path])\n",
    "            file.close()\n",
    "\n",
    "    def InitKeywordDictList(self):\n",
    "        self.one_keyword_dict_list.clear()\n",
    "        self.two_keyword_dict_list.clear()\n",
    "        self.three_keyword_dict_list.clear()\n",
    "        for s in self.skill_dict_list:\n",
    "            words = s.split()  \n",
    "            if len(words) == 1:\n",
    "                self.one_keyword_dict_list[s] = s\n",
    "            elif len(words) == 2:\n",
    "                self.two_keyword_dict_list[s] = s\n",
    "            else:\n",
    "                self.three_keyword_dict_list[s] = s\n",
    "        for s in self.group_dict_list:\n",
    "            words = s.split()\n",
    "            if len(words) == 1:\n",
    "                self.one_keyword_dict_list[s] = s\n",
    "            elif len(words) == 2:\n",
    "                self.two_keyword_dict_list[s] = s\n",
    "            else:\n",
    "                self.three_keyword_dict_list[s] = s\n",
    "\n",
    "    def InitLeetCodeCompanyNameDictList(self):\n",
    "        f = open(\"leetcode/companies.txt\", \"r\")\n",
    "\n",
    "        for c in f:\n",
    "            c = c.replace(\"\\n\", \"\")\n",
    "            key = c\n",
    "            key = key.lower()\n",
    "            self.leetcode_company_dict_list[key] = c\n",
    "        f.close()\n",
    "\n",
    "    def InitLeetcodeOverallFrequencyDictList(self):\n",
    "        df = pd.read_csv(\"leetcode/Question List.csv\")\n",
    "        for index, row in df.iterrows():\n",
    "            self.leetcode_overall_frequency_dict_list[str(row[\"No\"])] = str(row[\"Frequency\"])\n",
    "\n",
    "    def AllThisWillBeRemoveOnceFinalize(self):\n",
    "\n",
    "        self.exact_match_replace_dict_list[\"tdd\"] = \"testing\"\n",
    "        self.exact_match_replace_dict_list[\"webdriver\"] = \"web crawler\"\n",
    "        self.exact_match_replace_dict_list[\"vbnet\"] = \"visual basic .net\"\n",
    "        self.exact_match_replace_dict_list[\"vb.net\"] = \"visual basic .net\"\n",
    "        self.exact_match_replace_dict_list[\"vb\"] = \"visual basic\"\n",
    "        self.exact_match_replace_dict_list[\"html5\"] = \"html\"\n",
    "        self.exact_match_replace_dict_list[\"svn\"] = \"subversion\"\n",
    "        self.exact_match_replace_dict_list[\"unity3d\"] = \"unity\"\n",
    "        self.exact_match_replace_dict_list[\"mssql\"] = \"microsoft sql\"\n",
    "        self.exact_match_replace_dict_list[\"shaders\"] = \"shader\"\n",
    "        self.exact_match_replace_dict_list[\"uat\"] = \"testing\"\n",
    "        self.exact_match_replace_dict_list[\"mui\"] = \"material ui\"\n",
    "        self.exact_match_replace_dict_list[\"gui\"] = \"graphical user interface\"\n",
    "        self.exact_match_replace_dict_list[\"ui\"] = \"user interface\"\n",
    "        self.exact_match_replace_dict_list[\"aliyun\"] = \"alibaba cloud\"\n",
    "        self.exact_match_replace_dict_list[\"ali-cloud\"] = \"alibaba cloud\"\n",
    "        self.exact_match_replace_dict_list[\"asp.net mvc 5\"] = \"asp.net mvc\"\n",
    "\n",
    "        self.partial_match_replace_dict_list[\"ms\"] = \"microsoft\"\n",
    "        self.partial_match_replace_dict_list[\"db\"] = \"database\"\n",
    "\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "learning_resource = TechStack()\n",
    "#learning_resource.MakeDocsFromHtml()\n",
    "#learning_resource.SkillReClassification()\n",
    "learning_resource.FindClassificationKeyword()\n",
    "\n",
    "@app.route('/')\n",
    "def hello():\n",
    "    return 'Hello, World!'\n",
    "\n",
    "\n",
    "@app.route('/generate_skill_match_score', methods=['GET'])\n",
    "def generate_skill_match_score():\n",
    "    your_skill = \"\"\"\n",
    "    Clarence Ng Min Teck é»„æ˜Žå¾·\n",
    "Singapore\n",
    "ng_min_teck@hotmail.com 88454484\n",
    "linkedin.com/in/clarencengminteck\n",
    "Summary\n",
    "Born in Singapore and grew up in Singapore. Since young, I have been interested in Science, Geography,\n",
    "and Technology, with an academic background in computer science, information technology, multimedia,\n",
    "mathematics, and physics. My hobbies are playing video games, learning new stuff in online learning, and reading\n",
    "an articles about technology, science, space, and people's lifestyles around the world. I have many missions\n",
    "or goals in my life, like making MMO or open-world games about Singapore/World or building a little Singapore\n",
    "somewhere in the north as the earth is heating it.\n",
    "Currently pursuing part-time master's study in AI, focusing on computer vision and NLP. My research interest is AI\n",
    "predicts procedural generate 3d reconstruction building interior environment, layout, and dimension with different\n",
    "text/image/video models. Another research interest is to make AI translate existing songs with different languages\n",
    "and style covers, using translation and LLM to generate multiple sentences with about the same meaning and try\n",
    "to fit the tune, also AI tries to learn the singer's voice and generate what will be sound like when singing in different\n",
    "language and style. Or storybook to movie, movie to storybook, etc. I Still thinking about maybe going for a Ph.D.\n",
    "study after my master's course.\n",
    "Skills\n",
    "Programming languages: C/C++, C#, Java, Python, Groovy, JavaScript, Typescript\n",
    "Frameworks & Lib: .NET, Spring, Angular, Cuda, Imgui, WPF, OpenGL, Vulkan, Nvidia PhysX, Pandas, NumPy,\n",
    "Scikit learn, Spacy, NLTK, PySpark, Seaborn, Matplotlib, Selenium Base, Junit, PyTest, streamlit, transformers,\n",
    "PyTorch, xgboost, restful\n",
    "Databases: MS SQL, MySQL, JPA, Cassandra, SQLite, Neo4j\n",
    "Cloud: Azure, AWS\n",
    "Platform: Window, Linux, Ubuntu, Databrick\n",
    "Game Engine: Unreal Engine, Unity\n",
    "Web Development: HTML, CSS\n",
    "IDE:VS Code, IntelliJ, Anaconda, Pycharm\n",
    "Experience\n",
    "Software Engineer\n",
    "J.P. Morgan\n",
    "Sep 2022 - Present (1 year 7 months)\n",
    "CIB Tech Department, Payment Technology\n",
    "- Enhanced existing features or fixed bugs using Java, Spring, and Groovy at XXXXXXXX payment\n",
    "system\n",
    "- Performed automation testing for monthly releases\n",
    "- Resolved issues and inquiries for low-level environmental issues\n",
    "- Documented and created Jira tickets for each task\n",
    "- Supported XXX migration\n",
    "Clarence Ng Min Teck é»„æ˜Žå¾· - page 1\n",
    "-- Collaborate with various payment components and sub-component teams worldwide\n",
    "Software Engineer\n",
    "JPMorgan Chase & Co.\n",
    "Sep 2022 - Present (1 year 7 months)\n",
    "CIB Tech Department, Payment Technology\n",
    "- Enhanced existing features or fixed bugs using Java, Spring, and Groovy at XXXXXXXX payment\n",
    "system\n",
    "- Performed automation testing for monthly releases\n",
    "- Resolved issues and inquiries for low-level environmental issues\n",
    "- Documented and created Jira tickets for each task\n",
    "- Supported XXX migration\n",
    "- Collaborate with various payment components and sub-component teams worldwide\n",
    "Alumni Trainee in Full Stack Developer\n",
    "Wiley Edge\n",
    "Jun 2022 - Aug 2023 (1 year 3 months)\n",
    "- Trained in full-stack development using Java, Spring, JavaScript, Angular ,MySQL and Cloud\n",
    "Interactive Media Programmer\n",
    "MetaMedia People - MMP Singapore\n",
    "Jul 2021 - Dec 2021 (6 months)\n",
    "- Programmed user interactive programs for the Monetary Authority of Singapore gallery (Zone C & E),\n",
    "such as interactive media, questionnaires, games, e-books, etc.\n",
    "- Worked with one artist and one producer for each project\n",
    "- Met with MAS client and received feedback\n",
    "Interactive Media Programmer\n",
    "MetaMedia People - MMP Singapore\n",
    "Jan 2021 - Jun 2021 (6 months)\n",
    "- Programmed user interactive programs for the Monetary Authority of Singapore gallery (Zone C & E),\n",
    "such as interactive media, questionnaires, games, e-books, etc.\n",
    "- Worked with one artist and one producer for each project\n",
    "- Met with MAS client and received feedback\n",
    "Quality Assurance\n",
    "PTW\n",
    "May 2017 - Oct 2017 (6 months)\n",
    "Quality assurance video game, IT hardware and game test\n",
    "Pnsf ops team\n",
    "Singapore Police Force\n",
    "May 2015 - May 2017 (2 years 1 month)\n",
    "Clarence Ng Min Teck é»„æ˜Žå¾· - page 2\n",
    "Manage incident, dispatch resources to the incident.\n",
    "Fiber team\n",
    "Singtel\n",
    "Mar 2015 - May 2015 (3 months)\n",
    "Data entry and assign outsource contractors to attend any job case\n",
    "Poly Internship programer\n",
    "AviationLearn Pte Ltd\n",
    "Apr 2014 - Aug 2014 (5 months)\n",
    "Internship, programing assistance\n",
    "Game Master\n",
    "Cherry Credits Pte Ltd\n",
    "2012 - 2013 (1 year)\n",
    "Qa and game test\n",
    "Education\n",
    "National University of Singapore\n",
    "Master's degree, Artificial Intelligence\n",
    "Jan 2024 - Dec 2025\n",
    "Intelligent Reasoning Systems\n",
    "1. Machine Reasoning - supervised and unsupervised machine learning such as DT, KNN, NB, data\n",
    "preprocessing, grid search, and various machine learning tools like sci-kit learn, spacy\n",
    "2. Reasoning Systems - recommendation system, evolutionary & genetic algorithms\n",
    "3. Cognitive Systems\n",
    "Pattern Recognition Systems\n",
    "1. Problem Solving using Pattern Recognition\n",
    "2. Intelligent Sensing and Sense-making\n",
    "3. Pattern Recognition and Machine Learning Systems\n",
    "Intelligent Sensing Systems\n",
    "1. Vision Systems\n",
    "2. Spatial Reasoning from Sensor Data\n",
    "3. Real-Time Audio-Visual Sensing and Sense Making\n",
    "Practical Language Processing\n",
    "1. Text Analytics\n",
    "2. New Media and Sentiment Mining\n",
    "3. Text Processing using Machine Learning\n",
    "4. Conversational UI\n",
    "DigiPen Institute of Technology\n",
    "BA Computer Science and Game Design, Computer Science and Game Design\n",
    "Clarence Ng Min Teck é»„æ˜Žå¾· - page 3\n",
    "Sep 2017 - Aug 2021\n",
    "Technical\n",
    "CS 100 Computer Environment ( Basic Assembly code)\n",
    "CS 120 High-level Programming I: The C Programming Language\n",
    "CS 170 High-level Programming II: The C++ Programming Language\n",
    "CS 180 Operating Systems I: Man-Machine Interface (Context Switching, Basic Multi-thread)\n",
    "CS 225 Advanced C/C++\n",
    "CS 230 Game Implementation Techniques (Game loop, physics collision)\n",
    "CS 251 Introduction to Computer Graphics (OpenGL)\n",
    "CS 280 Data Structures\n",
    "CS 330 Algorithm Analysis\n",
    "CS 380 Artificial Intelligence for Games\n",
    "Game Project (Create Custom Engine, Lua, ImGui-UI Lib, Rapidjson-Serializer Lib)\n",
    "Unreal Engine 4 Blueprint and C++\n",
    "Math & Physic\n",
    "MAT 140 Linear Algebra and Geometry\n",
    "MAT 150 Calculus and Analytic Geometry I\n",
    "MAT 200 Calculus and Analytic Geometry II\n",
    "MAT 225 Calculus and Analytic Geometry III\n",
    "MAT 250 Linear Algebra\n",
    "MAT 258 Discrete Mathematics\n",
    "MAT 351 Quaternions, Interpolation and Animation\n",
    "PHY 200 Motion Dynamics\n",
    "PHY 250 Waves, Optics, and Thermodynamics\n",
    "Singapore Institute of Technology\n",
    "BSc (Hons) Computer Science in Interactive Media and Game Development,\n",
    "Computer Science, Game Design & Mathematics\n",
    "Sep 2017 - Aug 2021\n",
    "Game Design\n",
    "GAT101 Game History and Analysis\n",
    "GAT210 Game Mechanics I\n",
    "GAT211 Game Mechanics II\n",
    "GAT240 Technology for Designer (Unity3d/Unreal)\n",
    "GAT250 2D Game Design I (Level Design, puzzle game, top-down shooter, unity)\n",
    "GAT251 2D Game Design II (3D Level Design, RPG, Unreal )\n",
    "GAT260 User Experience Design (User Interface)\n",
    "GAT315 3D Game Design I (3D Level Design, Multiplayer Map, Unreal)\n",
    "GAT316 3D Game Design II (3D Level Design, Game Project, Custom Engine)\n",
    "Game Project and Internship\n",
    "GAM 100 Project Introduction (Ascii Game Project)\n",
    "GAM 150 Project I (2D Game Project using the in-house game engine)\n",
    "GAM 200 & 250 Project II (2D Game Project I using own build custom game engine)\n",
    "GAM 300 & 350 Project III (3D Game Project I using own build custom game engine)\n",
    "GAM 390 Internship I\n",
    "GAM 490 Internship II\n",
    "Clarence Ng Min Teck é»„æ˜Žå¾· - page 4\n",
    "Other\n",
    "ENG 116 Storytelling\n",
    "ENG 230 Speculative Fiction\n",
    "MUS 115 Fundamentals of Music and Sound Design\n",
    "PSY 101 Introduction to Psychology\n",
    "COM 150 Interpersonal and Workplace Communication\n",
    "Singapore Polytechnic\n",
    "Diploma Computer Science and Game development, Computer Science (c#,c+\n",
    "+,python and etc), Modeling, Animation, Level Design\n",
    "2012 - 2015\n",
    "Technical\n",
    "Java Programming\n",
    "Database Management System (Ms SQL)\n",
    "Web Client Development\n",
    "Infocomm Security\n",
    "Network and Operating System\n",
    "Interactive Computer Graphic (Adobe Flash)\n",
    "Mobile Game Development (Window phone, Dead*)\n",
    "Introduction game Development (C#)\n",
    "3D Game Development (C++, Directx)\n",
    "Console Game Development (Xbox)\n",
    "Simulation Physics and Artificial Intelligence (Python)\n",
    "Multiplayer Online Games (C#, unity)\n",
    "Design\n",
    "3D Level Design and Scripting Studio (Unreal Engine, 3ds Max)\n",
    "Digital Visual Design (Photoshop, Illustrator)\n",
    "Wiley Edge\n",
    "Full Stack Development, Finance Technology\n",
    "May 2022 - Sep 2023\n",
    "-Java Programming\n",
    "-Spring\n",
    "-Maven\n",
    "-MySQL\n",
    "-JDBC\n",
    "-JDBC Template\n",
    "-REST\n",
    "-JQuery\n",
    "-Spring Boot\n",
    "Institute of Technical Education\n",
    "Nitec of Multimedia Technology, Multimedia, photoshop, video edit, website\n",
    "development\n",
    "2010 - 2012\n",
    "Clarence Ng Min Teck é»„æ˜Žå¾· - page 5\n",
    "Learn how to use photoshop, illustor , video editng and webpage development\n",
    "National University of Singapore\n",
    "French Language, French Language\n",
    "Mar 2023 - Jun 2023\n",
    "Sejong Korean Language School\n",
    "Korean Language\n",
    "Jan 2018 - Dec 2019\n",
    "Udemy\n",
    "Game Development\n",
    "Jan 2020 - Aug 2021\n",
    "profile https://www.udemy.com/user/ng-min-teck/\n",
    "-OpenGL\n",
    "-Vulkan\n",
    "-Unreal Ability System\n",
    "-Unreal C++\n",
    "-Unreal Multiplayer C++\n",
    "-Basic GIS\n",
    "Licenses & Certifications\n",
    "Sejong Korean Language beginner Certificate - sejong language school\n",
    "SIT korean Language Level 1 and 2 - Singapore Institute of Technology\n",
    "SIT Japanese language Level 1 - Singapore Institute of Technology\n",
    "Microsoft Certified: Azure Fundamentals - Microsoft\n",
    "991194635\n",
    "Computer Graphics with Modern OpenGL and C++ - Udemy\n",
    "UC-be23cec3-03be-4e06-9dec-a06b83a3a1d5\n",
    "Unreal Engine C++ Developer: Learn C++ and Make Video Games - Udemy\n",
    "UC-d8fd0da1-05cd-43bd-bbfb-006ddc6a5c71\n",
    "Unreal Multiplayer Master: Video Game Dev In C++ - Udemy\n",
    "UC-35a20bf2-90de-475f-9ea2-f4507ebce0ae\n",
    "Clarence Ng Min Teck é»„æ˜Žå¾· - page 6\n",
    "Learn the Vulkan API with C++ - Udemy\n",
    "UC-c9cc1608-fbd9-43df-bd03-34f1d531a6e8\n",
    "Microsoft Certified: Azure Data Fundamentals - Microsoft\n",
    "CUDA programming Masterclass with C++ - Udemy\n",
    "UC-3ac6300b-b31e-4b17-a8b3-56cad0aec958\n",
    "Fundamentals of Accelerated Computing C/C++ - NVIDIA\n",
    "588b850026ca4931932e032cf6172168\n",
    "Modern C++ Concurrency in Depth ( C++17/20) - Udemy\n",
    "UC-52adb448-f749-4822-b1ca-5f876a56d1ed\n",
    "Microsoft Certified: Azure AI Fundamentals - Microsoft\n",
    "Certified Scrum DeveloperÂ® (CSDÂ®) - Scrum Alliance\n",
    "Issued Sep 2021 - Expires Sep 2023\n",
    "1446792\n",
    "Pro Unreal Engine Game Coding - Udemy\n",
    "UC-6bc7703d-2ca8-4903-943b-cda43e90effb\n",
    "Parallel and Concurrent Programming with C++ Part 1 - LinkedIn\n",
    "Parallel and Concurrent Programming with C++ Part 2 - LinkedIn\n",
    "Training Neural Networks in C++ - LinkedIn\n",
    "Accelerating CUDA C++ Applications with Concurrent Streams - NVIDIA\n",
    "5a7fb8443b184379ad1ef5ee65c46964\n",
    "Scaling Workloads Across Multiple GPUs with CUDA C++ - NVIDIA\n",
    "20c88c3b5f204c889d7e02dd10307717\n",
    "Rust Essential Training - LinkedIn\n",
    "IELTS Academic - British Council\n",
    "Issued Dec 2021 - Expires Dec 2023\n",
    "Clarence Ng Min Teck é»„æ˜Žå¾· - page 7\n",
    "21SG005938NGM002A\n",
    "The Complete Quantum Computing Course - Udemy\n",
    "UC-beb73e63-3440-42a0-87b1-bcd3f033f515/\n",
    "Learning Groovy - LinkedIn\n",
    "Software Development - Columbia Engineering\n",
    "French Elementary 1 - National University of Singapore\n",
    "76084369\n",
    "AWS Certified Cloud Practitioner - Amazon Web Services (AWS)\n",
    "G05TYDE18JBQ193N\n",
    "XFDS112: R Programming Fundamentals - EdX\n",
    "1555b0f59779471bb65a3795b1fcefbc\n",
    "Skills\n",
    "C++   â€¢   C#   â€¢   Python   â€¢   Java   â€¢   TypeScript   â€¢   SQL   â€¢   Machine Learning   â€¢   Natural Language\n",
    "Processing (NLP)   â€¢   Computer Vision   â€¢   C (Programming Language)\n",
    "    \"\"\"\n",
    "    job_skill = \"Responsibilities:\\nCollaborate with business stakeholders to understand their data needs and objectives.\\nCollect, clean, and preprocess data from various sources for analysis.\\nPerform exploratory data analysis to identify trends, patterns, and correlations.\\nDevelop and implement predictive models and machine learning algorithms to solve business challenges.\\nApply statistical analysis techniques to analyze complex datasets and draw meaningful conclusions.\\nCreate data visualizations and reports to communicate insights effectively to non-technical audiences.\\nCollaborate with data engineers to optimize data pipelines for efficient data processing.\\nConduct A/B testing and experimentation to evaluate the effectiveness of different strategies.\\nStay up-to-date with advancements in data science, machine learning, and artificial intelligence.\\nAssist in the development and deployment of machine learning models into production environments.\\nProvide data-driven insights and recommendations to support strategic decision-making.\\nCollaborate with other data scientists, analysts, and cross-functional teams to drive data initiatives.\\nRequirements:\\nBachelor's degree in Data Science, Computer Science, Statistics, Mathematics, or a related field (or equivalent practical experience).\\nProven experience as a Data Scientist or similar role, with a portfolio of data science projects that demonstrate your analytical skills.\\nProficiency in programming languages such as Python or R for data manipulation and analysis.\\nStrong understanding of statistical analysis, machine learning algorithms, and data visualization techniques.\\nExperience with machine learning frameworks and libraries (e.g., scikit-learn, TensorFlow, PyTorch).\\nFamiliarity with data manipulation libraries (e.g., Pandas, NumPy) and data visualization tools (e.g., Matplotlib, Seaborn).\\nSolid understanding of SQL and database concepts for querying and extracting data.\\nExcellent problem-solving skills and the ability to work with complex, unstructured datasets.\\nEffective communication skills to explain technical concepts to non-technical stakeholders.\\nExperience with big data technologies (e.g., Hadoop, Spark) is a plus.\\nKnowledge of cloud platforms and services for data analysis (e.g., AWS, Azure) is advantageous.\\nFamiliarity with natural language processing (NLP) and text analysis is a plus.\\nAdvanced degree (Master's or PhD) in a related field is beneficial but not required.\"\n",
    "    result = learning_resource.GenerateSkillMatchScore(your_skill, job_skill)\n",
    "\n",
    "    for i in result[\"Job Skills List\"]:\n",
    "        print(i)\n",
    "    \n",
    "    return jsonify(result)\n",
    "\n",
    "\n",
    "@app.route('/generate_learning_resource', methods=['GET'])\n",
    "def generate_learning_resource():\n",
    "    nodeflair = {}\n",
    "    file = open(\"nodeflair skill.txt\", \"r\")\n",
    "    for s in file:\n",
    "        s = s.replace(\"\\n\", \"\")\n",
    "        nodeflair[s] =s\n",
    "    file.close()\n",
    "    generated_directory = str(learning_resource.GetRequestQueueNo())\n",
    "    learning_resource.GenerateLearningResource(None, nodeflair, \"JPMorgan\", generated_directory)\n",
    "    learning_resource_zip_path = \"learning resource/\" + generated_directory + \"/learning resource.zip\"\n",
    "\n",
    "    return send_file(learning_resource_zip_path, as_attachment=True, download_name='learning resource.zip')\n",
    "\n",
    "\n",
    "# To run the Flask app with Werkzeug's run_simple function:\n",
    "if __name__ == '__main__':\n",
    "    run_simple('localhost', 5000, app)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f6de61-80b4-4201-9e88-411926f180f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2819df6-722a-4d9e-ba0e-bc56fca7a474",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
